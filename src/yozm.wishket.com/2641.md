---
lang: ko-KR
title: 눈과 귀가 달린 LLM, 멀티모달 AI
description: Article(s) > 눈과 귀가 달린 LLM, 멀티모달 AI
icon: fas fa-language
category: 
  - AI
  - LLM
  - OpenAI
  - Article(s)
tag: 
  - blog
  - yozm.wishket.com
  - ai
  - llm
  - large-language-model
  - openai
head:
  - - meta:
    - property: og:title
      content: Article(s) > 눈과 귀가 달린 LLM, 멀티모달 AI
    - property: og:description
      content: 눈과 귀가 달린 LLM, 멀티모달 AI
    - property: og:url
      content: https://chanhi2000.github.io/bookshelf/yozm.wishket.com/2641.html
prev: /ai/llm/articles/README.md
date: 2024-06-22
isOriginal: false
cover: https://yozm.wishket.com/media/news/2641/image2.png
---

# {{ $frontmatter.title }} 관련

```component VPCard
{
  "title": "LLM > Article(s)",
  "desc": "Article(s)",
  "link": "/ai/llm/articles/README.md",
  "logo": "https://chanhi2000.github.io/images/ico-wind.svg",
  "background": "rgba(10,10,10,0.2)"
}
```

[[toc]]

---

<SiteInfo
  name="눈과 귀가 달린 LLM, 멀티모달 AI | 요즘IT"
  desc="최근 openAI가 선보인 AI 모델 GPT-4o에 대한 관심이 뜨겁습니다. GPT-4o는 목소리 톤을 바꾸어 가며 감정을 담은 농담을 던지고, 주변 풍경을 정확히 인식하며 시각장애인을 위한 길 안내와 택시 잡기까지 대신해 줍니다. 마치 LLM이라는 두뇌에 눈과 귀가 달린 모양으로 혜성과 같이 등장했죠. GPT-4o의 진짜 정체는 무엇일까요? 이 모델은 최신 생성형 AI 트렌드인 멀티모달에 기반하고 있습니다. 이번 글에서는 원리와 사례를 위주로 멀티모달이란 무엇인지, 이 모델이 우리의 삶을 어떻게 바꾸어 나가고 있는지 소개하도록 하겠습니다."
  url="https://yozm.wishket.com/magazine/detail/2641/"
  logo="https://yozm.wishket.com/static/renewal/img/global/gnb_yozmit.svg"
  preview="https://yozm.wishket.com/media/news/2641/image2.png"/>

최근 OpenAI가 선보인 AI 모델 GPT-4o에 대한 관심이 뜨겁습니다. GPT-4o는 초거대언어모델(이하 LLM)에 기반해 매끄럽고 자연스러운 문장을, 텍스트와 음성으로 사람처럼 구사합니다. 게다가 약 232밀리 초(0.23초)라는 짧은 반응 시간 덕에 마치 실제 사람과 대화하는 듯한 느낌마저 듭니다.

놀라운 능력은 여기서 끝이 아닙니다. OpenAI가 공개한 시연 영상에서 GPT-4o는 목소리 톤을 바꾸어 가며 감정을 담은 농담을 던지기도 합니다. 또 단순 이미지 인식을 넘어 주변 풍경을 정확히 인식하며 시각장애인을 위한 길 안내와 택시 잡기까지 대신해 주었죠. GPT-4o는 마치 LLM이라는 두뇌에 눈과 귀가 달린 모양으로 혜성과 같이 등장했습니다.

![<출처: [<FontIcon icon="fas fa-globe"/>mashable](https://mashable.com/article/openai-announcement-gpt4o-voice-assistant)>](https://yozm.wishket.com/media/news/2641/image4.png)

파격적인 GPT-4o의 행보를 두고 사람들은 SF 영화 속 인공지능 로봇이 현실에 나타났다고 말했습니다. 한 발 나아가 정해진 분야의 태스크 수행에만 특화된 기존 인공지능을 넘어, 사람처럼 생각하고 행동하는 지적 존재를 뜻하는 **일반인공지능(Artificial General Intelligence, AGI)**이 GPT-4o로 구현되었다고 주장하는 이들도 생겼죠. 반면 일각에서는 GPT-4o를 두고 ‘대중의 관심을 끌기 위한 기존 기술의 짜깁기’라며 따가운 시선을 보내기도 합니다.

GPT-4o의 진짜 정체는 무엇일까요? 이 모델은 최신 생성형 AI 트렌드인 **멀티모달(multimodality)**에 기반하고 있습니다. 사실 멀티모달이라는 개념은 인공지능 개발자에게 그다지 새로운 개념은 아닙니다. GPT-4o 등장 이전부터 멀티모달 모델을 개발하고 활용한 사례가 많기 때문이죠. 이번 글에서는 원리와 사례를 위주로 멀티모달이란 무엇인지, 이 모델이 우리의 삶을 어떻게 바꾸어 나가고 있는지 소개하도록 하겠습니다.

---

## 멀티모달이란?

멀티모달 AI란 기존의 LLM같이 한 가지 인풋 & 아웃풋 데이터에 한정되지 않고 이미지, 텍스트, 음성 등 어떠한 형태의 데이터라도 입출력이 가능한 모델을 말합니다.

이미지 혹은 텍스트 같은 단일 형태 데이터 입출력만 할 수 있던 이미지 처리(Image Processing)나 자연어 처리(Natural Language Processing, NLP) 기반 딥러닝 모델과 비교해 멀티모달 모델이 가지는 장점은 명확합니다. 다양한 종류의 데이터를 융합적으로 이해하며 더 넓은 의미에서 문제를 해결할 수 있다는 것이죠. 그뿐만 아니라 GPT-4o 사례에서도 명확하게 알 수 있듯, 자연스러운 대화와 상호작용으로 유저 경험을 새로운 차원으로 끌어올릴 수도 있습니다.

멀티모달 AI는 실제로도 기존 딥러닝 모델들의 한계를 뛰어넘어 보다 복잡한 현실 세계의 문제를 해결해 나가고 있습니다. 예를 들어 자율주행 자동차 구현을 위해서는 속력, 차선, 운전자의 컨디션, 날씨, 도로 정보 등 여러 정보를 판단하는 AI 모델이 필요합니다. 이때 멀티모달 AI를 활용하면 다양한 형태의 인풋 데이터를 바탕으로 내린 종합적인 판단으로 안전한 운행이 가능해집니다.

![멀티모달 자율주행 아키텍처 <출처: [<FontIcon icon="fas fa-globe"/>카이스트](https://ave.kaist.ac.kr:59676/sub2_2.php)>](https://yozm.wishket.com/media/news/2641/image5.png)

마찬가지로 멀티모달 AI는 의료계에서도 그 존재감을 드러내고 있습니다. 멀티모달 AI는 환자를 촬영한 이미지, 병력, 가족력, 식습관 데이터 등 다양한 요소를 결합해 종합적으로 판단을 내릴 수 있습니다. 사람의 눈으로는 알아낼 수 없는 질병의 조기 진단이나 원격 진료에 활용할 수 있죠. 이처럼 AI 기술이 발전해 나가다 보면 환자의 음성과 이미지 데이터를 결합해 질병을 진단하고 치료를 제공하는 일이 머지않은 미래에 구현될 겁니다.

---

## 우리 생활 속 멀티모달

글의 서두에 언급했듯 멀티모달이라는 개념 자체는 인공지능 개발자들에게 그다지 새로운 개념은 아닙니다. 따라서 GPT-4o만큼 세련되지 않았을 뿐, 예전부터 우리가 활용하던 많은 인공지능 서비스가 사실은 멀티모달 기술에 기반하고 있습니다.

2020년대 초반 등장한 OpenAI의 Dall-e와 Stability AI의 Stable Diffusion은 프롬프트 몇 줄로 사람보다 더욱 그럴듯한 이미지를 그려내며, 크리에이터들에게 경각심을 일깨워주었습니다. 2022년 출시된 Text-to-Image 모델 Midjourney는 영국 잡지 이코노미스트의 표지 제작에 활용되기도 했죠. 이들 모두 텍스트와 이미지, 서로 다른 형태의 데이터를 다루는 멀티모달 AI입니다. 그럼 우리 생활 속 멀티모달 AI로는 어떤 것들이 있는지 구체적으로 살펴봅시다.

### 1. Image-to-Text 모델

가장 직관적이고 알기 쉬운 예시는 이미지 캡셔닝(Image Captioning)입니다. 이미지 캡셔닝 모델이라고도 불리는 **Image-to-Text 모델**은 인풋 이미지에 대한 설명을 텍스트로 달아주는 딥러닝 기반 모델을 말합니다. 이 역시 컴퓨터 비전(Computer Vision)과 자연어 처리(NLP)를 결합하여 만든 멀티모달입니다. 이러한 이미지 캡셔닝 기술은 비교적 간단한 멀티모달 아키텍처로 구현할 수 있지만, 그 활용 분야는 무궁무진합니다.

![이미지 캡셔닝 모델 <출처: [<FontIcon icon="fas fa-globe"/>analytics vidhya](https://analyticsvidhya.com/blog/2023/06/vision-transformers/)>](https://yozm.wishket.com/media/news/2641/image1.png)

예를 들어 간단한 이미지 캡셔닝 모델로도 GPT-4o 시연 영상에 등장한 장면처럼 주변을 인식하고 상황을 알리는 시각 장애인용 서비스를 만들어낼 수 있습니다. 들어온 이미지에 대한 텍스트 설명을 생성한 다음, TTS(Text To Speak) 기술을 활용해 이를 음성으로 바꿔 읽어주며 눈앞의 상황과 그림을 설명해 주는 방식으로요.

카피라이팅 문구 제작도 가능합니다. Image-to-Text 모델로 이미지에 대한 텍스트 설명을 생성하는 기술은 광고, 마케팅 자료, 소셜 미디어 콘텐츠 등에 써볼 수 있습니다. 만든 이미지와 함께 나갈 적절한 설명이나 광고 문구를 생성해 효과적으로 콘텐츠를 전달할 수 있겠죠.

### 2. Text-to-Image 모델

생성형 AI 열풍과 함께 뜨거운 관심을 받은 Midjourney, Stable Diffusion 그리고 Dall-e 같은 이미지 생성 모델 모두 멀티모달 기술에 기반합니다. 이처럼 텍스트를 기반으로 사용자들이 원하는 이미지를 생성해 내는 AI 모델을 **Text-to-Image 모델**이라고 합니다.

![Stable Diffusion을 활용한 이미지 수정 <출처: [<FontIcon icon="fas fa-globe"/>the-decoder](https://the-decoder.com/stable-diffusion-google-shows-new-method-for-more-control/)>](https://yozm.wishket.com/media/news/2641/image2.png)

이러한 이미지 생성 모델은 적게는 수십억 개에서 많게는 수천억 개에 이르는 매개변수(parameter)를 바탕으로 요구 사항에 따라 때로는 사람보다 더 정교하게 그림을 그려내고는 합니다. 활용도가 올라가며 Text-to-Image 모델을 보다 효과적으로 활용하기 위한 프롬프트 엔지니어링(prompt engineering) 역시 주목받고 있습니다.

---

## 멀티모달 모델은 어떻게 만들어질까?

그렇다면 이러한 멀티모달 모델은 어떻게 만들어질까요? 멀티모달 AI 모델의 개발 과정은 LLM 같은 단일 생성형 AI를 만들 때와 크게 다르지는 않습니다. 다른 모델처럼 방대한 양의 데이터를 사전학습(pre-training)한 다음, 수행하려는 특정 태스크에 맞게 이미지와 텍스트가 결합한 형태의 여러 데이터셋(dataset)을 추가 학습(fine-tuning)하는 방식을 적용하기도 하죠.

단일 모델과 멀티모달 AI의 가장 큰 차이점은 아키텍처에 있습니다. 멀티모달 모델은 주어진 여러 형식의 입력 데이터를 효과적으로 처리하고, 이를 통합해 유용한 출력을 생성하도록 설계한 딥러닝 아키텍처를 기반으로 합니다. 따라서 멀티모달 모델은 다양한 형식의 데이터가 가진 특성을 고려하여 이를 처리하기 위해 만든 여러 네트워크 구조의 결합이라고도 할 수 있습니다.

![<출처: [<FontIcon icon="fas fa-globe"/>medium](https://moon-walker.medium.com/openai-gpt-4o-omni-%EC%9D%B8%EA%B0%84%EC%88%98%EC%A4%80%EC%9D%98-%EC%9D%91%EB%8B%B5%EC%8B%9C%EA%B0%84%EA%B3%BC-%ED%91%9C%ED%98%84%EB%A0%A5%EC%9D%84-%EA%B0%96%EC%B6%98-%EB%A9%80%ED%8B%B0-%EB%AA%A8%EB%8B%AC-%EB%AA%A8%EB%8D%B8%EC%9D%98-f29ce75d4f33)>](https://yozm.wishket.com/media/news/2641/image3.png)

예를 들어, 이미지 데이터 인식을 위해 이미지 처리(Image Processing)에 특화된 컨볼루션 신경망(Convolutional Neural Network, CNN)을 사용하고, 텍스트 데이터 처리를 위해 연속 데이터(sequential data) 처리에 강점을 가진 순환 신경망(Recurrent Neural Network, RNN)이나 트랜스포머(Transformer) 구조를 각각 활용하는 것이죠. 이러한 여러 네트워크를 통합해 구성하면 멀티모달이 됩니다.

멀티모달 모델의 아키텍처 설계 기법으로는 대표적인 세 가지가 있습니다. Early Fusion, Late Fusion 그리고 Joint Fusion 기법이죠.

우선 Early Fusion은 모델 훈련(training) 단계에서 종류가 다른 두 가지 데이터를 하나의 데이터로 합칩니다. 이후 그 데이터를 모델에 학습(training)하는 과정을 거치게 하는 기법입니다.

다음 Late Fusion에서는 종류가 다른 두 가지 형태의 데이터를 각각 다른 모델에 학습시킵니다. 곧이어 나온 결과를 융합해 마치 멀티모달 형태로 보이게 만드는 기법이죠.

마지막, Joint Fusion은 두 가지 모달리티 데이터를 동시에 학습시키지 않고, 원하는 모델의 깊이에서 유연하게 모달리티를 병합할 수 있도록 하는 기법입니다.

---

## 마치며: 멀티모달과 생성형 AI의 미래

지금까지 멀티모달의 개념과 원리, 적용 사례를 소개했습니다. 그렇다면 GPT-4o 같은 멀티모달 모델은 생성형 AI의 미래를 어떻게 바꾸어 놓을까요?

혹자는 멀티모달 AI가 일반인공지능(AGI) 실현의 발판을 마련할 것이라고 이야기합니다. 그러나 멀티모달 방식으로 작동하는 모델을 AGI와 연결 짓는 일은 시기상조라고 생각합니다. 멀티모달 방식은 AGI 실현을 위한 필요조건은 될 수 있어도 충분조건은 아니기 때문입니다. 아직 일반인공지능은 단 하나 통용되는 명확한 정의가 없습니다. 게다가 시각, 청각, 언어 등 다양한 데이터를 받아들이고 이를 바탕으로 판단할 수 있다 해서 그것이 자율성을 가지고 있음을 뜻하지는 않죠. 여전히 생성형 AI가 특정한 행위를 하려면 인간에 의한 ‘트리거’가 필요합니다.

다만 한 가지 확실한 점은 생성형 AI가 더는 LLM과 이미지 생성 모델, 둘로 구분할 수 없게 진화하고 있다는 사실입니다. OpenAI 챗GPT에 탑재되는 기초 모델은 GPT-4 이후로 모두 멀티모달 기능을 제공하고 있습니다. 경쟁사 Google에서 만든 생성형 AI Gemini 역시 탄생부터 멀티모달을 염두에 둔 모델로 이미지의 인풋과 아웃풋이 가능합니다.

어쩌면 가까운 미래에는 LLM이라는 말이 낯설게 느껴지는 날이 올 수도 있습니다. 마치 고성능 카메라를 탑재한 스마트폰이 널리 퍼지며 디지털카메라, 필름카메라가 오히려 낯설게 느껴지기 시작한 것처럼 말이지요. 이제 멀티모달 AI는 특수한 기술이라기보다는 새로운 표준, 즉 뉴노멀(New Normal)이 되어버린 것일지도 모릅니다.

---

## 참고 글

<SiteInfo
  name="OpenAI GPT-4o(omni): 인간수준의 응답시간과 표현력을 갖춘 멀티 모달 모델의 등장 | by daewoo kim | May, 2024 | Medium"
  desc="OpenAI는 Google IO 2024를 하루 앞둔 5월 13일(현지시간. 한국시간 14일 오전 2시) 라이브 방송으로 GPT-4o를 전격 공개하였다. Google IO 2024 직전에 GPT-4o를 발표한 것은 Google을 의식한 김 빼기 작전이었는데 어느정도 성공을 거둔 것으로 보인다. Google IO에 관심이 집중된 시점에 GPT-4o를…"
  url="https://moon-walker.medium.com/openai-gpt-4o-omni-%EC%9D%B8%EA%B0%84%EC%88%98%EC%A4%80%EC%9D%98-%EC%9D%91%EB%8B%B5%EC%8B%9C%EA%B0%84%EA%B3%BC-%ED%91%9C%ED%98%84%EB%A0%A5%EC%9D%84-%EA%B0%96%EC%B6%98-%EB%A9%80%ED%8B%B0-%EB%AA%A8%EB%8B%AC-%EB%AA%A8%EB%8D%B8%EC%9D%98-f29ce75d4f33"
  logo="https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png"
  preview="https://miro.medium.com/v2/resize:fit:595/1*rpXX1NJgVcr4CUe9YEfjjA.png"/>

<SiteInfo
  name="Learning-based E2E | AVE Lab"
  desc="Learning-based End-to-End Autonomous Systems"
  url="https://ave.kaist.ac.kr:59676/sub2_2.php"
  logo="https://ave.kaist.ac.kr:59676/images/common/h_logo1.png"
  preview="https://ave.kaist.ac.kr:59676/images/sub/s_visual.jpg"/>

<SiteInfo
  name="Vision Transformers in Image Captioning - Analytics Vidhya"
  desc="Image captioning can be seen as a text or written description beneath an image to provide the details of the image using vision transformers."
  url="https://analyticsvidhya.com/blog/2023/06/vision-transformers/"
  logo="https://imgcdn.analyticsvidhya.com/favicon/av-fav.ico"
  preview="https://av-public-assets.s3.ap-south-1.amazonaws.com/logos/av-logo-svg.svg"/>

