---
lang: ko-KR
title: Ollama에 없는 모델 내가 만들어 사용하기
description: Article(s) > Ollama에 없는 모델 내가 만들어 사용하기
icon: fa-brands fa-meta
category: 
  - AI
  - Google 
  - Llama
  - Article(s)
tag: 
  - blog
  - yozm.wishket.com
  - ai
  - meta
  - llm
  - llama
  - ollama
head:
  - - meta:
    - property: og:title
      content: Article(s) > Ollama에 없는 모델 내가 만들어 사용하기
    - property: og:description
      content: Ollama에 없는 모델 내가 만들어 사용하기
    - property: og:url
      content: https://chanhi2000.github.io/bookshelf/yozm.wishket.com/2751.html
prev: /ai/llama/articles/README.md
date: 2024-09-06
isOriginal: false
cover: https://yozm.wishket.com/media/news/2751/image1.png
---

# {{ $frontmatter.title }} 관련

```component VPCard
{
  "title": "Google Gemini > Article(s)",
  "desc": "Article(s)",
  "link": "/ai/llama/articles/README.md",
  "logo": "https://chanhi2000.github.io/images/ico-wind.svg",
  "background": "rgba(10,10,10,0.2)"
}
```

[[toc]]

---

<SiteInfo
  name="Ollama에 없는 모델 내가 만들어 사용하기 (1) | 요즘IT"
  desc="K8sGPT는 쿠버네티스 클러스터의 정보를 스캐닝하여 이슈를 발견하고, 그 이슈의 원인과 결과, 해결 방안을 알려 줍니다. 이 프로젝트는 처음에는 오픈소스 LLM과 함께 시작했지만, 이제 직접 접근하고 관리할 수 있는 AI 제공자를 구성해 활용하는 기능을 제공합니다. 구성을 위해 필요한 것은 AI 모델의 실행과 해당 모델에게 요청할 API 주소, 두 가지입니다. 이때 권장하는 도구는 바로 Ollama입니다. 이번 글에서는 Ollama의 create 명령을 사용해 내가 필요한 모델을 활용하는 방법을 알아보겠습니다. 꼭 K8sGPT를 활용하지 않더라도 Ollama 활용법에 관심이 있는 분들에게 도움이 될 것으로 기대합니다."
  url="https://yozm.wishket.com/magazine/detail/2751/"
  logo="https://yozm.wishket.com/favicon.ico"
  preview="https://yozm.wishket.com/media/news/2751/image1.png"/>

[K8sGPT (<VPIcon icon="iconfont icon-github"/>`k8sgpt-ai/k8sgpt`)](https://github.com/k8sgpt-ai/k8sgpt)는 쿠버네티스와 대규모언어모델(이하 LLM) GPT를 결합해 만들어진 오픈소스 프로젝트입니다. 쿠버네티스 클러스터의 정보를 스캐닝하여 이슈를 발견하고, 그 이슈의 원인과 결과, 해결 방안을 알려 줍니다.

이 프로젝트는 처음에는 오픈소스 LLM과 함께 시작했지만, 이제 직접 접근하고 관리할 수 있는 AI 제공자를 구성해 활용하는 기능을 제공합니다. 따라서 데이터가 노출되는 것을 우려하거나 환경에 따라 또는 토큰을 크게 쓰고 싶은 경우, 로컬에서 LLM 환경을 만들고 이를 적용할 수 있습니다.

::: note 참고 글

```component VPCard
{
  "title": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기 (1,2) | 요즘IT",
  "desc": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기",
  "link": "/yozm.wishket.com/2515.md",
  "logo": "https://yozm.wishket.com/favicon.ico", 
  "background": "rgba(84,7,224,0.2)"
}
```

:::

구성을 위해 필요한 것은 AI 모델의 실행과 해당 모델에게 요청할 API 주소, 두 가지입니다. 이때 권장하는 도구는 바로 [<VPIcon icon="fas fa-globe"/>Ollama](https://ollama.com/)입니다. 로컬 환경에 LLM을 손쉽게 구현하도록 도와주는 도구죠. Ollama에는 llama3.1을 비롯해 다양한 공개 모델들이 매우 빠르게 포팅되어 올라옵니다. 따라서 활용 시 특별히 신경 써야 할 부분은 없습니다.

그럼에도 공개된 LLM 또는 sLM 모델을 모두 받을 수 있는 것은 아닙니다. 이를 위해 Ollama는 필요에 따라 다른 곳에 공개된 모델을 사용할 수 있도록 `create` 명령을 제공합니다.

![Ollama에서 llama3 또는 그 이상의 중국어 모델을 사용하고자 검색했으나 없는 경우](https://yozm.wishket.com/media/news/2751/image1.png)

따라서 이번 기고에서는 create 명령을 사용하기 위한 절차, 그리고 준비 과정을 살펴보며 전반적인 구조에 대한 이해를 높이려고 합니다. 꼭 K8sGPT를 활용하지 않더라도 Ollama 활용법에 관심이 있는 분들에게 도움이 될 것으로 기대합니다. 작업 순서는 아래와 같습니다.

> 1. 공개된 모델 내려받기
> 2. 파이토치(PyTorch) 프레임워크를 사용한 모델을 GGUF 포맷으로 변환하기
> 3. 기본 f16으로 생성된 GGUF 모델을 Q5_K_M으로 양자화
> 4. Ollama의 create 명령을 사용해 모델 추가하기
> 5. Ollama에 추가된 모델을 K8sGPT 분석에 사용하기

1부 글에서는 내가 활용하고자 하는 모델을 Ollama에 활용하기 좋게 변환하는 작업까지 알아볼 예정입니다.

![Ollama 로고를 응용한 그림 <br/><출처: [<VPIcon icon="fas fa-globe"/>Ollama 블로그](https://ollama.com/blog/tool-support)>](https://yozm.wishket.com/media/news/2751/image5.png)

---

## 0단계: 사전 준비

작업을 진행하려면 아래 도구들이 설치되어 있어야 합니다. (마이너 버전은 시기에 따라 다소 달라질 수 있습니다.)

- python3
- pip3
- huggingface-cli

또한 다음과 같은 구조로 만든 디렉터리를 작업에 활용할 예정입니다.

![](https://yozm.wishket.com/media/news/2751/1.png)
<!-- TODO: Google Lens -->

---

## 1단계: 허깅페이스(Hugging Face)에서 모델 내려받기

가장 먼저 해야 할 일은 허깅페이스에서 어떤 LLM을 내려받을 지 결정하는 것입니다.

이번 실습에 활용할 모델은 Llama3-Chinese-8B-Instruct 모델입니다. 그에 앞서 우선 Ollama에 llama2-chinese를 올려둔 곳에서 따로 제공하는 llama3이 있는지 해당 조직 저장소로 이동해 확인해 봅니다. 저장소에 아래 이미지처럼 `Llama3-Chinese-8B-Instruct`이 있는 것을 확인할 수 있습니다.

![FlagAlpha에서 제공하는 모델 <br/><출처: 허깅페이스, [<VPIcon icon="iconfont icon-huggingface"/>`FlagAlpha`](https://huggingface.co/FlagAlpha)>](https://yozm.wishket.com/media/news/2751/image2.png)

다음으로는 페이지에서 어떤 포맷으로 모델을 제공하는 지 확인해야 합니다. 모델이 세이프텐서(safetensor) 포맷으로 제공되는 것을 확인했습니다. 참고로 허깅페이스에서 제공하는 모델은 대부분 세이프텐서 형태로 제공합니다.

![FlagAlpha에서 제공하는 Llama3-Chinese-8B-Instruct 모델의 세부 사항들 <br/><출처: 허깅페이스, [<VPIcon icon="iconfont icon-huggingface"/>FlagAlpha](https://huggingface.co/FlagAlpha/Llama3-Chinese-8B-Instruct/tree/main)>](https://yozm.wishket.com/media/news/2751/image8.png)

해당 모델을 내려받으려면 `huggingface-cli login` 명령어로 허깅페이스에 로그인을 해야 합니다. 허깅페이스 로그인 토큰을 받으려면 마찬가지 [<VPIcon icon="iconfont icon-huggingface"/>허깅페이스 공식 문서](https://huggingface.co/docs/hub/security-tokens)를 참고할 수 있습니다.

![](https://yozm.wishket.com/media/news/2751/2.png)
<!-- TODO: Google Lens -->

이제 허깅페이스에서 모델을 내려받기 위해 주소를 웹 페이지에서 복사합니다.

![FlagAlpha의 Llama3-Chinese-8B-Instruct 소개 페이지 <br/><출처: 허깅페이스, [<VPIcon icon="iconfont icon-huggingface"/>FlagAlpha](https://huggingface.co/FlagAlpha/Llama3-Chinese-8B-Instruct)>](https://yozm.wishket.com/media/news/2751/image15.png)

모델 다운로드에는 `download` 명령을 활용합니다. 내려받는 주소를 정확하게 지정해 받기 위해 `--local-dir` 명령어도 함께 활용했습니다.

![](https://yozm.wishket.com/media/news/2751/3.png)
<!-- TODO: Google Lens -->

다운로드가 끝난 다음, 내려받은 모델과 관련 파일을 확인합니다.

![](https://yozm.wishket.com/media/news/2751/4.png)

---

## 2단계: 모델을 GGUF 파일 형식으로 변환하기 

앞서 확인했듯 허깅페이스에서 내려받은 모델은 세이프텐서(safetensors) 형식입니다. 허깅페이스에서 배포되는 모델이 주로 세이프텐서를 쓰는 주된 이유는 pt(PyTorch) 모델의 가중치에 악성코드가 포함될 수 있는 위험을 줄이기 위해서입니다. 아울러 이 형식은 효율적인 직렬화 및 압축 알고리즘으로 대용량 텐서의 크기를 줄여 줍니다.

::: info

세이프텐서에 대한 자세한 내용은 [<VPIcon icon="iconfont icon-huggingface"/>`safetensors`](https://huggingface.co/docs/safetensors/index), [<VPIcon icon="iconfont icon-huggingface"/>세이프텐서 로드](https://huggingface.co/docs/diffusers/main/ko/using-diffusers/using_safetensors), [<VPIcon icon="fas fa-globe"/>세이프티 센서 소개](https://ai.atsit.in/posts/458772629/) 문서를 참고해 보세요.

:::

![세이프텐서 형식의 내부 구조 <br/><출처: [<VPIcon icon="iconfont icon-huggingface"/>허깅페이스](https://huggingface.co/docs/safetensors/index)>](https://yozm.wishket.com/media/news/2751/image13.png)

하지만 세이프텐서 형식의 모델을 Ollama에서는 사용할 수 없습니다. Ollama에서 주로 추가해서 사용하는 파일 형식은 GGUF(GPT-Generated Unified Format)입니다.

2023년 8월 공개된 GGUF는 딥러닝 모델을 단일 파일로 저장하기 위해 사용됩니다. GGUF 등장 이전에는 GGML(GPT-Generated Model Language)이라는 파일 형식을 주로 활용했습니다. 다만 GGML은 모델에 추가 정보를 넣기 어렵다는 점, 모델 간의 호환성 문제, 그리고 rope-freq-base와 같은 값을 자주 바꿔줘야 하는 문제 등이 있었습니다. GGUF는 이를 개선한 형식으로 현재 시점에서 모델 저장을 위해 널리 쓰이고 있습니다.

::: info

GGML과 GGUF 모두 [Georgi Gerganov(<VPIcon icon="iconfont icon-github"/>`ggerganov`)](https://github.com/ggerganov)의 주도로 만들어졌습니다. 두 가지 형식의 상세한 차이가 궁금한 분은 [<VPIcon icon="fa-brands fa-medium"/>문서1](https://medium.com/@sandyeep70/ggml-to-gguf-a-leap-in-language-model-file-formats-cd5d3a6058f9), [<VPIcon icon="iconfont icon-ibm"/>문서2](https://ibm.com/think/topics/gguf-versus-ggml)를 참고하세요.

:::

GGUF 파일은 아래 이미지와 같은 세부 정보를 1개의 파일에 모두 담고 있습니다.

![GGUF 파일이 담고 있는 세부 정보 <br/><출처: [ggerganov 깃허브 (<VPIcon icon="iconfont icon-github"/>`ggerganov/ggml`)](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)>](https://yozm.wishket.com/media/news/2751/image6.png)

따라서 Ollama에서 세이프텐서 형식의 모델을 쓰려면 GGUF로 변환하는 과정이 필요합니다. 변환을 위해서는 GGUF 형식 개발을 주도한 개발자가 직접 만든 [llama.cpp (<VPIcon icon="iconfont icon-github"/>`ggerganov/llama.cpp`)](https://github.com/ggerganov/llama.cpp.git) 프로젝트의 도움을 받는 것이 편리합니다. 특히 해당 프로젝트는 단순히 GGUF 변환 뿐만 아니라 다양한 종류의 양자화(Quantization)도 제공하고 있어 CPU와 메모리가 부족한 기기에서도 모델이 동작할 수 있게 해줍니다.

이제 본격적으로 변환 과정을 따라가 보겠습니다.

우선 가장 먼저 `llama.cpp` 프로젝트를 `git clone` 명령어로 로컬에 내려받습니다.

![](https://yozm.wishket.com/media/news/2751/5.png)
<!-- TODO: Google Lens -->

곧이어 llama.cpp 프로젝트 구동을 위한 파이썬 관련 패키지를 `pip3`로 설치합니다.

![](https://yozm.wishket.com/media/news/2751/6.png)
<!-- TODO: Google Lens -->

설치가 끝나면 llama.cpp에 미리 작성된 `convert_hf_to_gguf.py`를 이용해 f16(float16) 수준의 gguf를 생성합니다. 하단의 `2.f16-modes` 디렉터리는 미리 만들어져 있습니다. 단순 파일로도 변환할 수 있습니다.

![](https://yozm.wishket.com/media/news/2751/7.png)
<!-- TODO: Google Lens -->

생성된 gguf 파일과 그 파일의 사이즈를 확인합니다.

![](https://yozm.wishket.com/media/news/2751/8.png)
<!-- TODO: Google Lens -->

문제가 없다면, 양자화(Quantization)를 진행해 보겠습니다. 다만 `convert_hf_to_gguf.py` 명령에서는 양자화 옵션을 `f32,f16, bf16, q8_0, auto`만 제공합니다. 좀 더 나은 양자화 방법이 필요해 보입니다.

![](https://yozm.wishket.com/media/news/2751/9.png)
<!-- TODO: Google Lens -->

---

## 3단계: f16 GGUF 모델을 Q5_K_M으로 양자화

양자화(Quantization)란 모델의 성능을 일정 수준 유지하면서도 사이즈와 동작에 필요한 리소스를 효과적으로 줄이기 위해 가장 많이 쓰이는 방법 중 하나입니다. 현재 기본값인 f16은 용량이 다소 큰 편으로 허깅페이스에서 배포된 모델은 여러 종류의 양자화 모델을 함께 제공합니다.

![second-state에서 제공하는 다양한 종류의 양자화 gguf 모델 <br/><출처: 허깅페이스, [<VPIcon icon="iconfont icon-huggingface"/>second-state](https://huggingface.co/second-state/Mistral-Nemo-Instruct-2407-GGUF/tree/main)>](https://yozm.wishket.com/media/news/2751/image21.png)

양자화로 모델의 비트가 낮아지면 일반적으로 정확도가 내려갑니다. 하지만 그만큼 용량이 줄어 들어 모델을 동작할 때 필요한 리소스를 줄일 수 있습니다.

허깅페이스에서 배포되는 대부분 모델은 README 문서에서 레벨을 Q5라고 소개합니다. 그 외 K_M, K-L 등 [<VPIcon icon="fas fa-globe"/>표기](https://wikidocs.net/251903)로 레벨을 더 자세히 알려 줍니다. 여기서 K는 양자화에 K-평균 알고리즘(means clustering)을 활용했음을, 그 다음 대문자는 모델의 사이즈를 의미합니다. S(Small)와 M(Medium), 그리고 L(Large)을 사용합니다.

::: info 

양자화에 대해서는 이 [<VPIcon icon="fas fa-globe"/>문서](https://tulip-phalange-a1e.notion.site/a947f0efb8eb4813a533b0d957134f6d)도 함께 보면 좋습니다.

:::

양자화 모델 중 주로 사용하는 것은 `Q5_K_M`입니다. 언어 모델의 주요 성능 지표인 펄플렉시티(perplexity, PPL)*이 허용 가능한 수준이며 모델의 사이즈도 적게 만든 모델이기 때문입니다.

::: info 

[<VPIcon icon="fas fa-globe"/>펄플렉시티(perplexity, PPL)](https://wikidocs.net/21697): 문장을 생성할 때 선택해야 하는 가짓수가 많을수록 모델은 혼란을 겪습니다. 잘 학습된 모델일수록 이러한 가짓수가 적습니다. 따라서 모델의 혼란스러움을 나타내는 수치가 낮은 것, 즉 펄플렉시티가 낮을수록 성능이 좋다고 볼 수 있습니다.

:::

![양자화에 따라 모델의 사이즈를 줄이면 펄플렉시티가 높다는 것을 보여주는 그래프 <br/><출처: [llama.cpp 깃허브 (<VPIcon icon="iconfont icon-github"/>`ggerganov/llama.cpp`)](https://github.com/ggerganov/llama.cpp/pull/1684)>](https://yozm.wishket.com/media/news/2751/image9.png)

따라서 이번에도 역시 기본 제공 양자화 외에 추가로 작업해 Q5_K_M를 만들어 보겠습니다. 이를 위해 llama.cpp 디렉터리에서 `make`를 실행합니다. 해당 명령으로 플랫폼에 맞는 다양한 명령어가 생성되는데, 그 중 `llama-quantize`를 사용하겠습니다. 참고로 새로 생성되는 명령어는 `llama-`로 시작됩니다.

![](https://yozm.wishket.com/media/news/2751/10.png)
<!-- TODO: Google Lens -->

`llama-quantize` 명령어를 실행하기 전에 `--help` 명령어로 허용되는 양자화 타입을 확인합니다. `Q5_K_M`이 존재하는 것을 확인합니다.

![](https://yozm.wishket.com/media/news/2751/11.png)
<!-- TODO: Google Lens -->

f16으로 양자화 되어 있는 gguf 파일을 `Q5_K_M`으로 변환했습니다. 변환 후에 줄어든 용량도 확인해 보겠습니다.

![](https://yozm.wishket.com/media/news/2751/12.png)
<!-- TODO: Google Lens -->

---

<SiteInfo
  name="Ollama에 없는 모델 내가 만들어 사용하기 (2) | 요즘IT"
  desc="K8sGPT에 로컬 AI 제공자를 만들어 적용하기 위해 Ollama의 create 명령으로 내가 필요한 모델을 적용하는 방법을 알아보고 있습니다. 앞서 1부 글에서는 모델을 내려받아 Ollama에 활용하기 좋게 변환하는 작업을 진행했습니다. 이번 2부 글에서는 본격적으로 Ollama create 명령을 활용해 모델을 추가하는 것부터 시작합니다. 꼭 K8sGPT를 활용하지 않더라도 Ollama 활용법에 관심이 있는 분들에게 도움이 될 것으로 기대합니다."
  url="https://yozm.wishket.com/magazine/detail/2752/"
  logo="https://yozm.wishket.com/favicon.ico"
  preview="https://yozm.wishket.com/media/news/2752/image5.png"/>

[K8sGPT (<VPIcon icon="iconfont icon-github"/>`k8sgpt-ai/k8sgpt`)](https://github.com/k8sgpt-ai/k8sgpt)는 쿠버네티스와 대규모언어모델(이하 LLM)을 결합해 만들어진 오픈소스 프로젝트입니다. 쿠버네티스 클러스터의 정보를 스캐닝하여 이슈를 발견하고, 그 이슈의 원인과 결과, 해결 방안을 알려 줍니다.

이 프로젝트는 처음에는 오픈소스 LLM과 함께 시작했지만, 이제 직접 접근하고 관리할 수 있는 AI 제공자를 구성해 활용하는 기능을 제공합니다. 따라서 데이터가 노출되는 것을 우려하거나 환경에 따라 또는 토큰을 크게 쓰고 싶은 경우, 로컬에서 LLM 환경을 만들고 이를 적용할 수 있습니다.

::: note 참고 글

```component VPCard
{
  "title": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기 (1,2) | 요즘IT",
  "desc": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기",
  "link": "/yozm.wishket.com/2515.md",
  "logo": "https://yozm.wishket.com/favicon.ico", 
  "background": "rgba(84,7,224,0.2)"
}
```

:::

구성을 위해 필요한 것은 AI 모델의 실행과 해당 모델에게 요청할 API 주소, 두 가지입니다. 이때 권장하는 도구는 바로 [<VPIcon icon="fas fa-globe"/>Ollama](https://ollama.com/)입니다. 이번 기고에서는 Ollama의 create 명령을 사용해 내가 필요한 모델을 활용하는 방법을 알아보겠습니다. 꼭 K8sGPT를 활용하지 않더라도 Ollama 활용법에 관심이 있는 분들에게 도움이 될 것으로 기대합니다. 작업 순서는 아래와 같습니다.

> 1. 공개된 모델 내려받기
> 2. 파이토치(PyTorch) 프레임워크를 사용한 모델을 GGUF 포맷으로 변환하기
> 3. 기본 f16으로 생성된 GGUF 모델을 Q5_K_M으로 양자화
> 4. Ollama의 create 명령을 사용해 모델 추가하기
> 5. Ollama에 추가된 모델을 K8sGPT 분석에 사용하기

![Ollama 로고를 응용한 그림 <br/><출처: [<VPIcon icon="fas fa-globe"/>Ollama 블로그](https://ollama.com/blog/tool-support)>](https://yozm.wishket.com/media/news/2752/image5.png)

---

## 4단계: GGUF 파일을 Ollama에 불러 사용하기

Ollama에서는 GGUF를 간단하게 불러 곧바로 사용할 수 있습니다. 하지만 더 잘 활용하고 싶은 이들을 위해 모델 파라미터를 변경하고 어느 정도 프롬프트 엔지니어링을 할 수 있는 옵션도 제공됩니다. 이번에는 이 옵션을 함께 활용해 보겠습니다.

우선 양자화된 GGUF 파일을 작업할 디렉터리를 만들어 옮깁니다. 해당 디렉터리의 이름은 현재 모델을 제공한 조직의 이름으로 했습니다.

![](https://yozm.wishket.com/media/news/2752/1.png)
<!-- TODO: Google Lens -->

작업할 디렉터리로 이동해 모델파일(modelfile)을 추가합니다. 모델파일은 Ollama에 모델을 추가하며 변경할 수 있는 다양한 설정을 제공합니다.

![](https://yozm.wishket.com/media/news/2752/2.png)
<!-- TODO: Google Lens -->

### 모델파일(modelfile)

Ollama에서는 모델파일이라는 형식으로 모델을 추가합니다. 이때 세부 설정이나 프롬프트 엔지니어링에 가까운 항목을 추가할 수 있습니다.

[공식 홈페이지 (<VPIcon icon="iconfont icon-github"/>`ollama/ollama`)](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)에서 설명하는 핵심 구문은 다음과 같습니다.

```modelfile
FROM llama3
# sets the temperature to 1 [higher is more creative, lower is more coherent]

PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are Mario from super mario bros, acting as an assistant.
```

`FROM`에는 GGUF 형식 모델 등 추가하고자 하는 모델의 경로와 이름을 넣습니다.

`PARAMETER`에서는 모델의 세부 설정을 할 수 있습니다.

- `temperature`는 모델이 얼마나 창의적으로 답변할 것인지 결정하는 부분입니다. 0~1까지 설정할 수 있으며 높을 수록 창의적인 답변을 내놓습니다. 기본 값은 0.8입니다.
- `num_ctx`는 그 전의 답변을 얼마나 기억해 다음 답변에 반영할 것인지 결정하는 부분입니다. 값을 높게 줄수록 하드웨어 및 성능에 영향을 줍니다. 기본 값은 2048입니다.
- `num_predict`는 모델이 답변에 사용할 수 있는 토큰의 수를 의미합니다. 기본 값은 128입니다.

이 구문에서는 그 외에도 다양한 파라미터를 제공합니다.

`SYSTEM`에서는 시스템의 역할을 부여해 모델의 답변을 어느 정도 유도할 수 있습니다. [<VPIcon icon="fa-brands fa-reddit"/>예를 들면](https://reddit.com/r/ollama/comments/1dn7mx0/prompt_for_llama_3_8b_instruct/) 다음과 같은 형태로도 넣을 수 있습니다.

```modelfile
SYSTEM """You are an assistant designed for question-answering tasks. Use the provided pieces of retrieved context to answer the questions accurately. Ensure that your answers are directly relevant to the questions and cite your sources when possible. Keep your responses concise and within three sentences."""
```

그 외 `TEMPLATE`은 Ollama에서 지정한 프롬프트를 위한 템플릿으로, 이는 각 모델마다 다를 수 있습니다. 따라서 모델 제공자의 지시를 따르거나 문법을 충분히 이해하고 고쳐야 합니다. 이에 대한 문법은 이 [<VPIcon icon="fas fa-globe"/>문서](https://pkg.go.dev/text/template)에서 제공하고 있습니다.

다시 실습으로 돌아가, 우리의 모델파일을 만들어 보겠습니다. 모델파일을 만드는 다양한 방법이 있지만, 무에서 유를 창조하는 것보다 조금 더 쉬운 방법으로 진행하겠습니다. 우선 현재 모델의 기준인 `llama3:8b`를 내려받습니다.

![](https://yozm.wishket.com/media/news/2752/3.png)
<!-- TODO: Google Lens -->

해당 모델에서 사용하고 있는 모델파일을 `ollama show <모델> --modelfile` 명령으로 추출합니다. 참고로 `--modelfile`을 넣지 않으면 양자화 정보 등을 확인할 수 있습니다.

![](https://yozm.wishket.com/media/news/2752/4.png)
<!-- TODO: Google Lens -->

기본 모델파일에서 확인된 정보를 바탕으로 만든 모델파일은 아래와 같습니다.

![](https://yozm.wishket.com/media/news/2752/5.png)
<!-- TODO: Google Lens -->

`FROM` 부분을 변경하고 `SYSTEM`에 영어와 중국어로 시스템 메시지를 넣었습니다. 여기서 중국어를 추가하지 않으면 중국어에 대한 답변이 나오지 않거나 영어로 답변할 가능성이 높습니다.

이제 세부 설정까지 모두 조정했으니 모델을 추가할 차례입니다. `ollama create` 명령을 사용해 GGUF로 생성한 모델을 추가합니다. 세부 설정은 새로 만든 모델파일에서 가지고 오도록 `-f` 옵션을 사용합니다.

![](https://yozm.wishket.com/media/news/2752/6.png)
<!-- TODO: Google Lens -->

Ollama에 추가된 모델을 확인해 보겠습니다. 우리가 추가한 모델이 제대로 올라와 있습니다.

![](https://yozm.wishket.com/media/news/2752/7.png)
<!-- TODO: Google Lens -->

---

## 5단계: Ollama에 추가된 모델을 K8sGPT 분석에 사용하기

해당 모델을 활용해 K8sGPT에 질문해 봅시다. 간략한 활용을 위해 직접 만든 <VPIcon icon="iconfont icon-shell"/>`run_ollama_n_k8sgpt.sh` 명령으로 질문을 시작했습니다. K8sGPT에서 로컬 AI 모델을 활용하는 localai 기능은 앞서 작성한 글을 참고할 수 있습니다. (`run_ollama_n_k8sgpt.sh`에 대해서는 요청이 많으면 추후에 설명을 고려해 보겠습니다. 간단히 말하면, K8sGPT의 localai 기능을 반복해 사용하기 번거로워 스크립트로 처리했다고 볼 수 있습니다.)

::: note 참고 글

```component VPCard
{
  "title": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기 (1,2) | 요즘IT",
  "desc": "로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기",
  "link": "/yozm.wishket.com/2515.md",
  "logo": "https://yozm.wishket.com/favicon.ico", 
  "background": "rgba(84,7,224,0.2)"
}
```

:::

![](https://yozm.wishket.com/media/news/2752/8.png)
<!-- TODO: Google Lens -->

원하는 대로 작동하는 것을 확인했습니다.

---

## 마치며

지금까지 Ollama에 존재하지 않는 모델을 추가해 K8sGPT에서 사용하는 방법을 확인해 보았습니다.

결괏값에서 나타나듯 사실 대부분의 모델은 파운데이션 모델(대표적인 공개 모델은 llama, gemma, mistral 등)을 그대로 사용하는 것보다 좋은 결과(낮은 펄플렉시티 값)를 만들기 어렵습니다. 그럼에도 최근 K8sGPT처럼 특정 목적을 가진 sLM이 요구되는 만큼, 모델을 직접 붙여 활용해야 하는 시점이 곧 올 것으로 보입니다. 따라서 이와 연관된 기술들을 살펴보고 준비할 필요가 있습니다.

이번 글에서 소개한 내용을 바탕으로 KubeCon China 2024에서 [<VPIcon icon="fas fa-globe"/>발표](https://kccncossaidevchn2024.sched.com/event/1eYZI)하기도 했습니다. 더 자세한 내용이 궁금한 분은 [<VPIcon icon="fa-brands fa-youtube"/>발표 영상](https://youtu.be/xy7tJTIfyg8)을 살펴볼 수 있습니다.

<VidStack src="youtube/xy7tJTIfyg8" />

