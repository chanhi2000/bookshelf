---
lang: ko-KR
title: 로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기
description: Article(s) > 로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기
icon: iconfont icon-k8s
category: 
  - Kubernetes
  - VM
  - OpenAI
  - ChatGPT
  - Google
  - Cohere
  - LLM
  - Ollama
tag: 
  - blog
  - yozm.wishket.com
  - kubernetes
  - google
  - google-ai-studio
  - cohere
head:
  - - meta:
    - property: og:title
      content: Article(s) > 로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기
    - property: og:description
      content: 로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기
    - property: og:url
      content: https://chanhi2000.github.io/bookshelf/yozm.wishket.com/2515.html
prev: /devops/k8s/articles/README.md
date: 2024-03-27
isOriginal: false
cover: https://yozm.wishket.com/media/news/2515/image4.png
---

# {{ $frontmatter.title }} 관련

```component VPCard
{
  "title": "Kubernetes > Article(s)",
  "desc": "Article(s)",
  "link": "/devops/k8s/articles/README.md",
  "logo": "https://chanhi2000.github.io/images/ico-wind.svg",
  "background": "rgba(10,10,10,0.2)"
}
```

[[toc]]

---

<SiteInfo
  name="로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기 (1) | 요즘IT"
  desc="쿠버네티스는 계속 고도화되고 있어서 이를 분석하고 조치하는 것은 다양한 기반 지식을 필요로 합니다. 작년에 이어 올해도 인기 있는 인공지능(AI, Artificial Intelligence)을 이용해서 쿠버네티스를 분석하고 이에 맞는 조치를 할 수 있습니다."
  url="https://yozm.wishket.com/magazine/detail/2515/"
  logo="https://yozm.wishket.com/static/renewal/img/global/gnb_yozmit.svg"
  preview="https://yozm.wishket.com/media/news/2515/image4.png"/>

쿠버네티스는 계속 고도화되고 있어서 이를 분석하고 조치하는 것은 다양한 기반 지식을 필요로 합니다. 작년에 이어 올해도 인기 있는 인공지능(AI, Artificial Intelligence)을 이용해서 쿠버네티스를 분석하고 이에 맞는 조치를 할 수 있습니다.

해당 프로젝트의 이름은 K8sGPT로, 이미 1차례 요즘IT에서 대략 1년 전 “[ChatGPT로 쿠버네티스 관리하는 방법](1990.md)”이라는 글을 통해 소개된 적이 있습니다. 간단한 사용성에 대해서 소개하는 글이었는데, 이 글에서 그에 더해 추가적으로 설명하고자 하는 것은 다음과 같습니다.

1. 1년간의 변화
2. 공개된 AI 제공자를 사용하는게 아닌 나만의 AI 제공자를 사용하는 법
3. K8sGPT의 미래 전망

![K8sGPT 로고(출처: [<FontIcon icon="iconfont icon-github"/>`k8sgpt-ai/k8sgpt`](https://github.com/k8sgpt-ai/k8sgpt))](https://yozm.wishket.com/media/news/2515/image4.png)

---

## 1년간의 변화

K8sGPT의 v0.0.1은 2023년 3월에 시작되었습니다. 그리고 이 글이 작성되는 2024년 3월 (그러고 보니 딱 1년이 지났네요?)의 버전은 v0.3.28이 되었습니다.

### 다양한 서비스 제공자

1년간의 주요한 변화 중 하나는 K8sGPT가 질의 구문을 요청할 수 있는 제공자(Provider)가 매우 많아졌다는 것입니다.

![K8sGPT의 인증을 통해서 서비스를 제공할 수 있는 API 리스트](https://yozm.wishket.com/media/news/2515/1_iru0ARX.png)

v0.3.28에는 `googlevertexai`가 추가되었고, v0.3.27에는 `huggingface`가 추가되었습니다. 이러한 빠른 AI 서비스 제공자와의 통합을 통해서 성장 속도는 더더욱 가속화 되고 있습니다. 우선 간단하게 현재 상태를 점검해 보겠습니다. 점검을 위해서 구글클라우드플랫폼에서 제공하는 마이크로 서비스 아키텍처 앱인 “Online Boutine”를 배포하겠습니다.

![구글클라우드플랫폼에서 제공하는 “[<FontIcon icon="iconfont icon-github"/>Online Boutine](https://github.com/GoogleCloudPlatform/microservices-demo)” 애플리케이션(출처 :[<FontIcon icon="iconfont icon-github"/>Online Boutine](https://github.com/GoogleCloudPlatform/microservices-demo))](https://yozm.wishket.com/media/news/2515/image2.png)

![실행 결과는 다음과 같습니다.](https://yozm.wishket.com/media/news/2515/2_NgxMTnp.png)

![배포가 끝났다면 이제 `k8sgpt analyze`를 실행해서 나오는 결과를 통해서 현재 문제가 없는 상태인지 확인합니다.](https://yozm.wishket.com/media/news/2515/3.png)

::: tip 노트

설치는 `brew`, `rpm`, `deb` 등 다양한 방법을 제공합니다. 다음[링크](https://docs.k8sgpt.ai/getting-started/installation/)에서 확인하세요)

:::

현재는 문제가 없는 상태입니다. 따라서 AI가 문제를 분석할 수 있도록 문제를 만들어 봅시다. 예제로 괜찮은 문제는 서비스는 존재하지만 파드가 없는 상태입니다. 

![이 상태로 만들기 위해 frontend 애플리케이션을 `0`으로 바꿔서 서비스를 받아줄 파드가 없는 상태로 만듭니다.](https://yozm.wishket.com/media/news/2515/4.png)

![그리고 문제가 발생한 상태에서 다시 `k8sgpt analyze`를 실행합니다.](https://yozm.wishket.com/media/news/2515/5.png)

사실 아직까지는 AI의 도움을 받아서 분석한 상태는 아닙니다.

![따라서 AI의 도움을 받기 위해서 `--explain` 옵션과 `--backend openai`를 추가 입력하고 결과를 확인합니다.](https://yozm.wishket.com/media/news/2515/6_2beWtS4.png)

::: tip 노트

기본 값이 `--backend openai` 입니다. 그리고 인증 절차는 이미 처리되었습니다.

인증은 다음의 [<FontIcon icon="fas fa-globe"/>링크](https://docs.k8sgpt.ai/reference/providers/backend/)를 참고하세요

:::

아마 저 뿐만 아니라 openai를 사용하는 많은 무료 사용자는 위와 같은 메시지를 자주 만나게 될 것입니다. 따라서 기본 제공자인 opanai가 아니라 다른 AI 제공자를 선택해야 하는데 가장 무난한 선택지는 google(제미나이)와 [<FontIcon icon="fas fa-globe"/>cohere](https://cohere.com/)입니다. 각 제공자는 무료 사용자도 간단한 테스트를 할 수 있는 수준으로는 서비스를 제공합니다. 각 제공자에 대한 인증 설정은 계속 변화하기 때문에 위에서도 언급한 다음의 [<FontIcon icon="fas fa-globe"/>링크](https://docs.k8sgpt.ai/reference/providers/backend/)를 참고하는 것이 좋습니다.

![구글 AI 스튜디오](https://yozm.wishket.com/media/news/2515/image3.png)

![Cohere에서 제공하는 API 키](https://yozm.wishket.com/media/news/2515/image1.png)

해당 설정을 완료했다면 이제 백엔드(Backend) 제공자를 google과 cohere로 변경하여 결괏값을 비교해 보도록 하겠습니다. 각 제공자마다 알려주는 방법과 내용은 유사하지만 문맥과 흐름은 다소 차이가 있는 것을 확인할 수 있습니다.

![백엔드를 google로 한 결과값](https://yozm.wishket.com/media/news/2515/7.png)

![백엔드를 cohere로 한 결과값](https://yozm.wishket.com/media/news/2515/8.png)

이렇게 영어로 나오는 결과는 아무래도 한국어보다는 불편합니다. 따라서 한국어로 어떻게 설명해 주는지 확인해 보기 위해 옵션`--language`를 사용합니다. 한국어 뿐만 아니라 스페인어, 프랑스어, 독일어, 이탈리아어, 포르투갈어, 네덜란드어, 러시아어, 중국어, 일본어를 제공합니다.

![google 백엔드 상태에서 언어를 한국어로 선택한 결괏값](https://yozm.wishket.com/media/news/2515/9_kI0xqpR.png)

![cohere 백엔드 상태에서 언어를 한국어로 선택한 결괏값](https://yozm.wishket.com/media/news/2515/10.png)

위의 결괏값을 미루어보아 한국어와 같은 다른 언어를 사용하여 질의하는 경우 영어의 결과를 단순히 번역하는 것이 아니라 새로운 결과를 만들어 내려고 하며, 아무래도 한국어를 기반으로 좀 더 많은 데이터를 학습한 것으로 알려져 있는 google이 더 자연스러운 응답을 보여주는 것으로 확인되었습니다. 이와 같은 분석 외에도 인공지능이 현재처럼 유명해진 계기가 된 대화(Chat)를 기반으로 진행할 수도 있습니다.

---

## 상호 작용 (Interactive) 모드

상호 작용 모드는 v0.3.26(출시일: 2024년 1월)부터 지원하기 시작했습니다. 이는 ChatGPT와 같이 대화를 기반으로 좀 더 상세한 내용을 질의하거나 연관된 내용을 물어보기 위해서 사용될 수 있습니다.

현재 사용 가능한 google과 cohere에 `--interative` 옵션을 써서 관련 내용을 물어보도록 하겠습니다. 가장 처음으로 google 백엔드에 상호 작용 모드로 진입한 이후에 좀 더 자세한 내용 설명을 요청(**Please let me know more detail about it**)하고 나오는 결괏값을 확인합니다.

![`--interactive` 옵션을 이용해서 상호 작용 모드로 진입하고 원하는 내용을 입력](https://yozm.wishket.com/media/news/2515/11.png)

처음 내용보다 훨씬 더 상세한 내용이 입력된 것을 확인할 수 있습니다.

![이번에는 현재 내용을 **한국어로 설명**해 달라고 입력합니다.](https://yozm.wishket.com/media/news/2515/12_QZuvVLx.png)

일부 오류가 있긴 하지만 읽고 이해할 수 있는 수준의 한국어가 출력되는 것을 확인할 수 있습니다.

![마지막으로 현재 분석된 내용이 아닌 **현재 클러스터의 버전을 확인해 달라는 문구**(Could I get the current kubernetes version in this cluster?)를 입력합니다.](https://yozm.wishket.com/media/news/2515/13.png)

현재 발생한 문제와 연관이 없는 내용은 답변하지 않는 것을 확인할 수 있습니다. 

![확인하고자 하는 내용을 완료했으니 `exit`를 입력해서 상호 작용 모드를 나옵니다.](https://yozm.wishket.com/media/news/2515/14.png)

google 말고 다른 AI는 어떻게 상호 작용되는지 확인하기 위해 cohere로 바꿔서 다시 테스트해 보겠습니다.

![AI만 바뀔 뿐 입력되는 구문 및 상황은 동일합니다.](https://yozm.wishket.com/media/news/2515/15.png)

![google과 다르게 대화하는 느낌으로 여러 가지 가능성에 대해서 설명해 주고 있습니다. 그렇다면 한국어로는 설명해 달라고 하면 어떻게 다를까요?](https://yozm.wishket.com/media/news/2515/16.png)

한국어로 설명이 나오지도 않고 한국어와 출력 코드가 안 맞는지 알 수 없는 내용만 출력됩니다.

![마지막으로 현재 장애와 직접적인 연관이 없는 현재 쿠버네티스 버전을 알려달라는 요청을 합니다.](https://yozm.wishket.com/media/news/2515/17.png)

google 때와 다르게 이번에는 버전을 확인할 수 있는 명령어와 예시를 들어서 설명해 줍니다.

![다음 진행을 위해서 `exit`를 치고 상호 작용 모드를 종료합니다.](https://yozm.wishket.com/media/news/2515/18.png)

이번 결과를 통해서 모델에 따라 답변이 변하는 것을 확인할 수 있고, 이에 따라 적합한 모델을 선택해야 하는 것이 더 원하는 답에 가까운 것을 얻을 수 있을 것이라는 부분을 예상할 수 있습니다. 이러한 변화 이외에 쿠버네티스 클러스터 분석 이외에 기능이 확장된 부분에 대해서 알아보겠습니다.

---

## 기능 확장

![v.0.3.28을 기준으로 다음과 같이 총 3개의 기능을 통합해서 확장할 수 있습니다.](https://yozm.wishket.com/media/news/2515/19.png)

각 기능은 다음과 같습니다.

- `trivy`: 쿠버네티스 클러스터의 보안 취약점 등을 알려줌
- `prometheus`: 프로메테우스가 설치되어 있는 환경에서 설정 등을 점검해줌
- `aws`: EKS와 같은 AWS의 리소스를 직접 분석해줌

이 중에서 가장 간편하게 기능을 확인할 수 있는 `trivy`를 통해서 기능을 어떻게 확장하고 어떤 이점을 얻을 수 있는지 확인해 보겠습니다. 

![`integrations` 옵션을 통해 `trivy`를 우선 활성화시키겠습니다.](https://yozm.wishket.com/media/news/2515/20_6DQ2d0Q.png)

![기능이 활성화 되었는지 확인하기 위해서 `filters list` 명령을 입력하고 integration된 `VulnerabilityReport`와 `ConfigAuditReport`를 확인합니다.](https://yozm.wishket.com/media/news/2515/21_aopySgO.png)

![활성화된 필터인 `VulnerabilityReport`를 통해서 현재 클러스터의 취약점을 분석해 봅니다. 이를 위해서 `--filter VulnerabilityReport` 을 추가합니다.](https://yozm.wishket.com/media/news/2515/22_GtGf0tj.png)

AI 제공자가 설정되어 있지 않아 해당 하는 취약점에 해당하는 CVE(Common Vulnerabilities and Exposures)만 출력되는 것을 확인할 수 있습니다.

![따라서 제공자를 google 그리고 cohere로 입력하여 어떤 결괏값이 나오는지 확인합니다.](https://yozm.wishket.com/media/news/2515/23_GIpAfwO.png)

이번에는 cohere를 통해서 어떤 결괏값이 나오는지 확인합니다. 

![AI 서비스 제공자의 차이로 분석하는 형태가 완전히 다른 것을 알 수 있습니다.](https://yozm.wishket.com/media/news/2515/24_ZlMiebp.png)

취약점 점검 이외에 trivy를 통해서 현재 구성 상태를 감사(Audit)하도록 하겠습니다. 

![이때 사용되는 필터는 `ConfigAuditReport` 입니다. 일반적으로 취약점보다는 많은 내용이 출력됩니다.](https://yozm.wishket.com/media/news/2515/25_nVDvzvl.png)

분석된 내용을 기반으로 AI 제공자에게 설명을 요청했을 때 어떤 결과가 나오는지 확인해 보겠습니다.

우선 google로 진행하겠습니다.

![현재 배포된 `ReplicaSet`이 사용하는 default 네임스페이스에 `SECCOMP` 이 특정되어 있지 않다는 부분 외에 이게 어떤 의미를 가지는지, 어떻게 조치해야 하는지 등 많은 내용을 설명해 주고 있습니다.](https://yozm.wishket.com/media/news/2515/26_vetXeCQ.png)

cohere는 어떤 답변을 주는지 진행해 보겠습니다.

![cohere의 경우에는 trial 사용자가 1분에 5번 요청까지 처리하도록 허용하는데, 현재 44개의 요청이 진행되어야 하므로 진행 자체가 어려운 것을 확인할 수 있습니다.](https://yozm.wishket.com/media/news/2515/27.png)

이와 같이 겨우 1년 밖에 지나지 않았지만 매우 많은 발전과 개선이 있었음을 알 수 있습니다. 지금까지는 외부 AI 서비스 제공자에게 데이터를 보내고 설명을 요청하는 방식으로 이루어졌습니다. 이와 같은 방식은 꽤 많은 기업에서 많이 우려하고 있는 방식일 수 있습니다.

---

<SiteInfo
  name="로컬 LLM에서 K8sGPT로 쿠버네티스 AIOps 실행하기 (2) | 요즘IT"
  desc="앞서 K8sGPT를 통한 쿠버네티스 AIOps의 가능성 1부에서는 K8sGPT에서 지난 1년간 보여준 변화를 소개했습니다. 이번 글에서는 공개된 AI 제공자를 사용하는 게 아닌 '나만의 AI 제공자를 사용하는 법'을 소개하고 K8sGPT의 미래 전망에 대해 나름대로의 생각을 공유하도록 하겠습니다."
  url="https://yozm.wishket.com/magazine/detail/2516/"
  logo="https://yozm.wishket.com/static/renewal/img/global/gnb_yozmit.svg"
  preview="https://yozm.wishket.com/media/news/2515/image4.png"/>

## 공개된 AI 제공자를 사용하는 게 아닌 나만의 AI 제공자를 사용하는 법

기업의 데이터라면 어떤 종류든 유출되는 것이 매우 민감한 문제일 수 있습니다. 이에 따라 k8sGPT도 `--anonymize` 옵션을 통해서 민감한 정보가 유출되지 않도록 지원하지만, 그럼에도 불구하고 공개된 AI 제공자를 사용하는 것 자체에 우려가 있습니다.

::: info --anonymize

Anonymize data before sending it to the AI backend. This flag masks sensitive data, such as Kubernetes object names and labels, by replacing it with a key. However, please note that this flag does not currently apply to events.

:::

따라서 기업 입장에서는 기업에서 접근하고 관리할 수 있는 AI 제공자를 구성하고 사용할 필요가 있는데, 이를 위해서 K8sGPT는 `localai`라는 제공자를 설정할 수 있습니다. 이름은 local이지만 이를 활용하면 본인의 AI 서비스를 제공하는 위치에 있는 API에 호출해서 결과를 가지고 오는 구조 이므로 꼭 local 국한되지 않습니다.

자세한 내용은 실습을 통해서 확인하겠습니다!

### (사전 준비) 확장된 기능 비활성화

앞서 발행된 K8sGPT를 통한 쿠버네티스 AIOps의 가능성 1부에 이어, 2부를 진행한다면, 가장 먼저 필요한 작업은 기능을 확장했던 trivy를 비활성화 시키는 것입니다. v0.3.28까지는 기능을 확장한 경우 필터를 사용하지 않으면 정상적으로 사용하기 어렵습니다.

![기능을 확장한 경우에 `k8sgpt analyze`를 실행하면 오류 발생](https://yozm.wishket.com/media/news/2516/1.png)

![`k8sgpt analyze`를 실행 시 `--filter` 옵션을 추가하면 오류가 발생하지 않음](https://yozm.wishket.com/media/news/2516/2.png)

![따라서 우선 활성화시킨 trivy를 비활성(`deactivate`)화 시키겠습니다.](https://yozm.wishket.com/media/news/2516/3.png)

![비활성화한 후에 현재 활성화된 기능이 있는지 확인합니다.](https://yozm.wishket.com/media/news/2516/4.png)

---

## LLM 모델 구성 및 실행

K8sGPT에 나만의 AI 제공자를 구성하기 위해서는 우선 로컬에 LLM 모델이 구성 및 실행되어 있어야 합니다. 여기서 꼭 로컬일 필요는 없으나, K8sGPT가 요청할 수 있는 곳에 AI API가 동작하고 있어야 합니다. 이를 이해하기 위해서 간단히 K8sGPT가 어떻게 동작하는지를 다음의 그림으로 표현하였습니다.

![K8sGPT가 쿠버네티스 클러스터를 분석하고 설명하는 과정](https://yozm.wishket.com/media/news/2516/image4.png)

지금까지의 과정에서는 공개된 AI 서비스 제공자(google, cohere 등)가 설명을 진행해줬고, 이를 공개된 AI 서비스 제공자가 아닌 내가 직접 구성한 AI 서비스 제공자로 바꾸는 것이 핵심입니다. 이를 위해 K8sGPT는 `localai`라는 제공자를 가지고 있으며, 해당 제공자는 API 주소을 입력하면 여기에 질의하여 해당 결과를 반환해 줍니다.

이 과정을 진행하기 위해서 가장 먼저 필요한 것은 AI 서비스 제공자를 만드는 것입니다. 이것은 매우 복잡할 수도 있으나, 간단한 테스트를 위해서 저희가 필요한건 **AI 모델의 실행**과 해당 모델에게 요청할 수 있는 **API 주소** 2가지 입니다.

이 2가지를 손쉽게 구현할 수 있는 도구로는 [<FontIcon icon="fas fa-globe"/>ollama](https://ollama.com)가 있습니다. `ollam`는 로컬 환경에 대규모 언어 모델(LLM, Large Language Models)을 손쉽게 구현할 수 있도록 도와줍니다.

![우선 다음의 [사이트](https://ollama.com/download)를 통해서 현재 로컬 환경에 맞는 `ollama`를 설치합니다.](https://yozm.wishket.com/media/news/2516/image1.png)

![설치를 완료했다면 다음과 같이 `ollam`를 실행할 수 있게 됩니다.](https://yozm.wishket.com/media/news/2516/5.png)

`ollama`를 통해서 LLM를 실행하는 방법은 크게 2가지가 있습니다. (만약 도커에 익숙하시다면 유사한 느낌이 드실 겁니다.)

![첫 번째는 모델을 내려받고 저장할 수 있는 `pull`입니다. pull 명령을 통해서 모델을 내려받는 과정은 매우 간단하고 쉽습니다.](https://yozm.wishket.com/media/news/2516/6.png)

내려받은 모델은 다음의 위치에 저장됩니다.

- .<FontIcon icon="fa-brands fa-apple"/>macOS: `~/.ollama/models`
- .<FontIcon icon="fa-brands fa-linux"/>Linux: `/usr/share/ollama/.ollama/models`
- .<FontIcon icon="fa-brands fa-windows"/>Windows: `C:\Users\<username>\.ollama\models`

![내려받은 모델은 `ollama list`를 통해서 확인이 가능합니다.](https://yozm.wishket.com/media/news/2516/7.png)

내려받을 수 있는 대표 모델은 다음과 같습니다.

| Model | Parameters | Size | Download |
| :---: | :----: | :---: | :---- |
| Llama 2 | 7B | 3.8GB | `ollama run llama2` |
| Mistral | 7B | 4.1GB | `ollama run mistral` |
| Dolphin Phi | 2.7B | 1.6GB | `ollama run dolphin-phi` |
| Phi-2 | 2.7B | 1.7GB | `ollama run phi` |
| Neural Chat | 7B | 4.1GB | `ollama run neural-chat` |
| Starling | 7B | 4.1GB | `ollama run starling-lm` |
| Code Llama | 7B | 3.8GB | `ollama run codellama` |
| Llama 2 Uncensored | 7B | 3.8GB | `ollama run llama2-uncensored` |
| Llama 2 13B | 13B | 7.3GB | `ollama run llama2:13b` |
| Llama 2 70B | 70B | 39GB | `ollama run llama2:70b` |
| Orca Mini | 3B | 1.9GB | `ollama run orca-mini` |
| Vicuna | 7B | 3.8GB | `ollama run vicuna` |
| LLaVA | 7B | 4.5GB | `ollama run llava` |
| Gemma | 2B | 1.4GB | `ollama run gemma:2b` |
| Gemma | 7B | 4.8GB | `ollama run gemma:7b` |

> 출처: [<FontIcon icon="iconfont icon-github"/>`ollama/ollama`](https://github.com/ollama/ollama?tab=readme-ov-file#model-library)

![위의 모델 이외에도 [<FontIcon icon="fas fa-globe"/>ollama 라이브러리 사이트](https://ollama.com/library)를 통해서 검색도 가능합니다.](https://yozm.wishket.com/media/news/2516/image2.png)

두 번째는 모델을 내려받고 실행할 수 있는 `run`입니다. 이때 모델을 이미 내려받은 상태라면 내려받는 과정이 없이 바로 실행됩니다.

![가장 작은 모델 중에 하나인 `gemma:2b`를 실행하겠습니다.](https://yozm.wishket.com/media/news/2516/8.png)

이렇게 해서 모델이 실행되고 나면, openai 포맷으로 API 요청을 할 수 있습니다.

![테스트를 위해서 새로운 창을 열고 `curl` 명령을 통해서 실행합니다.](https://yozm.wishket.com/media/news/2516/9.png)

API 요청을 할 수 있는 LLM 모델 구성, 그리고 실행이 매우 손쉽게 완료되었습니다.

---

## K8sGPT에 `localai` 등록 후 실행

localai에 등록하는 과정도 또한 매우 쉽습니다. (인증은 1부에서 소개한 다음의 [링크](https://docs.k8sgpt.ai/reference/providers/backend/)를 참고하세요.)

![ollama를 통해서 실행된 모델에 접근하기 위한 주소인 <FontIcon icon="fas fa-globe"/>`http://localhost:11434/v1`를 `--baseurl`에 입력하고, ollama를 통해서 실행한 모델인 `gemma:2b` 이름을 넣습니다.](https://yozm.wishket.com/media/news/2516/10.png)

![그리고 등록된 localai 서비스 제공자를 확인합니다.](https://yozm.wishket.com/media/news/2516/11.png)

![이제 실제로 쿠버네티스 클러스터를 `localai`를 통해서 분석해 볼 차례입니다.](https://yozm.wishket.com/media/news/2516/12.png)

2B밖에 안되는 모델인 것을 고려했을 때 결과가 나쁘지 않게 나옵니다. 한국어로 답변을 줄 수 있는지 확인해 보겠습니다. 

![한국어로는 출력되지 않고 약간 내용이 바뀐 것을 확인할 수 있습니다.](https://yozm.wishket.com/media/news/2516/13.png)

다른 모델은 어떤 답변을 제공하는지 확인해 보는 게 좋을 것 같습니다. 현재 실습 가능한 모델 중에 매개변수가 가장 큰 것 중에 하나인 `llama2:70b` 모델을 동작시키겠습니다.

![모델을 바꾸기 위해서는 다른 모델을 우선 로드(`load`) 해야 합니다.](https://yozm.wishket.com/media/news/2516/14.png)

::: tip load 시에 모델이 없는 경우는?

![`llama2:70b`는 기존에 내려받아 두었기 때문에 `load` 명령이 실행되었던 것이고, 없는 모델을 load 한다면 다음과 같이 오류가 발생하게 됩니다.](https://yozm.wishket.com/media/news/2516/15.png)

:::

![새로운 모델을 `load`했다면 이번에는 `localai`에 `llama2:70b`를 다시 등록해야 합니다. 따라서 기존에 등록한 `localai`를 지우고 다시 등록합니다.](https://yozm.wishket.com/media/news/2516/16.png)

![등록이 완료되었다면, 다시 `localai`에 설명을 요청합니다.](https://yozm.wishket.com/media/news/2516/17.png)

한국어로도 물어봅니다. 

![내용이 약간 바뀌었을 뿐 한국어는 여전히 지원되지 않는 것을 확인할 수 있습니다.](https://yozm.wishket.com/media/news/2516/18.png)

![마지막으로 `localai`를 사용한 상호 작용 모드의 경우는 어떤지 확인해 보겠습니다.](https://yozm.wishket.com/media/news/2516/19.png)

이와 같이 한국어 지원은 다소 어려운 부분이 있지만, `localai`를 통해서 공개된 AI 서비스에 질의하지 않고도 현재 상황에 대해 설명을 받을 수 있는 것을 확인하였습니다.

`ollama`를 통해서 실행된 모델 별로 얻을 수 있는 내용의 특장점은 추후에 다시 알아보기로 하겠습니다.

---

## K8sGPT의 미래 전망

K8sGPT는 현재 쿠버네티스 클러스터에 존재하는 여러 가지 문제 가능성을 분석해주는 훌륭한 도구입니다. 이러한 분석을 위해 다양한 AI 서비스 제공자를 선택할 수 있고, 또한 단순히 쿠버네티스 클러스터 분석뿐만 아니라 기능을 통합해서 확장할 수 있도록 지원하고 있습니다. 다만 상호 작용 모드에서 제한적으로 동작하고, 한국어와 같이 사용률이 높지 않은 경우에는 모델에서 지원하지 않는 경우가 많아서 사용에 약간 제약이 있을 수 있습니다.

그럼에도 불구하고 K8sGPT라는 프로젝트는 매우 빠르게 성장하고 있고, 커뮤니티의 의견을 적극적으로 받아들여 많은 기능들이 빠르게 추가되고 있습니다. 기업이 공개된 AI제공자를 사용함으로써 정보가 공개되는 것을 우려하는 부분도 localai를 통해서 해소할 수 있을 것으로 보이며, localai를 응용해 다양한 기업 환경에 맞는 모델을 직접 K8sGPT에서 사용하도록 설정할 수도 있을 것입니다.

따라서 K8sGPT는 현재 LLM의 추세와 맞물려 2024년 현재에 가장 인기 있는 주제 중에 하나가 될 것으로 보입니다.. 이대로만 발전한다면 상호 작용 모드 또한 현재와 같이 다소 제한이 있는 형태가 아닌, 사용자가 원하는 다양한 정보를 얻을 수 있는 형태로 변화할 것으로 예상됩니다.

