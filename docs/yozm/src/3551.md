---
lang: ko-KR
title: "네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리"
description: "Article(s) > 네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리"
icon: fas fa-language
category:
  - AI
  - LLM
  - Article(s)
tag:
  - blog
  - yozm.wishket.com
  - ai
  - artificial-intelligence
  - llm
  - large-language-models
head:
  - - meta:
    - property: og:title
      content: "Article(s) > 네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리"
    - property: og:description
      content: "네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리"
    - property: og:url
      content: https://chanhi2000.github.io/bookshelf/yozm.wishket.com/3551.html
prev: /ai/llm/articles/README.md
date: 2026-01-15
isOriginal: false
author:
  - name: 신대리
    url : https://yozm.wishket.com/magazine/@holiymusic/
cover: https://yozm.wishket.com/media/news/3551/1__3_.png
---

# {{ $frontmatter.title }} 관련

```component VPCard
{
  "title": "LLM > Article(s)",
  "desc": "Article(s)",
  "link": "/ai/llm/articles/README.md",
  "logo": "/images/ico-wind.svg",
  "background": "rgba(10,10,10,0.2)"
}
```

[[toc]]

---

<SiteInfo
  name="네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리"
  desc="최근 IT 업계가 뜨겁게 달아올랐습니다. 바로 AI 국가대표 선발전 때문입니다. 정부는 글로벌 빅테크에 맞설 기술 주권을 확보하기 위해 수천억 원의 예산을 투입, ‘독자 AI 파운데이션 모델’ 구축 프로젝트를 시작했습니다. 네이버, SKT, 업스테이지 등 내로라하는 AI 기업들이 출사표를 던지며 기대감은 최고조에 달했습니다. 하지만 최종 AI 국가대표가 선발되기 전에 파열음이 들려왔습니다. 논란의 핵심은 참가 기업들의 모델이 ”프롬 스크래치(From Scratch)가 아니다”라는 의혹입니다. 도대체 ‘프롬 스크래치’가 무엇이길래 국가대표 자격 논란까지 번진 걸까요?"
  url="https://yozm.wishket.com/magazine/detail/3551/"
  logo="https://yozm.wishket.com/favicon.ico"
  preview="https://yozm.wishket.com/media/news/3551/1__3_.png"/>

::: info 국가대표 AI 선발전, 화려한 모델 성능 뒤의 잡음

최근 IT 업계가 뜨겁게 달아올랐습니다. 바로 AI 국가대표 선발전 때문입니다(공식 사업명: 독자 AI 파운데이션 모델 프로젝트). 정부는 글로벌 빅테크에 맞설 기술 주권을 확보하기 위해 수천억 원의 예산을 투입, ‘독자 AI 파운데이션 모델’ 구축 프로젝트를 시작했습니다. 네이버, SKT, 업스테이지 등 내로라하는 AI 기업들이 출사표를 던지며 기대감은 최고조에 달했습니다.

![출처: 과학기술정보통신부](https://wishket.com/media/news/3551/1__1_.png)

:::

하지만 최종 AI 국가대표가 선발되기 전에 파열음이 들려왔습니다. 논란의 핵심은 참가 기업들의 모델이 **"프롬 스크래치(From Scratch)가 아니다"**라는 의혹입니다. 즉, 맨바닥에서 우리 기술로 만든 것이 아니라 해외 오픈소스를 베끼거나 빌려왔다는 지적입니다. 막대한 혈세가 투입되는 사업인 만큼, 일각에서는 이를 무임승차로 규정하며 강도 높은 비판을 쏟아내고 있습니다.

도대체 ‘프롬 스크래치’가 무엇이길래 국가대표 자격 논란까지 번진 걸까요? 이것은 단순한 기업의 도덕적 해이일까요, 아니면 모호한 기준이 낳은 예고된 비극일까요? 지금부터 복잡한 기술 용어 대신 쉬운 비유를 통해 이 논란의 실체를 파헤쳐 봅니다.

---

## ‘프롬 스크래치’란 무엇인가: 요리와 달리기로 본 정의

'프롬 스크래치(From Scratch)'라는 말, 참 생소하시죠? 하지만 이 단어의 유래를 알면 이번 논란의 본질이 한눈에 들어옵니다. 원래 이 말은 스포츠 경기, 특히 달리기에서 유래했습니다. 과거에는 출발선이 따로 없어 땅바닥에 선(Scratch)을 긋고 경기를 시작했는데, **어떤 핸디캡이나 이점 없이 모두가 똑같은 '맨바닥'에서 출발한다는 뜻을 담고 있습니다.**

![출처: 네이버 영어사전](https://wishket.com/media/news/3551/1__2_.png)

이걸 요리에 비유하면 더 이해하기 쉽습니다. 만약 당신이 마트에서 케이크 믹스를 사 와서 우유만 붓고 굽는다면, 맛있는 케이크는 만들었을지 몰라도 **"처음부터(From Scratch) 만들었다"**고 하지는 않습니다. 반면, 밀가루를 직접 체치고, 계란을 풀고, 설탕과 버터의 비율을 연구해 반죽부터 시작했다면, 비로소 "프롬 스크래치로 요리했다"고 말할 수 있습니다.

AI 개발도 이와 똑같습니다. AI 모델을 만든다는 건 텅 빈 뇌(초기화된 신경망)에 수많은 책(데이터)을 읽혀 지능을 만드는 과정입니다. 여기서 남이 이미 똑똑하게 학습시켜 둔 가중치를 가져다 조금 고쳐 쓰는 건 시판 믹스를 쓰는 것과 같습니다. 쉽고 빠르고, 실패 확률도 적죠.

![출처: 나노바나나 프로](https://wishket.com/media/news/3551/1__3_.png)

하지만 **AI 분야에서 ‘프롬 스크래치'는 아무것도 모르는 백지상태에서 오직 우리의 데이터와 계산 능력만으로 학습을 시작하는 것**을 의미합니다. 정부가 이번 국대 선발전에서 굳이 이 힘든 길을 고집한 이유는 명확합니다. 남의 믹스를 쓰면 당장은 맛있는 빵을 빨리 만들 수 있겠지만, 믹스 공급이 끊기면 빵을 굽지 못하게 되니까요. 즉, 해외 빅테크에 휘둘리지 않는 진정한 AI 기술 독립, 즉, 소버린 AI를 위한 최소한의 자격 요건이었던 셈입니다.

---

## 세 가지 쟁점: 모듈, 코드, 그리고 가중치

이번 ‘국가대표 AI 선발전’의 논란이 복잡한 이유는 참여 기업들이 받고 있는 의혹의 기술적 층위(Layer)가 모두 다르기 때문입니다. **네이버는 ‘구성 요소’를, SK텔레콤은 ‘실행 코드’를, 업스테이지는 ‘학습 결과값(가중치)’을 두고 각기 다른 해석의 싸움을 벌이고 있습니다.** 2026년 1월 중순까지 밝혀진 사실을 바탕으로 각 사의 쟁점을 상세히 분석합니다.

### 1. 네이버: 비전 인코더(Vision Encoder)의 전략적 차용 논란

![출처: 네이버 HyperClova X, Hugging Face](https://wishket.com/media/news/3551/1__4_.png)

> "두뇌인 언어 모델은 국산이지만, 눈 역할을 하는 비전 인코더는 외부 기술을 채택했다."

네이버의 '하이퍼클로바X' 기반 멀티모달 모델(HyperCLOVA X 32B Think)은 구성 요소의 독립성이 쟁점입니다. 멀티모달 AI는 텍스트를 이해하는 거대언어모델(LLM)과 이미지를 처리하는 비전 인코더와 음성을 처리하는 음성 인코더가 결합해 작동하는데, 네이버는 비전과 음성 인코더에 중국 알리바바의 Qwen 2.5 기술을 사용했습니다.

- **검증된 사실**: 검증 결과, 비전 인코더가 Qwen 2.5 모델과 코사인 유사도 99.51%, 피어슨 상관계수 98.98% 이상으로 일치한다고 인정했습니다. 사실상 해당 모듈의 가중치를 그대로 가져와 사용한 것입니다.
- **네이버의 입장**: 네이버는 이를 통상 이루어지는 **전체 시스템의 효율을 위한 관행**이라고 설명합니다. 비전 인코더는 교체 가능한 모듈이며, 이미 VUClip 등 독자 기술도 보유하고 있어 언제든 교체가 가능하다는 것입니다. 글로벌 표준 기술을 활용해 호환성을 높이고, 핵심 역량인 LLM 학습에 집중하기 위한 선택이었다는 논리입니다.
- **남은 쟁점**: 기술적 효율성은 인정되나, 소버린 AI라는 국책 사업의 취지에 부합하느냐는 비판은 여전합니다. **"국가대표 모델이라면 데이터가 입력되는 첫 관문(눈)인 인코더까지 기술적 통제권을 가져야 한다"는 지적**과, **"핵심 경쟁력인 언어 지능이 독자적이라면 부품 차용은 문제없다"는 현실론이 맞서고 있습니다.**

![출처: 나노바나나 프로](https://wishket.com/media/news/3551/1__5_.png)

### 2. SK텔레콤: 아키텍처 모방이 아닌 '인퍼런스 코드' 차용

![출처: SKT](https://wishket.com/media/news/3551/1__6_.png)

> "학습은 백지에서 시작했으나, 구동을 위한 '인퍼런스 코드'가 유사했다."

초기에는 SK텔레콤의 'A.X' 모델(A.X K1)이 중국 DeepSeek의 아키텍처(설계도)를 통째로 베꼈다는 의혹이 있었으나, 1월 8일 공식 해명을 통해 논란의 핵심이 인퍼런스 코드로 좁혀졌습니다.

- **검증된 사실**: 논란이 된 유사성은 모델을 학습시키는 '설계도(아키텍처)' 자체가 아니라, 학습된 모델을 실행하고 검증하는 보조 도구인 인퍼런스 코드에서 발견되었습니다. SKT는 DeepSeek의 추론 효율성을 참고하기 위해 해당 코드를 활용했으나, 해당 코드는 인퍼런스 코드는 공개된 모델을 실행할 때 개발 편의를 위해 제공되는 지원용 코드라고 설명했습니다..
- **SK텔레콤의 입장**: SKT는 "DeepSeek의 MLA(Multi-Head Latent Attention) 방식을 참고하긴 했지만, 이를 그대로 쓴 것이 아니라 파라미터 규모를 키워 최적화했다"고 밝혔습니다. 이는 오히려 모델의 내부 구조를 깊이 이해하고 수정할 능력이 있음을 방증한다는 것입니다. 단순히 남의 모델을 가져왔다면 파라미터 규모를 자유롭게 조정하거나 최적화하는 것이 불가능하기 때문입니다.
- **남은 쟁점**: 업계 전문가들은 인퍼런스 코드는 모델의 성능(지능)과는 직접적 연관이 적은 실행 도구에 가깝기 때문에 프롬 스크래치 요건을 위반했다고 보기 어렵다는 쪽에 힘을 싣고 있습니다. 다만, 외부 코드를 차용하는 과정에서 오해를 살만한 유사성을 남긴 점은 아쉬움으로 지적됩니다.

![출처: 이승현 포티투마루 부사장 링크드인](https://wishket.com/media/news/3551/1__1_.jpg)

### 3. 업스테이지: 통계적 착시로 밝혀진 '가중치 재사용' 의혹

![](https://wishket.com/media/news/3551/1__7_.png)

<출처: Upstage Hugging Face>

업스테이지의 솔라 모델(solar-open-100b)은 타사 모델(Mistral, GLM)의 가중치를 그대로 가져왔다는 가장 치명적인 의혹을 받았으나, 1월 초 공개 검증을 통해 통계적 착시였음이 밝혀졌습니다.

- **검증된 사실**: 초기 의혹은 모델 간 '코사인 유사도'가 0.99에 달한다는 점 때문에 제기되었습니다. 그러나 검증 결과, 이는 최신 AI 모델들이 사용하는 'RMSNorm' 기술의 특성상 벡터의 방향이 자연스럽게 비슷해지는 현상일 뿐임이 드러났습니다. 결정적으로 모델의 진짜 유사성을 보여주는 '피어슨 상관계수'는 -0.0163(사실상 0)으로 나타났습니다. 이는 두 모델의 가중치 패턴이 전혀 상관관계가 없으며, 업스테이지가 독자적으로 학습했다는 명확한 증거입니다.
- **결과**: 이 수치는 업스테이지의 모델이 외부 모델의 파생형이 아니라, 완전히 독립적인 학습 과정을 거쳤음을 수학적으로 증명했습니다. 의혹을 제기했던 측에서도 분석 오류를 인정하고 사과문을 게시하며 논란은 일단락되었습니다.

![출처: 고석현 사이오닉에이아이 대표 링크드인](https://wishket.com/media/news/3551/1__8_.png)

- **시사점**: 이 사례는 고도화된 AI 모델들이 최적의 정답을 찾아가는 과정에서 구조적으로 비슷해지는 **수렴 진화** 현상을 표절로 오인할 수 있음을 보여준 대표적인 사례로 남게 되었습니다.

---

## 이번 논란의 진짜 문제는 기술이 아니라 기준이다

이번 논란을 단순히 기업들의 도덕적 해이나 기술력 부족으로 몰아가는 것은 위험합니다. 사태의 이면을 들여다보면, 선수들이 반칙을 했다기보다 심판이 그어 놓은 **출발선 자체가 희미했다는 근본적인 문제**가 드러납니다.

### 1. 심판 없는 달리기 경주: "어디까지가 우리 것인가?"

정부가 내건 ‘독자 파운데이션 모델’이라는 과제는 그 명분은 훌륭했으나, 구체적인 기술 가이드라인(RFP)은 놀라울 정도로 추상적이었습니다. 공고문에는 ‘새로운’, ‘독자적인’, ‘자체 기술’ 같은 정성적인 표현이 주를 이뤘을 뿐, 엔지니어들이 실제로 참고할 수 있는 정량적 기준은 부재했습니다.

![출처: 독자 AI 파운데이션 프로젝트 공고문](https://wishket.com/media/news/3551/1__9_.png)

- **코드 한 줄도 베끼면 안 되는가?** 아니면 아키텍처는 가져오되 학습만 새로 하면 되는가?
- **학습 데이터는 100% 국산이어야 하는가?** 아니면 해외 데이터셋을 섞어도 되는가?
- **보조 모듈(인코더 등)은 사다 써도 되는가?**

이 질문들에 대한 명확한 합의 없이 수천억 원의 예산이 집행되었습니다. 네이버는 "핵심 두뇌(LLM)가 우리 것이면 눈(비전 인코더)은 부품처럼 써도 된다"고 해석했고, SKT는 "설계도(아키텍처)가 같아도 시공(학습)을 우리가 했으니 독자 기술"이라고 해석했습니다. 기준이 없는 운동장에서 기업들은 **각자 자신에게 유리한 방식으로 '프롬 스크래치'를 정의**했고, 이것이 **결국 "국민 세금으로 남의 기술을 베꼈다"는 거센 역풍**으로 돌아온 것입니다.

### 2. 무빙 타겟의 딜레마

정부는 급변하는 AI 기술 트렌드에 맞춰 사업 목표를 반기마다 수정하는 무빙 타겟 방식을 도입했습니다. 하지만 정작 갱신된 것은 성능 목표치뿐이었고, 참가 자격에 대한 정의는 업데이트되지 않았습니다.

![출처: 독자 AI 파운데이션 프로젝트 공고문](https://wishket.com/media/news/3551/1__10_.png)

2026년 현재, 글로벌 AI 생태계는 **밑바닥부터 모든 것을 새로 만드는 시대**를 지나, **검증된 아키텍처(Transformer, MoE 등)와 모듈을 조립해 최적의 성능을 내는 효율성의 시대**로 접어들었습니다. 기업들은 글로벌 트렌드에 맞춰 오픈소스와 외부 모듈을 적극적으로 활용해 개발 속도를 높이려 했지만, 정부와 여론이 기대한 것은 **순도 100%의 국산 AI**였습니다. 기술의 속도와 제도의 속도 사이에서 발생한 이 시차가 이번 논란의 가장 큰 원인 중 하나입니다.

### 3. 개발 효율성과 AI 주권의 충돌

결국 이번 사태는 엔지니어의 현실과 정책가의 이상이 충돌한 지점입니다.

- **기업(엔지니어)의 입장:** "이미 검증된 중국/미국의 아키텍처나 모듈을 쓰면 개발 기간을 반으로 줄이고 성능은 2배로 높일 수 있다. 굳이 맨땅에 헤딩할 필요가 있는가?"
- **정부(정책가)의 입장:** "이 사업의 본질은 소버린 AI다. 효율이 떨어지더라도 남의 기술이 끊겼을 때 자립할 수 있는 완전한 독자 기술을 확보해야 한다."

두 입장 모두 틀린 말은 아닙니다. 하지만 사전에 "효율성을 위해 아키텍처 참조는 허용하되, 가중치는 100% 자체 학습이어야 한다"거나 "핵심 모듈의 국산화 비율은 80% 이상이어야 한다"는 식의 구체적인 기술적 교통정리가 선행되지 않았기에, 기업들은 효율을 쫓다 '기술 카피’, ‘기술력 부족’이라는 오명을 쓰게 되었습니다.

지금 필요한 것은 특정 기업에 대한 마녀사냥이 아니라, **대한민국이 정의하는 독자 기술의 범위가 어디까지인가를 사회적으로 합의하는 일입니다**. 이 기준이 바로 서지 않는다면, 제2, 제3의 국가대표 AI가 나와도 똑같은 논란은 반복될 것입니다.

---

## 마녀사냥을 멈추고 운동장을 정비할 때

지금 우리에게 필요한 것은 특정 기업을 향한 비난과 색안경이 아닙니다. 이번 '프롬 스크래치' 논란은 한국 AI 산업이 성장통을 겪으며 한 단계 성숙해지는 과정에서 필연적으로 터져 나올 수밖에 없었던 고름과도 같습니다. 이제는 게임의 규칙을 다시 써야 할 때입니다.

### 1. 비난보다 선행되어야 할 '사회적 합의'

네이버, SKT, 업스테이지 같은 기업들이 기술이 부족해서 도덕적 해이로 인해 타사의 기술을 카피한 것이 아닙니다. 그들은 글로벌 경쟁 속에서 **속도와 효율이라는 엔지니어의 미덕**을 좇았고, 정부는 **주권과 기술 독립이라는 정책적 이상**을 요구했을 뿐입니다. 이 간극을 메우지 못한 채 시작된 경기는 필연적으로 파열음을 낼 수밖에 없었습니다.

이제 우리는 "베꼈다, 아니다"의 소모적인 OX 퀴즈를 멈추고, "대한민국이 용인할 수 있는 기술적 참조(벤치마킹)의 범위는 어디까지인가?"를 사회적으로 합의해야 합니다. 맹목적인 비난은 기업들의 도전 의지를 꺾고, 그들을 안전한 길(단순 API 활용)로만 숨게 만들 뿐입니다.

### 2. 새로운 '국가대표'의 기준: 가중치, 아키텍처, 데이터

앞으로의 국책 과제와 기술 평가에서는 '독자적', '새로운' 같은 모호한 문학적 표현을 걷어내고, 엔지니어가 납득할 수 있는 구체적인 가이드라인이 제시되어야 합니다.

#### 가중치(Weights): 타협 없는 독자성

모델의 '지능'이자 '영혼'인 가중치만큼은 타협해선 안 됩니다. 초기화된 상태에서 우리의 데이터와 컴퓨팅 자원으로 학습된 결과물이어야만 진정한 '우리 AI'라고 부를 수 있습니다.

#### 아키텍처(Architecture): 유연한 수용과 내재화

건물의 설계도는 참고할 수 있습니다. 단, 남의 설계도를 그대로 베끼는 것을 넘어, 우리 지형에 맞게 수치(하이퍼파라미터)를 최적화하고 수정할 수 있는 '설계 변경 능력'을 입증해야 합니다.

#### 데이터(Data): 가장 확실한 주권

기술은 비슷할 수 있어도, 데이터는 복제할 수 없습니다. 한국의 역사, 문화, 가치관이 담긴 고품질 데이터를 얼마나 확보하고 학습시켰는가가 기술 독립의 가장 강력한 척도가 되어야 합니다.

### 3. 진정한 기술 독립은 '통제권'에 있다

진정한 기술 독립은 우리가 통제할 수 있는가에 달려 있습니다. 해외 오픈소스나 모듈을 가져다 쓰더라도, 그 내부 원리를 완벽하게 이해하고 있어 언제든 문제가 생기면 우리 손으로 뜯어고칠 수 있다면 그것은 우리의 기술입니다. 반면, 아무리 좋은 성능의 AI라도 블랙박스처럼 그 속을 알 수 없어 남의 손을 빌려야만 고칠 수 있다면, 그것은 우리의 기술이 아닙니다.

이번 논란이 남긴 상처가 헛되지 않으려면, 정부와 기업은 더 투명하게 소통하고, 국민은 이 과정을 인내심을 갖고 지켜봐야 합니다. 지금의 진통은 구글 대신 네이버를 사용하는 것처럼, 왓츠앱 대신 카카오톡을 사용하는 것처럼 대한민국이 일방적인 AI 소비국에서 진정한 AI 기술 보유국으로 나아가기 위한, 아프지만 꼭 필요한 예방주사가 될 것입니다.

©️요즘IT의 모든 콘텐츠는 저작권법의 보호를 받는 바, 무단 전재와 복사, 배포 등을 금합니다.

<!-- TODO: add ARTICLE CARD -->
```component VPCard
{
  "title": "네이버·SKT·업스테이지가 베꼈다고? '프롬 스크래치' 논란 완벽 정리",
  "desc": "최근 IT 업계가 뜨겁게 달아올랐습니다. 바로 AI 국가대표 선발전 때문입니다. 정부는 글로벌 빅테크에 맞설 기술 주권을 확보하기 위해 수천억 원의 예산을 투입, ‘독자 AI 파운데이션 모델’ 구축 프로젝트를 시작했습니다. 네이버, SKT, 업스테이지 등 내로라하는 AI 기업들이 출사표를 던지며 기대감은 최고조에 달했습니다. 하지만 최종 AI 국가대표가 선발되기 전에 파열음이 들려왔습니다. 논란의 핵심은 참가 기업들의 모델이 ”프롬 스크래치(From Scratch)가 아니다”라는 의혹입니다. 도대체 ‘프롬 스크래치’가 무엇이길래 국가대표 자격 논란까지 번진 걸까요?",
  "link": "https://chanhi2000.github.io/bookshelf/yozm.wishket.com/3551.html",
  "logo": "https://yozm.wishket.com/favicon.ico",
  "background": "rgba(84,7,224,0.2)"
}
```
