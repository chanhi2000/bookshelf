import{_ as L}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c,d as e,f as n,b as d,a as h,t as A,n as m,g as p,w as o,e as i,r as s,o as I}from"./app-BVguHYKu.js";const y={},f={id:"frontmatter-title-관련",tabindex:"-1"},M={class:"header-anchor",href:"#frontmatter-title-관련"},k={class:"table-of-contents"},b={class:"hint-container details"},w={href:"https://goddoe.github.io/research/engineering/2024/06/01/Scaling.html",target:"_blank",rel:"noopener noreferrer"},z={class:"hint-container details"},x={href:"https://arxiv.org/abs/2506.22403",target:"_blank",rel:"noopener noreferrer"},R={class:"hint-container details"},v={class:"hint-container details"},P={href:"https://linkedin.com/in/sungju-kim-3b0406b0/",target:"_blank",rel:"noopener noreferrer"},C={href:"mailto:heesun.noh@wishket.com",target:"_blank",rel:"noopener noreferrer"};function T(g,t){const r=s("VPCard"),a=s("router-link"),u=s("SiteInfo"),l=s("VPIcon");return I(),c("div",null,[e("h1",f,[e("a",M,[e("span",null,A(g.$frontmatter.title)+" 관련",1)])]),n(r,m(p({title:"LLM > Article(s)",desc:"Article(s)",link:"/ai/llm/articles/README.md",logo:"/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),n(r,m(p({title:"Career > Article(s)",desc:"Article(s)",link:"/projects/career/articles/README.md",logo:"/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),e("nav",k,[e("ul",null,[e("li",null,[n(a,{to:"#하이퍼클로바-x-개발기-코드생성llm"},{default:o(()=>t[0]||(t[0]=[i("하이퍼클로바 X 개발기: 코드생성LLM")])),_:1,__:[0]})]),e("li",null,[n(a,{to:"#연구-개발-문화는-어떤가요"},{default:o(()=>t[1]||(t[1]=[i("연구&개발 문화는 어떤가요?")])),_:1,__:[1]})]),e("li",null,[n(a,{to:"#ai-리서치-엔지니어에게-필요한-것-수학-머신러닝-그리고-집요함"},{default:o(()=>t[2]||(t[2]=[i("AI 리서치 엔지니어에게 필요한 것: 수학, 머신러닝, 그리고 집요함")])),_:1,__:[2]})]),e("li",null,[n(a,{to:"#슈퍼-인텔리전스가-온다"},{default:o(()=>t[3]||(t[3]=[i("슈퍼 인텔리전스가 온다")])),_:1,__:[3]})])])]),t[30]||(t[30]=e("hr",null,null,-1)),n(u,{name:"네이버에서 LLM 개발을 한다는 것",desc:"최근 ‘독자 AI 파운데이션 모델’, ‘소버린 AI’ 개념이 떠오르며 국내 LLM도 주목을 받고 있는데요. 네이버 하이퍼클로바X 개발에 참여하고 현재 오디오 LLM을 연구하는 김성주 리서치 엔지니어를 인터뷰 했습니다. 소버린 AI의 중요성, 텍스트를 넘어 오디오로 확장하는 AI의 최신 트렌드, 가상 데이터 생성과 강화학습 등 프런티어 기술의 뒷이야기, 그리고 AI 리서치 엔지니어의 커리어에 대한 깊이 있는 조언을 담았습니다. AI 개발에 대한 다양한 트렌드와 담론 속에서 국내 LLM 개발 트렌드를 파악하고 관련 커리어를 쌓아나가고 싶은 분들께 도움이 되길 바랍니다.",url:"https://yozm.wishket.com/magazine/detail/3276/",logo:"https://yozm.wishket.com/favicon.ico",preview:"https://yozm.wishket.com/media/news/3276/image3.png"}),t[31]||(t[31]=d('<blockquote><p><strong>김성주 네이버클라우드 AI 리서치 엔지니어/테크리드 인터뷰</strong></p></blockquote><p>최근 ‘독자 AI 파운데이션 모델’, ‘소버린 AI’ 개념이 떠오르며 국내 LLM도 주목을 받고 있는데요. 국내 대표 테크 기업인 네이버는 자회사 네이버클라우드를 통해 ‘한국의 문화와 맥락을 가장 잘 이해’한다는 점을 강조하는 초거대 LLM 모델 하이퍼클로바X를 개발해냈죠.</p><p>최근에는 한국어 성능을 평가하는 벤치마크에서 좋은 평가를 받는 국내 LLM들이 있지만, 네이버는 그 길을 가장 먼저 갔다고 할 수 있습니다. 얼마 전 ‘독자 AI 파운데이션 모델 프로젝트’에서 네이버클라우드가 주도하는 컨소시엄이 5개 국가대표 AI 정예 팀 중 하나로 선정되기도 했죠.</p><p>이제 텍스트를 넘어 오디오 LLM 등 다양한 모달리티를 높이며 고도화해나가고 있는데요. 네이버의 초거대 LLM 모델 하이퍼클로바X에 테크리드 중 한 명으로 참여했고, 현재도 오디오 LLM을 개발하고 있는 김성주 네이버클라우드 리서치 엔지니어를 만나 LLM 개발과 리서치 엔지니어 커리어에 관한 이야기를 나눴습니다.</p><p>김 테크리드는 “지금처럼 전 세계 기업이 AI 기술을 서로 공개하고 확산시키는 흐름 속에서는 OpenAI와 같은 프런티어 기업의 기술도 더 이상 독보적이라 보기 어렵다”며 “결국 기술이 수렴하는 가운데, 네이버 LLM도 지속적인 개발만 이어진다면, 머지않아 그들과 어깨를 나란히 하게 될 것”이라고 말했습니다.</p><p>AI 개발에 대한 다양한 트렌드와 담론 속에서 국내 LLM 개발 트렌드를 파악하고 관련 커리어를 쌓아나가고 싶은 분들께 도움이 되고자 인터뷰 내용을 공유합니다.</p><figure><img src="https://wishket.com/media/news/3276/image4.jpg" alt="김성주 네이버클라우드 AI 리서치 엔지니어/테크리드" tabindex="0" loading="lazy"><figcaption>김성주 네이버클라우드 AI 리서치 엔지니어/테크리드</figcaption></figure><hr><h2 id="하이퍼클로바-x-개발기-코드생성llm" tabindex="-1"><a class="header-anchor" href="#하이퍼클로바-x-개발기-코드생성llm"><span>하이퍼클로바 X 개발기: 코드생성LLM</span></a></h2><details class="hint-container details"><summary>Q. 네이버클라우드에서 AI 리서치 엔지니어 겸 테크 리드로 재직 중이신데요, 구체적으로 어떤 일을 하고 계신지 설명해주세요.</summary><p>네이버클라우드 하이퍼스케일AI 조직의 파운데이션 리서치에 소속된 AI 리서치 엔지니어 겸 테크리드로 일하고 있습니다. 하이퍼스케일 AI 조직은 이전에 클로바(Clova)라고 불리던 연구 조직의 전신을 이어받은 곳이고요. 현재는 팀 리딩보다 기술적 의사 결정에 많이 참여합니다. 과제나 프로젝트를 진행할 때 기술적 접근 방식을 제안하고 주도적으로 진행하는 &#39;행동대장&#39; 역할이라고 할 수 있습니다. 최근에는 특히 오디오 LLM(Audio LLM) 미션에서 강화학습(Reinforcement Learning) 관련 기술을 직접 만들고 학습하며 연구와 실험을 하고 있습니다.</p></details><details class="hint-container details"><summary>Q. 하이퍼클로바 X(HyperCLOVA X)프로젝트의 기술 리드 중 한 명으로서 하이퍼클로바 X의 코드 생성 및 관련 기능 강화에 기여하셨는데, 어떤 종류의 기여를 하셨는지 구체적인 사례를 들어 설명해 주실 있을까요?</summary><p>코드생성 능력을 향상 시키기 위한 기술적 의사결정들을 했어요. 대표적으로 가상 데이터 생성을 극한으로 밀어붙여서 성능을 끌어올렸고, 여러 종류의 사용 가능한 오픈소스 데이터 수집하고 정제해 하이퍼클로바X의 기반 능력을 향상시켰습니다.</p><p>제가 기술적 의사결정에서 중요하게 생각하는 건 개발하는 기술이 스케일러블(Scalable)해야 한다는 점인데요. 이는 기술을 극한으로 밀어붙였을 때 성능이 단조 증가(Monotonically Increasing) 하는 축을 찾는 것을 의미합니다. 성능이 단조 증가한다는 것은 성능이 점차 증가한다는 건데요. 극한까지 밀어붙이면 어느 순간까지는 성능이 올라가다가, 성능 개선폭이 감소하는 시점이 옵니다. 그 시점에 도달하면 다른 방법론으로 넘어가게 되죠.</p><p>예를 들어, GPU는 코어 밀접도를 높이는 방식으로, CPU는 나노 공정을 통해 선폭을 얇게 만드는 방식으로 스케일링됩니다. 코드 생성 능력을 높이는 미션에서 가상 데이터 생성(Synthetic Data Generation)이 이러한 &#39;극한의 축&#39;이었고, 이를 많이 할수록 모델 성능이 계속 증가한다는 것을 확인했습니다.</p><figure><img src="https://wishket.com/media/news/3276/image1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></details><details class="hint-container details"><summary>Q. 앞서 말씀하신 가상 데이터는 어떻게 생성되고, 모델에 어떻게 활용되는 건가요?</summary><p>가상 데이터 생성은 RFT(Rejection Fine-Tuning) 기법을 통해 이뤄져요. LLM에게 질문을 주고 수많은 응답을 만들게 한 뒤, 응답 중에서 정답에 해당하는 답변만 골라 모아서 모델을 학습시키는 방식입니다. 정답을 고르는 과정에서도 LLM이나 AI를 활용할 수 있죠. 이렇게 생성된 가상 데이터를 필터링하고 모델에게 학습시키는 것이 RFT의 핵심이에요. 이걸 극한으로 밀어붙이는 작업은 시간과 GPU의 영향을 크게 받아요. 많은 질문 세트를 준비하고 응답도 충분히 많이 만들어야 하는데, 가용한 시간 동안 GPU 자원을 최대한 활용해야 하는 것입니다. GPU가 많을수록 같은 시간 내 더 많은 데이터를 만들 수 있죠.</p></details><details class="hint-container details"><summary>Q. 내부 개발자용으로 깃헙 코파일럿(GitHub Copilot)과 유사한 도구를 개발하기도 하셨다고요. 어떤 도구였고, 개발 과정에서 어려웠던 점과 배운 점은 무엇인가요?</summary><p>대표적으로 코드 자동 완성 기능이 있었죠. 코드를 작성하는 가운데 코드를 추천해주고, 탭을 누르면 자동 완성되는 기능이요. 또 대화형 챗봇 기능도 만들었어요. 코드나 개발 관련 질문에 답변해주는 창인데, 두 개 다 깃헙 코파일럿과 같은 기능이죠. 사내 개발자들이 자신이 이용하는 VS Code나 인텔리제이(Intellij)에 이를 붙여서 코드 자동완성, 챗봇, 리팩토링 기능을 썼어요. 현재는 생산성 도구는 내부에서 만들지 않는다는 정책에 따라 이 프로젝트는 중단하긴 했지만, 코딩 자동 완성 기능의 이용 비율이 초기 출시 대비 800% 증가하고 대화 기능도 많이 활용될 정도로 내부 반응은 매우 좋았어요.</p></details>',13)),e("details",b,[t[10]||(t[10]=e("summary",null,"Q. 하이퍼클로바 X 프로젝트를 진행하면서 가장 인상 깊었던 점은 무엇이, 대규모 언어 모델 개발에서 특히 중요하다고 생각하는 부분은 무엇인가요?",-1)),t[11]||(t[11]=e("p",null,"LLM 자체가 컴퓨팅 파워를 얼마나 투입하느냐에 따라 성능이 올라간다는 점이에요. 대규모 언어 개발에서 특히 중요한 건 GPU의 양이에요. 그건 곧 자금을 뜻하기도 하고요. 중요한 점은 현재 발전이 느려보여도 성능이 점점 글로벌 모델을 따라잡고 있다는 거예요. 어느 지점에 도달하면 성능이 수렴하는 지점이 오게 될 겁니다. 그때까지 네이버클라우드 같은 회사들이 지금처럼 개발을 따라가기만 한다 해도 소버린 AI는 할 수 있을 거라 생각해요.",-1)),e("figure",null,[t[9]||(t[9]=e("img",{src:"https://wishket.com/media/news/3276/image7.png",alt:'스케일링에 대한 생각<br/><출처: 김성주 <VPIcon icon="fas fa-globe"/>깃헙 블로그>',tabindex:"0",loading:"lazy"},null,-1)),e("figcaption",null,[t[5]||(t[5]=i("스케일링에 대한 생각")),t[6]||(t[6]=e("br",null,null,-1)),t[7]||(t[7]=i("<출처: 김성주 ")),e("a",w,[n(l,{icon:"fas fa-globe"}),t[4]||(t[4]=i("깃헙 블로그"))]),t[8]||(t[8]=i(">"))])]),t[12]||(t[12]=e("hr",null,null,-1)),t[13]||(t[13]=e("h2",{id:"듣고-말하는-ai를-위한-오디오-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#듣고-말하는-ai를-위한-오디오-llm"},[e("span",null,"듣고 말하는 AI를 위한 ‘오디오 LLM’")])],-1)),t[14]||(t[14]=e("details",{class:"hint-container details"},[e("summary",null,"Q. 이제는 코드 생성 LLM이 아니라 오디오 LLM을 연구하고 계시죠. 현재 연구하고 계신 내용을 간략히 소개해주실 수 있나요?"),e("p",null,"하이퍼클로바 X에 '듣는 기능'이나 '말하는 기능'과 같은 새로운 모달리티(Modality)를 추가하기 위해 연구하고 있어요. 이를 위해 음성 데이터와 텍스트 데이터를 얼라인(Align)시켜 텍스트 LLM이 가진 지식을 오디오에서도 활용할 수 있도록 합니다. 즉, 음성으로 질문해도 텍스트 지식을 활용하여 음성으로 대답할 수 있게 하는 거예요. 또, 리얼타임 풀 듀플렉스(Real-time Full Duplex) 기술을 통해 실시간으로 듣고 말하는 기능도 연구하고 있습니다.")],-1))]),t[32]||(t[32]=e("details",{class:"hint-container details"},[e("summary",null,"Q. 오디오LLM 연구의 데이터는 어떻게 수집하고, 가장 큰 기술적 도전은 무엇인지 궁금해요."),e("p",null,"데이터는 주로 인터넷에 공개된 사용 가능한 오픈 소스 음성 데이터와 별도로 구매한 음성 데이터를 활용하여 수집합니다. 데이터를 구매하는 전담 팀이 따로 있고요."),e("p",null,"가장 큰 기술적 도전은 텍스트 LLM의 지식을 오디오 모달리티로 전이(transfer)할 때 지식의 퇴화(degradation)가 일어나는 것을 해결하는 것입니다. 오디오 이해와 발화 능력을 학습시키면 텍스트 기반의 지식이 약해지는 경향이 있거든요. 이건 뇌의 크기(모델 규모)는 동일한데 더 많은 종류의 데이터를 이해해야 하는 한계 때문이죠. 오픈AI와 같은 곳은 방대한 GPU 자원과 데이터를 통해 이를 해결하고 있지만, 네이버는 한정된 자원으로 퍼포먼스를 올려야 하는 과제를 안고 있습니다. 그럼에도 이를 해결할 수 있는 방법을 많이 고안했고 성과를 내고 있다고 말하고 싶어요.")],-1)),t[33]||(t[33]=e("details",{class:"hint-container details"},[e("summary",null,"Q. 현재 네이버 서비스 중 오디오 LLM을 탑재해 테스트하고 있는 사례가 있나요? 또 연구 중인 오디오 LLM이 서비스에 적용된다면 어떤 서비스에 어떻게 적용될 수 있을까요?"),e("p",null,"현재는 실험적으로 어디에 적용할지 찾고 있는 단계예요. 서비스 적용 시점이나 확정된 서비스는 없습니다. 내부에는 음성 대화 모델 데모가 있지만, 서비스 적용은 수익성 등 많은 의사결정이 필요하여 아직 상용화하고 있지는 않습니다. 자연스러운 음성 대화를 만드는 방향의 연구에 집중하고 있어요. 텍스트를 음성으로 잘 변환해 주기만 해도 많은 활용처가 있을 것으로 보고 있죠.")],-1)),e("details",z,[t[17]||(t[17]=e("summary",null,"한국어 특화된 LLM을 개발 중이신데, 한글 발화가 영문 발화와 다른 특징이 있다면 무엇인가요?",-1)),t[18]||(t[18]=e("p",null,"딥러닝 시대로 넘어오면서 언어별 특별한 차이를 이용해 모델에 적용하기보다는, 타깃 언어의 데이터를 많이 모으면 AI 모델이 해당 언어를 잘하게 되는 방식이 됐어요. 그래서 한국어라고 해서 특별히 다른 방식으로 처리하고 영어라고 해서 다른 방식으로 처리하는 별도의 처리는 하지 않습니다. 한국어 발화 데이터를 많이 모으면 한국어를 더 잘하게 되고, 영어를 더 많이 모으면 영어를 더 잘하게 되는 방식이죠.",-1)),e("figure",null,[t[16]||(t[16]=e("img",{src:"https://wishket.com/media/news/3276/image2.png",alt:'<VPIcon icon="iconfont icon-arxiv"/>하이퍼클로바 X 씽크 테크니컬 리포트',tabindex:"0",loading:"lazy"},null,-1)),e("figcaption",null,[e("a",x,[n(l,{icon:"iconfont icon-arxiv"}),t[15]||(t[15]=i("하이퍼클로바 X 씽크 테크니컬 리포트"))])])])]),t[34]||(t[34]=e("hr",null,null,-1)),t[35]||(t[35]=e("h2",{id:"연구-개발-문화는-어떤가요",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#연구-개발-문화는-어떤가요"},[e("span",null,"연구&개발 문화는 어떤가요?")])],-1)),t[36]||(t[36]=e("details",{class:"hint-container details"},[e("summary",null,"Q. 네이버클라우드의 LLM 개발 환경은 어떤 특징을 가지고 있나요? 특히 연구와 개발 문화에 대해 설명해 주실 수 있나요?"),e("p",null,"'미션 제도'를 특징으로 합니다. LLM 연구는 프런티어 연구로, 긴급한 이슈 대응이나 기술 개발이 필요할 때가 많습니다. 이러한 경우, 특정 미션 달성을 위해 해당 미션 조직을 만들고 여러 팀에서 인력이 차출되거나 통째로 투입되어 문제를 함께 해결합니다. 미션이 완수되면 다시 해산하는 문화가 특별하다고 생각해요. 이는 오픈AI에서도 유사하게 운영되었던 방식이라고 합니다. 또, 기술적 해결 방법에 관해서는 개인의 전문성을 존중하는 방식으로 진행됩니다. 예를 들어, 가상 데이터 생성과 같은 과제에서는 특정 팀원이 구체적인 개발 방법이나 일정을 스스로 정하여 주도적으로 진행하도록 의사를 존중하죠.")],-1)),e("details",R,[t[26]||(t[26]=e("summary",null,"Q. 초거대 언어 모델의 윤리적 문제나 사회적 영향에 대해 네이버에서는 어떤 방식으로 접근하고 계시나요?",-1)),t[27]||(t[27]=e("p",null,"초거대 언어 모델의 윤리적, 사회적 문제 해결을 위해 전담 팀을 운영하고 있어요. 이 팀은 윤리적이거나 정치적인 문제로 인한 논란이 발생하지 않도록 대응하고 연구하죠. 문제 되는 발언을 하지 않도록 모델을 훈련하는 데 집중합니다. 모델 학습 단계 중 포스트 트레이닝(Post-training)의 SFT(Supervised Fine-Tuning) 및 RL(Reinforcement Learning) 단계에서 문제가 있는 발언을 하지 않도록 학습이 진행돼요. 다만, LLM은 확률적으로 작동하기 때문에 의도치 않은 동작이 발생할 수 있어서, 이를 지속적으로 개선하고 있습니다.",-1)),e("details",v,[t[24]||(t[24]=e("summary",null,"AI 리서치 엔지니어로서 AI 리서치 사이언티스트, ML옵스, 데이터 엔지니어 등 다른 직무와는 어떻게 협업이 이루어지나요?*",-1)),t[25]||(t[25]=e("p",null,"AI 리서치 사이언티스트와 AI 리서치 엔지니어들이 모델에 대한 실험을 수행하고, ML옵스와 데이터 엔지니어는 데이터 생성과 모델 생산 관리를 담당합니다. 최종적으로 모델이 완성되면 개발자들이 해당 모델을 실제 서비스에 연동(서빙)하여 유저들이 사용할 수 있도록 합니다. 모델의 스펙이나 사용 방법은 미리 논의해 개발자에게 전달되고, 개발자는 만들어진 모델을 최대한 엔지니어링 기법을 통해 서비스에 잘 활용될 수 있도록 하죠.",-1)),e("figure",null,[t[23]||(t[23]=e("img",{src:"https://wishket.com/media/news/3276/image5.png",alt:'김성주 AI 리서치 엔지니어/테크리드링크드인 (<VPIcon icon="fa-brands fa-linkedin"/>)',tabindex:"0",loading:"lazy"},null,-1)),e("figcaption",null,[t[22]||(t[22]=i("김성주 AI 리서치 엔지니어/테크리드")),e("a",P,[t[19]||(t[19]=i("링크드인 (")),n(l,{icon:"fa-brands fa-linkedin"}),t[20]||(t[20]=e("code",null,"sungju-kim-3b0406b0",-1)),t[21]||(t[21]=i(")"))])])])])]),t[37]||(t[37]=d('<hr><h2 id="ai-리서치-엔지니어에게-필요한-것-수학-머신러닝-그리고-집요함" tabindex="-1"><a class="header-anchor" href="#ai-리서치-엔지니어에게-필요한-것-수학-머신러닝-그리고-집요함"><span>AI 리서치 엔지니어에게 필요한 것: 수학, 머신러닝, 그리고 집요함</span></a></h2><details class="hint-container details"><summary>Q. 이제 이 분야의 실무에 대해서 여쭤볼게요. AI 리서치 엔지니어로서 성공하기 위해 가장 중요하다고 생각하는 역량은 무엇인가요?</summary><p>가장 기본이 되는 것은 ‘태도’라고 생각해요. AI 분야는 매우 빠르게 변화하므로, 며칠 또는 심지어 오늘 안에 해결해야 하는 긴급한 문제들이 많습니다. 이러한 부담스러운 상황에서도 긍정적인 태도로 문제를 해낼 수 있는 힘이 중요합니다. 또 긴급성을 인지하고 빠르게 대응하는 능력과 집요함이 필요합니다. 솔직히 말하면 프런티어 리서치 특성상 ‘워라밸’을 지키기 힘들 정도로 많은 시간과 노력을 투입해야 할 때가 많습니다.</p></details><details class="hint-container details"><summary>Q. 네이버클라우드에서 신입 AI 리서치 엔지니어를 채용할 때 가장 중요하게 보는 자질이나 역량은 무엇인가요?</summary><p>기본기를 가장 중요하게 봅니다. 머신러닝, 딥러닝에 대한 기본적인 지식, 그리고 수학적 백그라운드가 탄탄한 사람을 선호해요. 최신 AI 기술이나 프레임워크 사용 경험도 중요하지만, 그 밑바탕이 되는 수학, 머신러닝 백그라운드에 대한 깊은 이해가 있어야 빠르게 새로운 지식을 습득하고 좋은 의사결정을 내릴 수 있기 때문입니다. 기본기 다음으로는 AI에 대한 관심도와 새로운 지식을 빠르게 습득하는 능력이 중요하죠. 면접에서는 자신이 진행했던 프로젝트나 경험, 특히 가장 도전적인 프로젝트가 무엇이었는지를 질문하여 이러한 역량과 관심을 파악합니다.</p></details><details class="hint-container details"><summary>Q. 소프트웨어 개발자 또는 AI 리서치 엔지니어 지망생이 LLM 분야에 진입하기 위해 준비해야 할 기술이나 지식은 무엇인가요?</summary><p>AI 엔지니어는 범위가 넓어요. 그래서 크게 AI 소프트웨어 개발자와 AI 리서치 엔지니어로 나누어 설명할 수 있습니다.</p><p><strong>AI 소프트웨어 개발자</strong></p><ul><li>LLM에 대한 기본적인 이해와 LLM 활용 방법이 중요합니다.</li><li>플로우 엔지니어링(Flow Engineering) 기법에 대한 이해가 필요합니다. 플로우 엔지니어링은 LLM 호출 플로우를 디자인하는 엔지니어링을 의미합니다. 이는 LLM을 여러 번 호출하거나, 특정 정보를 LLM에 컨텍스트로 주입하는 방식 등을 포함합니다. RAG는 이러한 플로우 엔지니어링 안에 포함될 수 있는 하나의 기법이라고 볼 수 있습니다.</li><li>RAG(Retrieval Augmented Generation)와 같은 지식 및 LangChain, LangGraph와 같은 프레임워크 사용법을 잘 알아야 합니다.</li><li>전반적으로 밑바닥부터 AI 애플리케이션을 만들 수 있는 지식이 있다면 좋습니다.</li></ul><p><strong>AI 리서치 엔지니어</strong></p><ul><li>모델링(Modeling)을 수행하여 모델을 디자인하고 아키텍처를 만들며, 실제 학습 데이터를 이용해 학습, 실험, 평가를 진행합니다.</li><li>ML, 딥러닝, 수학적 백그라운드를 탄탄히 하는 것이 중요합니다.</li><li>실제 AI 모델을 학습해보고 개선한 경험이 중요합니다.</li><li>현실적으로는 석사 또는 박사 학위를 취득하는 것이 유리하지만, 실제 제품 개발 프로젝트나 스타트업 경험을 통해 모델 개선 경험을 증명하는 경우도 있습니다.</li></ul><figure><img src="https://wishket.com/media/news/3276/image3.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></details><hr><h2 id="슈퍼-인텔리전스가-온다" tabindex="-1"><a class="header-anchor" href="#슈퍼-인텔리전스가-온다"><span>슈퍼 인텔리전스가 온다</span></a></h2><details class="hint-container details"><summary>Q. 평소에 개인적으로 좋아하거나 자주 사용하는 AI 모델이나 서비스가 있다면 무엇이고, 선호하는 이유는 무엇인가요</summary><p>개인적으로 클로드 코드(Claude Code)를 많이 사용합니다. 터미널에서 실행되고 자연어 입력에 따라 프로그램을 만들거나 코드를 수정해 주는 기능을 제공하는데요. 성능이 가장 뛰어나 요청한 의도대로 코드를 틀리지 않고 완성해 주는 비율이 높습니다. 특히 VIM을 활용한 터미널 개발 환경에 클로드 코드를 띄워놓고 개발하면 편리합니다. 그 외에 ChatGPT도 많이 활용해요. 논문 조사나 기술 조사 시에는 오픈AI의 O3나 딥리서치를, 내용 요약이나 번역에는 Clova-x나 GPT-4o를 사용하죠. 코딩은 주로 클로드 코드를 씁니다.</p></details><details class="hint-container details"><summary>Q. AI 분야에서 앞으로 가장 기대되는 기술적 발전이나 트렌드는 무엇이라고 생각하시나요?</summary><p>앞으로 1년간 가장 주목되는 트렌드는 RL(Reinforcement Learning) 컴퓨트 스케일링과 테스트 타임 컴퓨트 스케일링이에요.</p><ul><li><strong>RL 컴퓨트 스케일링</strong>: 모델에 강화 학습을 많이 시킬수록 AI 성능이 지속적으로 증가하는 추세이며, 강화 학습에 투입되는 컴퓨팅 자원 양에 따라 성능이 크게 늘어납니다. 과거에는 모델 크기가 커질수록 성능이 증가했지만, 지금은 강화 학습 단계에서 주어지는 컴퓨트가 성능 개선에 큰 영향을 미칩니다.</li><li><strong>테스트 타임 컴퓨트 스케일링</strong>: LLM을 한 번만 호출하는 것이 아니라 여러 번 호출하여 그 결과를 취합함으로써 성능을 높이는 방식입니다. 이 또한 LLM 호출 횟수가 많아질수록 성능이 높아집니다.</li></ul><p>이 두 가지 스케일링 방법은 LLM 모델 크기나 사전 학습 데이터의 한계에도 불구하고 성능을 크게 향상시킬 수 있는 돌파구를 찾았다는 것을 의미합니다. 이를 통해 사람보다 똑똑한 AI, 즉 슈퍼 인텔리전스(Superintelligence)를 만들 수 있는 단계에 도달하고 있죠. 최근 Grok-4가 PhD 학생들보다 똑똑한 성능을 보여주며 수학 벤치마크 AIME 25 (American Invitational Mathematics Examination)에서 100점을 받은 사례가 이 두 가지 스케일링 기법의 중요성을 입증했어요.</p></details><details class="hint-container details"><summary>Q. AI 리서치 엔지니어 직무도 언젠가 AI로 대체될 수 있다고 보시나요?</summary><p>현재 수준의 작업은 10년 안에 대체될 수 있다고 생각합니다. 다만, AI가 발전하며 해결할 수 있는 문제가 더 많아져서 문제마다 도메인마다 좀 더 뾰족한 문제를 푸는 AI 리서치 엔지니어들이 더 많아질 수도 있다고 생각합니다. 물론 그 시점의 AI 리서치 엔지니어의 일은 지금의 일과는 전혀 다를 것 같습니다. 어떤 문제를 풀 때 AI가 풀 문제의 범위를 정하고, 문제 푸는 방식에 대해 감독하는 감독관 같은 느낌 일 것 같습니다.</p></details><details class="hint-container details"><summary>Q. 앞으로 LLM과 관련하여 개인적으로 연구하거나 도전해보고 싶은 주제가 있다면 무엇인가요</summary><p>요즘 가장 관심이 많은 주제는 강화 학습(Reinforcement Learning)입니다. 그록 4(Grok-4)가 강화 학습 스케일링을 통해 성능 향상을 보였듯이, 저도 RL 컴퓨트 증가와 테스트 타임 스케일링 증가를 통해 LLM 성능 향상을 이루어보고 싶다는 구체적인 계획이 있어요.</p></details><details class="hint-container details"><summary>Q. 앞서 소버린 AI에 대해 언급해주셨어요. 아무래도 요즘 가장 주목받는 키워드 중 하나인데요. 소버린 AI를 뭐라고 정의하시는지, 기술적, 사회적으로 왜 중요하다고 보시는지 말씀해주세요.</summary><p>소버린 AI는 한 나라가 특정 기술을 외부에 의존하지 않고 스스로 통제하며 보유하는 것이라고 생각합니다. 이는 특히 일자리 창출 측면에서 매우 중요하다고 생각해요. 저는 네이버가 검색을 잃지 않았기 때문에 국내에서 검색과 관련한 좋은 일자리를 만들 수 있었다고 생각해요. 마찬가지로 카카오톡이 있었기에 관련 일자리들이 생겼고요. 코어 기술이 국내에 없다면 관련 산업 전체 일자리가 국내에서 사라지는 거라고 봅니다. AI도 마찬가지로 ‘소버린 AI’를 통해 그 핵심 기술에서 파생되는 많은 일자리를 국내에 만들 수 있다고 생각해요. 또 코어 기술을 외부에 의존한다면 가격 인상이나 사용 중지 등 통제할 수 없는 의사결정의 피해를 볼 수 있고, 군사적 도입 등 국가 안보 측면의 위험 방지를 위해서도 소버린 AI가 중요하다고 생각합니다.</p><figure><img src="https://wishket.com/media/news/3276/image6.jpg" alt="클로바 스피커에 들어가는 모델을 연구개발하던 시절에 라인에 출장가서 찍은 기념 사진" tabindex="0" loading="lazy"><figcaption>클로바 스피커에 들어가는 모델을 연구개발하던 시절에 라인에 출장가서 찍은 기념 사진</figcaption></figure><p>원래 ‘생각하는 기계’를 만드는 게 꿈이었던 그는 “그 꿈이 이렇게 젊은 날에 이룰 수 있는 건지 몰랐다”며 “이제 그걸 어떻게 만드는지 알게 되고 직접 만들게 되니 너무 재밌다”고 말합니다. 그런 만큼 빠르게 바뀌는 기술 환경 속에서도 호기심을 갖고 긍정적인 태도로 ‘집요’하게 문제를 해결하는 역량을 AI 리서치 엔지니어의 중요한 역량으로 꼽았는데요. 그 자신도 빠르게 진행해야 하는 프로젝트로 집중해야 하는 기간은 시간 가는 줄 모르고 새벽까지 일한다고 합니다. 그만큼 한국의 프런티어 모델을 개발한다는 자부심을 갖고 있었습니다.</p><p>그가 몸담은 네이버클라우드는 한국어 LLM의 개척자로서, 하이퍼클로바X에 이어 오디오 LLM까지 이어지는 새로운 도전을 통해 ‘듣고 말하는’ AI의 가능성을 넓혀가고 있습니다. 한정된 자원 속에서도 글로벌 수준의 기술을 만들어가는 네이버의 도전은 단순히 모델 성능을 넘어서, 우리 사회가 주체적으로 기술을 설계하고 발전시킬 수 있다는 가능성을 상징합니다.</p><p>지금 이 순간에도 생각하는 기계를 만들고 있는 이들 덕분에, 한국의 AI는 더 멀리 나아가고 있는 게 아닐까요.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>네이버 LLM인 하이퍼클로바 X 개발은 네이버클라우드에서 진행하고 있어서 글에서는 네이버, 네이버클라우드를 혼용해 사용했습니다</p></div></details>',12)),e("p",null,[t[29]||(t[29]=i("노희선 에디터 ")),e("a",C,[n(l,{icon:"fas fa-envelope"}),t[28]||(t[28]=e("code",null,"heesun.noh@wishket.com",-1))])]),h(" TODO: add ARTICLE CARD "),n(r,m(p({title:"네이버에서 LLM 개발을 한다는 것",desc:"최근 ‘독자 AI 파운데이션 모델’, ‘소버린 AI’ 개념이 떠오르며 국내 LLM도 주목을 받고 있는데요. 네이버 하이퍼클로바X 개발에 참여하고 현재 오디오 LLM을 연구하는 김성주 리서치 엔지니어를 인터뷰 했습니다. 소버린 AI의 중요성, 텍스트를 넘어 오디오로 확장하는 AI의 최신 트렌드, 가상 데이터 생성과 강화학습 등 프런티어 기술의 뒷이야기, 그리고 AI 리서치 엔지니어의 커리어에 대한 깊이 있는 조언을 담았습니다. AI 개발에 대한 다양한 트렌드와 담론 속에서 국내 LLM 개발 트렌드를 파악하고 관련 커리어를 쌓아나가고 싶은 분들께 도움이 되길 바랍니다.",link:"https://chanhi2000.github.io/bookshelf/yozm.wishket.com/3276.html",logo:"https://yozm.wishket.com/favicon.ico",background:"rgba(84,7,224,0.2)"})),null,16)])}const X=L(y,[["render",T]]),V=JSON.parse('{"path":"/yozm.wishket.com/3276.html","title":"네이버에서 LLM 개발을 한다는 것","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"네이버에서 LLM 개발을 한다는 것","description":"Article(s) > 네이버에서 LLM 개발을 한다는 것","icon":"fas fa-language","category":["AI","LLM","Career","Tip","Article(s)"],"tag":["blog","yozm.wishket.com","ai","artificial-intelligence","llm","large-language-models","career","tip"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"네이버에서 LLM 개발을 한다는 것\\",\\"image\\":[\\"https://wishket.com/media/news/3276/image4.jpg\\",\\"https://wishket.com/media/news/3276/image1.png\\",\\"https://goddoe.github.io/research/engineering/2024/06/01/Scaling.html\\",\\"https://arxiv.org/abs/2506.22403\\",\\"https://linkedin.com/in/sungju-kim-3b0406b0/\\",\\"https://wishket.com/media/news/3276/image3.png\\",\\"https://wishket.com/media/news/3276/image6.jpg\\"],\\"datePublished\\":\\"2025-08-08T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"요즘IT\\",\\"url\\":\\"https://yozm.wishket.com/magazine/@yozm_it/\\"}]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/yozm.wishket.com/3276.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"네이버에서 LLM 개발을 한다는 것"}],["meta",{"property":"og:description","content":"Article(s) > 네이버에서 LLM 개발을 한다는 것"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yozm.wishket.com/media/news/3276/image3.png"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://yozm.wishket.com/media/news/3276/image3.png"}],["meta",{"name":"twitter:image:alt","content":"네이버에서 LLM 개발을 한다는 것"}],["meta",{"property":"article:author","content":"요즘IT"}],["meta",{"property":"article:tag","content":"tip"}],["meta",{"property":"article:tag","content":"career"}],["meta",{"property":"article:tag","content":"large-language-models"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"artificial-intelligence"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:tag","content":"yozm.wishket.com"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:published_time","content":"2025-08-08T00:00:00.000Z"}],[{"meta":null},{"property":"og:title","content":"Article(s) > 네이버에서 LLM 개발을 한다는 것"},{"property":"og:description","content":"네이버에서 LLM 개발을 한다는 것"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/yozm.wishket.com/3276.html"}]],"prev":"/ai/llm/articles/README.md","date":"2025-08-08T00:00:00.000Z","isOriginal":false,"author":[{"name":"요즘IT","url":"https://yozm.wishket.com/magazine/@yozm_it/"}],"cover":"https://yozm.wishket.com/media/news/3276/image3.png"},"git":{},"readingTime":{"minutes":2.05,"words":614},"filePathRelative":"yozm.wishket.com/3276.md","copyright":{"author":"요즘IT"}}');export{X as comp,V as data};
