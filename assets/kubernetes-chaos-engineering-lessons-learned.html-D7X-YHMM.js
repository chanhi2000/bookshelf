import{_ as m}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as b,d as n,f as a,b as p,a as g,t as k,n as c,g as d,w as l,e as s,r as i,o as v}from"./app-BVguHYKu.js";const f={},y={id:"frontmatter-title-관련",tabindex:"-1"},w={class:"header-anchor",href:"#frontmatter-title-관련"},x={class:"table-of-contents"},T={href:"https://blog.manabusakai.com/2018/02/fault-tolerance-of-kubernetes/",target:"_blank",rel:"noopener noreferrer"},P={href:"https://kubernetes.io/docs/reference/generated/kube-proxy/",target:"_blank",rel:"noopener noreferrer"},L={href:"https://twitter.com/manabusakai",target:"_blank",rel:"noopener noreferrer"},A={href:"https://twitter.com/Valentin_NC",target:"_blank",rel:"noopener noreferrer"};function I(u,e){const r=i("VPCard"),t=i("router-link"),h=i("SiteInfo"),o=i("VPIcon");return v(),b("div",null,[n("h1",y,[n("a",w,[n("span",null,k(u.$frontmatter.title)+" 관련",1)])]),a(r,c(d({title:"Kubernetes > Article(s)",desc:"Article(s)",link:"/devops/k8s/articles/README.md",logo:"https://chanhi2000.github.io/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),n("nav",x,[n("ul",null,[n("li",null,[a(t,{to:"#but-what-happens-when-kube-proxy-crashes"},{default:l(()=>e[0]||(e[0]=[s("But what happens when kube-proxy crashes?")])),_:1,__:[0]})]),n("li",null,[a(t,{to:"#it-s-time-to-break-things"},{default:l(()=>e[1]||(e[1]=[s("It's time to break things")])),_:1,__:[1]})]),n("li",null,[a(t,{to:"#lessons-learned"},{default:l(()=>e[2]||(e[2]=[s("Lessons learned")])),_:1,__:[2]})]),n("li",null,[a(t,{to:"#that-s-all-folks"},{default:l(()=>e[3]||(e[3]=[s("That's all folks!")])),_:1,__:[3]})])])]),e[19]||(e[19]=n("hr",null,null,-1)),a(h,{name:"Kubernetes Chaos Engineering: Lessons Learned — Part 1",desc:"When you deploy an app in Kubernetes, your code ends up running on one or more worker nodes. But what happens when a node breaks and the network proxy crashes?",url:"https://learnk8s.com/learnk8s.ioblogkubernetes-chaos-engineering-lessons-learned",logo:"https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg",preview:"https://static.learnk8s.io/24f18c579e9cd93a6504110cd0e9ad65.png"}),e[20]||(e[20]=p('<p>When you deploy an application in Kubernetes, your code ends up running on one or more worker nodes.</p><p>A node may be a physical machine or VM such as AWS EC2 or Google Compute Engine and having several of them means you can run and scale your application across instances efficiently.</p><p>If you have a cluster made of three nodes and decide to scale your application to have four replicas, Kubernetes will spread the replicas across the nodes evenly like so:</p><figure><img src="https://learnk8s.io/a/15261f4c090b83cef6a3f3c447e7d898.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The architecture described above works particularly well in case of failures.</p><p>If the first node were to be unavailable, the other two could still serve the application.</p><p>Meanwhile, Kubernetes has enough time to reschedule the fourth replica to another node.</p><figure><img src="https://learnk8s.io/a/95fa67fa9ea88d3d6ce85ac95952ee33.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Even better, if all of the nodes were to become isolated, they could still serve traffic. Let&#39;s scale down the application to two replicas:</p><figure><img src="https://learnk8s.io/a/5adaae4b4e1f1027d82c0ff164bd1633.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Now imagine that the three pods belong to a service of <code>type: NodePort</code>.</p><p>A NodePort service exposes a port in the range between 30000-32767 in each node in the cluster.</p><p>That means that every node can respond to incoming requests, even if the node itself doesn&#39;t host the app.</p><p>So how does the third node know that it doesn&#39;t run the pod and has to route the traffic to one of the other nodes?</p><figure><img src="https://learnk8s.io/a/05881ce7e8c56dc886227c50a56cb307.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Kubernetes has a binary called <code>kube-proxy</code> that runs on each node, and that is in charge of routing the traffic from a service to a specific pod.</p><p>You can think of <code>kube-proxy</code> like a receptionist.</p><p>The proxy intercepts all the traffic directed to the service and routes it to the right pod.</p><p><strong>But how does <code>kube-proxy</code> know where all the pods are?</strong></p><p><strong>And how does <code>kube-proxy</code> know about the services?</strong></p><p>It doesn&#39;t.</p><p>The master node knows <em>everything</em> and is in charge of creating the list with all the routing rules.</p><p><code>kube-proxy</code> is in charge of checking and enforcing the rules on the list.</p><p>In the simple scenario above, the list looks like this:</p><ul><li>A Service points to Application instance 1 and 2</li><li>Application instance 1 is available on Node 1</li><li>Application instance 2 is available on Node 2</li></ul><p>It doesn&#39;t matter which node the traffic is coming from; <code>kube-proxy</code> knows where the traffic should be forwarded to by looking at the list.</p><figure><img src="https://learnk8s.io/a/9b046391dbc619b6617161a248d6124b.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><h2 id="but-what-happens-when-kube-proxy-crashes" tabindex="-1"><a class="header-anchor" href="#but-what-happens-when-kube-proxy-crashes"><span>But what happens when <code>kube-proxy</code> crashes?</span></a></h2><p><strong>And what if the list of rules is lost?</strong></p><p><strong>What happens when there&#39;s no rule to forward the traffic to?</strong></p>',31)),n("p",null,[n("a",T,[a(o,{icon:"fas fa-globe"}),e[4]||(e[4]=s("Manabu Sakai had the same questions"))]),e[5]||(e[5]=s(". So he decided to find out."))]),e[21]||(e[21]=p(`<p>Let&#39;s assume you have a 2 node cluster on GCP:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get nodes</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME        STATUS  ROLES   AGE VERSION</span></span>
<span class="line"><span class="token comment"># node1       Ready   &lt;none&gt;  17h v1.8.8-gke.0</span></span>
<span class="line"><span class="token comment"># node2       Ready   &lt;none&gt;  18h v1.8.8-gke.0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>And you deployed Manabu&#39;s application with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl create <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/manabusakai/k8s-hello-world/master/kubernetes/deployment.yml</span>
<span class="line">kubectl create <span class="token parameter variable">-f</span> https://raw.githubusercontent.com/manabusakai/k8s-hello-world/master/kubernetes/service.yml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>The application is simple. It displays the hostname of the current pod in a web page:</p><figure><img src="https://learnk8s.io/a/07f12509e17ec374e92ae4808a43c37a.svg" alt="App display &quot;Hello World&quot;" tabindex="0" loading="lazy"><figcaption>App display &quot;Hello World&quot;</figcaption></figure><p>You should scale the deployments to ten replicas with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl scale <span class="token parameter variable">--replicas</span> <span class="token number">10</span> deployment/k8s-hello-world</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The ten replicas are distributed evenly across the two nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pods</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                              READY STATUS  NODE</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-7shq5  1/1   Running node1</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-9w5tj  1/1   Running node1</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-cdc64  1/1   Running node2</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-lkdvj  1/1   Running node2</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-npkn6  1/1   Running node1</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-ppsqk  1/1   Running node2</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-sc9pf  1/1   Running node1</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-tjg4n  1/1   Running node2</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-vrkr9  1/1   Running node1</span></span>
<span class="line"><span class="token comment"># k8s-hello-world-55f48f8c94-xzvlc  1/1   Running node2</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://learnk8s.io/a/87836a9f146ea0596787c4db8cbc0e3b.svg" alt="Distributing traffic" tabindex="0" loading="lazy"><figcaption>Distributing traffic</figcaption></figure><p>A Service was created to load balance the requests across the ten replicas:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get services</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME              TYPE      CLUSTER-IP      EXTERNAL-IP PORT(S)         AGE</span></span>
<span class="line"><span class="token comment"># k8s-hello-world   NodePort  100.69.211.31   &lt;none&gt;      8080:30000/TCP  3h</span></span>
<span class="line"><span class="token comment"># kubernetes        ClusterIP 100.64.0.1      &lt;none&gt;      443/TCP         18h</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The service is exposed to the outside world using <code>NodePort</code> on port 30000. In other words, each node has port 30000 opened to the public internet and can accept incoming traffic.</p><figure><img src="https://learnk8s.io/a/ef529c736e7538daac10f7f358cbf307.svg" alt="" tabindex="0" loading="lazy"><figcaption><code>NodePort</code></figcaption></figure><p><strong>But how is the traffic routed from port 30000 to my pod?</strong></p><p><code>kube-proxy</code> is in charge of setting up the rules to route the incoming traffic from port 30000 to one of the ten pods.</p><p>You should try to request the node on port 30000:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">curl</span> <span class="token operator">&lt;</span>node ip<span class="token operator">&gt;</span>:30000</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">Note</p><p>Please note that you can retrieve the node&#39;s IP with <code>kubectl get nodes -o wide</code></p></div><p>The application replies with <em>Hello World!</em> and the hostname of the container is running on.</p><p>In the previous command, you should be greeted by <code>Hello world! via &lt;hostname&gt;</code>.</p><p>If you keep requesting the same URL, you may notice how sometimes you get the same response and sometimes it changes.</p><p><code>kube-proxy</code> is acting as a load balancer and is looking at the routing list and distributing the traffic across the ten pods.</p><p>What&#39;s more interesting is that it doesn&#39;t matter which node you request.</p><p>The response could come from any pod, even one that is not hosted on the same node you requested.</p><p>To complete your setup, you should have an external load balancer routing the traffic to your nodes on port 30000.</p><figure><img src="https://learnk8s.io/a/785878ae3443b851659c00d2a1108894.svg" alt="Load balancer" tabindex="0" loading="lazy"><figcaption>Load balancer</figcaption></figure><p>The load balancer will route the incoming traffic from the internet to one of the two nodes.</p><p>If you&#39;re confused by how many load balancer-like things we have, let&#39;s quickly recap:</p><ol><li>Traffic coming from the internet is routed to the primary load balancer</li><li>The load balancer forwards the traffic to one of the two nodes on port 30000</li><li>The rules set up by <code>kube-proxy</code> route the traffic from the node to a pod</li><li>the traffic reaches the pod</li></ol><p>Phew! That was long!</p><hr><h2 id="it-s-time-to-break-things" tabindex="-1"><a class="header-anchor" href="#it-s-time-to-break-things"><span>It&#39;s time to break things</span></a></h2><p>Now that you know how things are plugged in together let&#39;s get back to the original question.</p><p><em>What if you tamper with the routing rules?</em></p><p><em>Will the cluster still work?</em></p><p><em>Do the pods still serve requests?</em></p><p>Let&#39;s go ahead and delete the routing rules.</p><p>In a separate shell, you should monitor the application for time and dropped requests.</p><p>You could write a loop that every second prints the time and request the application:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token keyword">while</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">do</span> <span class="token function">date</span> +%X<span class="token punctuation">;</span> <span class="token function">curl</span> <span class="token parameter variable">-sS</span> http://<span class="token operator">&lt;</span>your load balancer ip<span class="token operator">&gt;</span>/ <span class="token operator">|</span> <span class="token function">grep</span> ^Hello<span class="token punctuation">;</span></span>
<span class="line"><span class="token keyword">done</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 10:14:41 Hello world! via k8s-hello-world-55f48f8c94-vrkr9</span></span>
<span class="line"><span class="token comment"># 10:14:43 Hello world! via k8s-hello-world-55f48f8c94-tjg4n</span></span>
<span class="line"><span class="token comment"># ^C</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>In this case, you have the time in the first column and the response from the pod in the other.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>The first call was made to the <em>k8s-hello-world-55f48f8c94-vrkr9</em> pod at 10:14 and 41 seconds.</p><p>The second call was made to the <em>k8s-hello-world-55f48f8c94-tjg4n</em> pod at 10:14 and 43 seconds.</p></div><p>Let&#39;s delete the routing rules from the node.</p><p><code>kube-proxy</code> can operate in three modes: <strong>userspace</strong>, <strong>iptables</strong> and <strong>ipvs</strong>. The default since Kubernetes 1.2 is <strong>iptables</strong>.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>Please note that if you&#39;re using a cluster with 1.11 or more recent, you might be using <strong>ipvs</strong>.</p></div><p>In <strong>iptables</strong> mode, <code>kube-proxy</code> writes the list of routing rules to the node using iptables rules.</p><p>So you could log in into one of the node servers and delete the iptables rules with <code>iptables -F</code>.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>Please note that <code>iptables -F</code> may interfere with your SSH connection.</p></div><p>If everything went according to plan you should experience something similar to this:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token keyword">while</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">do</span> <span class="token function">date</span> +%X<span class="token punctuation">;</span> <span class="token function">curl</span> <span class="token parameter variable">-sS</span> http://<span class="token operator">&lt;</span>your load balancer ip<span class="token operator">&gt;</span>/ <span class="token operator">|</span> <span class="token function">grep</span> ^Hello<span class="token punctuation">;</span></span>
<span class="line"><span class="token keyword">done</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 10:14:41 Hello world! via k8s-hello-world-55f48f8c94-xzvlc</span></span>
<span class="line"><span class="token comment"># 10:14:43 Hello world! via k8s-hello-world-55f48f8c94-tjg4n</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># this is when \`iptables -F\` was issued</span></span>
<span class="line"><span class="token comment"># 10:15:10 Hello world! via k8s-hello-world-55f48f8c94-vrkr9</span></span>
<span class="line"><span class="token comment"># 10:15:11 Hello world! via k8s-hello-world-55f48f8c94-vrkr9</span></span>
<span class="line"><span class="token comment"># ^C</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>As you noticed, it took about 27 seconds from when you dropped the iptables rules and the next response, from 10:14:43 to 10:15:10. <em>What happened in this 27 seconds?</em></p><p><em>Why is everything back to normal after 27 seconds?</em></p><p>Perhaps it&#39;s just a coincidence. Let&#39;s flush the rules again:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token keyword">while</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">do</span> <span class="token function">date</span> +%X<span class="token punctuation">;</span> <span class="token function">curl</span> <span class="token parameter variable">-sS</span> http://<span class="token operator">&lt;</span>your load balancer ip<span class="token operator">&gt;</span>/ <span class="token operator">|</span> <span class="token function">grep</span> ^Hello<span class="token punctuation">;</span></span>
<span class="line"><span class="token keyword">done</span></span>
<span class="line"><span class="token comment">#</span></span>
<span class="line"><span class="token comment"># 11:29:55 Hello world! via k8s-hello-world-55f48f8c94-xzvlc</span></span>
<span class="line"><span class="token comment"># 11:29:56 Hello world! via k8s-hello-world-55f48f8c94-tjg4n</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># this is when \`iptables -F\` was issued</span></span>
<span class="line"><span class="token comment"># 11:30:25 Hello world! via k8s-hello-world-55f48f8c94-npkn6</span></span>
<span class="line"><span class="token comment"># 11:30:27 Hello world! via k8s-hello-world-55f48f8c94-vrkr9</span></span>
<span class="line"><span class="token comment"># ^C</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>There was a gap of 29 seconds, from 11:29:56 to 11:30:25, but the cluster is back to normal.</p><p><em>Why does it take about 30 seconds to reply?</em></p><p><em>Is the node receiving traffic despite no routing table?</em></p><p>Maybe you could investigate what happens to the node in this 30 seconds.</p><p>In another terminal, you should write a loop to make requests to the application every second. But this time, you should request the node and not the load balancer:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token keyword">while</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">do</span> <span class="token builtin class-name">printf</span> %<span class="token string">&quot;s<span class="token entity" title="\\n">\\n</span>&quot;</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">curl</span> <span class="token parameter variable">-sS</span> http://<span class="token operator">&lt;</span>ip of the node<span class="token operator">&gt;</span>:30000<span class="token variable">)</span></span><span class="token punctuation">;</span></span>
<span class="line"><span class="token keyword">done</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>And let&#39;s drop the iptables rules. The log from the previous command is:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token keyword">while</span> <span class="token function">sleep</span> <span class="token number">1</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">do</span> <span class="token builtin class-name">printf</span> %<span class="token string">&quot;s<span class="token entity" title="\\n">\\n</span>&quot;</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">curl</span> <span class="token parameter variable">-sS</span> http://<span class="token operator">&lt;</span>ip of the node<span class="token operator">&gt;</span>:30000<span class="token variable">)</span></span><span class="token punctuation">;</span></span>
<span class="line"><span class="token keyword">done</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># Hello world! via k8s-hello-world-55f48f8c94-xzvlc</span></span>
<span class="line"><span class="token comment"># Hello world! via k8s-hello-world-55f48f8c94-tjg4n</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># this is when \`iptables -F\` was issued</span></span>
<span class="line"><span class="token comment"># curl: (28) Connection timed out after 10003 milliseconds</span></span>
<span class="line"><span class="token comment"># curl: (28) Connection timed out after 10004 milliseconds</span></span>
<span class="line"><span class="token comment"># Hello world! via k8s-hello-world-55f48f8c94-npkn6</span></span>
<span class="line"><span class="token comment"># Hello world! via k8s-hello-world-55f48f8c94-vrkr9</span></span>
<span class="line"><span class="token comment"># ^C</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>It shouldn&#39;t come as a surprise that connections to the node are timing out after you drop the iptables rules. What&#39;s more interesting is that <code>curl</code> waits for ten seconds before giving up.</p><p><em>What if in the previous example the load balancer is waiting for the connection to be made?</em></p><p>That would explain the 30 seconds delay. But it doesn&#39;t tell why the node is ready to accept a connection when you wait long enough.</p><p><em>So why is the traffic recovering after 30 seconds?</em></p><p><em>Who is putting the iptables rules back?</em></p><p>Before you drop the iptables rules, you can inspect them with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-L</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Soon after you drop the rules, you should keep executing <code>iptables -F</code> and notice that the rules are back in a few seconds!</p><p><em>Is this you, <code>kube-proxy</code>?</em></p><p>Yes, it is.</p>`,74)),n("p",null,[e[8]||(e[8]=s("Digging in the ")),n("a",P,[a(o,{icon:"iconfont icon-k8s"}),e[6]||(e[6]=s("official documentation for ")),e[7]||(e[7]=n("code",null,"kube-proxy",-1))]),e[9]||(e[9]=s(" reveals two interesting flags:"))]),e[22]||(e[22]=p(`<ul><li><code>--iptables-sync-period</code> - The maximum interval of how often iptables rules are refreshed (e.g. &#39;5s&#39;, &#39;1m&#39;, &#39;2h22m&#39;). Must be greater than 0. (default 30s)</li><li><code>--iptables-min-sync-period</code> - The minimum interval of how often the iptables rules can be refreshed as endpoints and services change (e.g. &#39;5s&#39;, &#39;1m&#39;, &#39;2h22m&#39;). (default 10s)</li></ul><p><code>kube-proxy</code> refreshes the iptables rules every 10 to 30 seconds.</p><p>If we drop the iptables rules, it will take up to 30 seconds for <code>kube-proxy</code> to realise and restore them back.</p><p>That explains why it took 30 seconds to get your node back!</p><p>It also explains how routing tables are propagated from the master node to the worker node.</p><p><code>kube-proxy</code> is in charge of syncing them on a regular basis.</p><p>In other words, every time a pod is added or deleted, the master node recomputes the routing list.</p><p>On a regular interval, <code>kube-proxy</code> syncs the rules into the current node.</p><p>Let&#39;s recap how Kubernetes and <code>kube-proxy</code> can recover from someone tampering with the iptables rules on the node:</p><ol><li>The iptables rules are deleted from the node</li><li>A request is forwarded to the load balancer and routed to the node</li><li>The node doesn&#39;t accept incoming requests, so the load balancer waits</li><li>After 30 seconds <code>kube-proxy</code> restores the iptables</li><li>The node can serve traffic again. The iptables rules forward the request from the load balancer to the pod</li><li>The pod replies to the load balancer with a 30 seconds delay</li></ol><p>Waiting for 30 seconds may be unacceptable for your application. You may be interested in tweaking the default refresh interval for <code>kube-proxy</code>.</p><p><em>So where are the settings and how can you change them?</em></p><p>It turns out that there&#39;s an agent on the node — <em>the kubelet</em> — that is in charge of starting <code>kube-proxy</code> as a static pod on each node.</p><p>The documentation for static pods suggests that the kubelet scans a specific folder and creates all the resources contained in that folder.</p><p>If you inspect the kubelet process in the node, you should be able to see the kubelet running with <code>--pod-manifest-path=/etc/kubernetes/manifests</code>.</p><p>Running a simple <code>ls</code> reveals the truth:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">ls</span> <span class="token parameter variable">-l</span> /etc/kubernetes/manifests</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># total 4 -rw-r--r-- 1 root root 1398 Feb 24 08:08 kube-proxy.manifest</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>And a quick <code>cat</code> of <code>kube-proxy.manifest</code> reveals the content:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="kube-proxy.manifest"><span>kube-proxy.manifest</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>proxy</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">hostNetwork</span><span class="token punctuation">:</span> <span class="token boolean important">true</span></span>
<span class="line">  <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>proxy</span>
<span class="line">    <span class="token key atrule">image</span><span class="token punctuation">:</span> gcr.io/google_containers/kube<span class="token punctuation">-</span>proxy<span class="token punctuation">:</span>v1.8.7<span class="token punctuation">-</span>gke.1</span>
<span class="line">    <span class="token key atrule">command</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> /bin/sh</span>
<span class="line">    <span class="token punctuation">-</span> <span class="token punctuation">-</span>c</span>
<span class="line">    <span class="token punctuation">-</span><span class="token punctuation">&gt;</span><span class="token scalar string"></span>
<span class="line">    echo -998 &gt; /proc/$$$/oom_score_adj &amp;&amp;</span>
<span class="line">    exec kube-proxy</span>
<span class="line">    --master=https://35.190.207.197</span>
<span class="line">    --kubeconfig=/var/lib/kube-proxy/kubeconfig</span>
<span class="line">    --cluster-cidr=10.4.0.0/14</span>
<span class="line">    --resource-container=&quot;&quot;</span>
<span class="line">    --v=2</span>
<span class="line">    --feature-gates=ExperimentalCriticalPodAnnotation=true</span>
<span class="line">    --iptables-sync-period=30s</span>
<span class="line">    1&gt;&gt;/var/log/kube-proxy.log 2&gt;&amp;1</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><div class="hint-container note"><p class="hint-container-title">Note</p><p>Please note that the content was truncated and is not shown in full.</p></div><p><strong>Mystery unravelled!</strong></p><p>You can see how <code>--iptables-sync-period=30s</code> is used to refresh the iptables rules every 30 seconds.</p><p>You could go ahead and modify that command to customise the min and max time to update the iptables rules for that node.</p><hr><h2 id="lessons-learned" tabindex="-1"><a class="header-anchor" href="#lessons-learned"><span>Lessons learned</span></a></h2><p>Dropping iptables rules is similar to make a node unavailable. The traffic is still routed to the node, but the node is not able to forward it further.</p><p>Kubernetes can recover from a similar failure by monitoring the state of the routing rules and updating them when necessary.</p>`,27)),n("p",null,[e[16]||(e[16]=s("Many thanks to ")),n("a",L,[e[10]||(e[10]=s("Manabu Sakai (")),a(o,{icon:"fa-brands fa-linkedin"}),e[11]||(e[11]=n("code",null,"manabusakai",-1)),e[12]||(e[12]=s(")"))]),e[17]||(e[17]=s("'s blog post that was a huge inspiration and to ")),n("a",A,[e[13]||(e[13]=s("Valentin Ouvrard (")),a(o,{icon:"fa-brands fa-linkedin"}),e[14]||(e[14]=n("code",null,"Valentin_NC",-1)),e[15]||(e[15]=s(")"))]),e[18]||(e[18]=s(" for investigating the issue with the iptables propagation."))]),e[23]||(e[23]=n("hr",null,null,-1)),e[24]||(e[24]=n("h2",{id:"that-s-all-folks",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#that-s-all-folks"},[n("span",null,"That's all folks!")])],-1)),e[25]||(e[25]=n("p",null,"If you enjoyed this article, you might find the following articles interesting:",-1)),a(r,c(d({title:"3 simple tricks for smaller Docker images",desc:"When it comes to building Docker containers, you should always strive for smaller images. Images that share layers and are smaller in size are quicker to transfer and deploy. But how do you keep the size under control when every RUN statement creates a new layer, and you need intermediate artefacts before the image is ready?",link:"/learnk8s.com/smaller-docker-images.md",logo:"https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg",background:"rgba(102,152,204,0.2)"})),null,16),e[26]||(e[26]=n("ul",null,[n("li",null,[n("a",{href:"https://learnk8s.io/blog/scaling-spring-boot-microservices",target:"_blank",rel:"noopener noreferrer"},"Scaling Microservices with Message Queues, Spring Boot and Kubernetes."),s(" Learn how to use the Horizontal Pod Autoscaler to resize your fleet of applications dynamically.")])],-1)),g(" TODO: add ARTICLE CARD "),a(r,c(d({title:"Kubernetes Chaos Engineering: Lessons Learned — Part 1",desc:"When you deploy an app in Kubernetes, your code ends up running on one or more worker nodes. But what happens when a node breaks and the network proxy crashes?",link:"https://chanhi2000.github.io/bookshelf/learnk8s.com/learnk8s.ioblogkubernetes-chaos-engineering-lessons-learned.html",logo:"https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg",background:"rgba(102,152,204,0.2)"})),null,16)])}const S=m(f,[["render",I]]),q=JSON.parse('{"path":"/learnk8s.io/kubernetes-chaos-engineering-lessons-learned.html","title":"Kubernetes Chaos Engineering: Lessons Learned — Part 1","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"Kubernetes Chaos Engineering: Lessons Learned — Part 1","description":"Article(s) > Kubernetes Chaos Engineering: Lessons Learned — Part 1","icon":"iconfont icon-k8s","category":["DevOps","VM","Kubernetes","Article(s)"],"tag":["blog","learnk8s.io","kubernetes","k8s"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Kubernetes Chaos Engineering: Lessons Learned — Part 1\\",\\"image\\":[\\"https://learnk8s.io/a/15261f4c090b83cef6a3f3c447e7d898.svg\\",\\"https://learnk8s.io/a/95fa67fa9ea88d3d6ce85ac95952ee33.svg\\",\\"https://learnk8s.io/a/5adaae4b4e1f1027d82c0ff164bd1633.svg\\",\\"https://learnk8s.io/a/05881ce7e8c56dc886227c50a56cb307.svg\\",\\"https://learnk8s.io/a/9b046391dbc619b6617161a248d6124b.svg\\",\\"https://learnk8s.io/a/07f12509e17ec374e92ae4808a43c37a.svg\\",\\"https://learnk8s.io/a/87836a9f146ea0596787c4db8cbc0e3b.svg\\",\\"https://learnk8s.io/a/ef529c736e7538daac10f7f358cbf307.svg\\",\\"https://learnk8s.io/a/785878ae3443b851659c00d2a1108894.svg\\"],\\"datePublished\\":\\"2019-04-15T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Daniele Polencic\\",\\"url\\":\\"https://linkedin.com/in/danielepolencic\\"}]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-chaos-engineering-lessons-learned.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"Kubernetes Chaos Engineering: Lessons Learned — Part 1"}],["meta",{"property":"og:description","content":"Article(s) > Kubernetes Chaos Engineering: Lessons Learned — Part 1"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://static.learnk8s.io/24f18c579e9cd93a6504110cd0e9ad65.png"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://static.learnk8s.io/24f18c579e9cd93a6504110cd0e9ad65.png"}],["meta",{"name":"twitter:image:alt","content":"Kubernetes Chaos Engineering: Lessons Learned — Part 1"}],["meta",{"property":"article:author","content":"Daniele Polencic"}],["meta",{"property":"article:tag","content":"k8s"}],["meta",{"property":"article:tag","content":"kubernetes"}],["meta",{"property":"article:tag","content":"learnk8s.io"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:published_time","content":"2019-04-15T00:00:00.000Z"}],[{"meta":null},{"property":"og:title","content":"Article(s) > Kubernetes Chaos Engineering: Lessons Learned — Part 1"},{"property":"og:description","content":"Kubernetes Chaos Engineering: Lessons Learned — Part 1"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-chaos-engineering-lessons-learned.html"}]],"prev":"/devops/k8s/articles/README.md","date":"2019-04-15T00:00:00.000Z","isOriginal":false,"author":[{"name":"Daniele Polencic","url":"https://linkedin.com/in/danielepolencic"}],"cover":"https://static.learnk8s.io/24f18c579e9cd93a6504110cd0e9ad65.png"},"git":{},"readingTime":{"minutes":9.4,"words":2819},"filePathRelative":"learnk8s.io/kubernetes-chaos-engineering-lessons-learned.md","copyright":{"author":"Daniele Polencic"}}');export{S as comp,q as data};
