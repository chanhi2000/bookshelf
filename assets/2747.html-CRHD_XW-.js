import{_ as d}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as u,d as n,f as i,b as g,t as f,n as L,g as b,w as a,e,r,o as M}from"./app-BVguHYKu.js";const k={},y={id:"frontmatter-title-관련",tabindex:"-1"},z={class:"header-anchor",href:"#frontmatter-title-관련"},P={class:"table-of-contents"},w={href:"https://ai.meta.com/blog/meta-llama-3-1/",target:"_blank",rel:"noopener noreferrer"},I={href:"https://medium.com/@florian_algo/model-quantization-1-basic-concepts-860547ec6aa9",target:"_blank",rel:"noopener noreferrer"},A={href:"https://medium.com/@olga.zem/exploring-the-impact-of-quantization-on-llm-performance-5698e16c5564",target:"_blank",rel:"noopener noreferrer"},T={href:"https://medium.com/@kanikaadik07/peft-parameter-efficient-fine-tuning-55e32c60c799",target:"_blank",rel:"noopener noreferrer"};function E(m,t){const s=r("VPCard"),o=r("router-link"),p=r("SiteInfo"),l=r("VPIcon");return M(),u("div",null,[n("h1",y,[n("a",z,[n("span",null,f(m.$frontmatter.title)+" 관련",1)])]),i(s,L(b({title:"LLM > Article(s)",desc:"Article(s)",link:"/ai/llm/articles/README.md",logo:"https://chanhi2000.github.io/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),n("nav",P,[n("ul",null,[n("li",null,[i(o,{to:"#대표적인-llm-경량화-테크닉"},{default:a(()=>t[0]||(t[0]=[e("대표적인 LLM 경량화 테크닉")])),_:1,__:[0]}),n("ul",null,[n("li",null,[i(o,{to:"#양자화-quantization-란"},{default:a(()=>t[1]||(t[1]=[e("양자화(quantization)란?")])),_:1,__:[1]})]),n("li",null,[i(o,{to:"#peft-parameter-efficient-fine-tuning-란"},{default:a(()=>t[2]||(t[2]=[e("PEFT(Parameter Efficient Fine-Tuning)란?")])),_:1,__:[2]})])])]),n("li",null,[i(o,{to:"#마치며-llm-경량화-테크닉은-만능이-아니다"},{default:a(()=>t[3]||(t[3]=[e("마치며: LLM 경량화 테크닉은 만능이 아니다")])),_:1,__:[3]})])])]),t[28]||(t[28]=n("hr",null,null,-1)),i(p,{name:"AI 스마트폰의 핵심, LLM 경량화 테크닉 | 요즘IT",desc:"최근 갤럭시 s24를 시작으로 서버나 클라우드에 연동할 필요 없이 스마트폰에서 직접 생성형 AI 기능을 활용할 수 있는 ‘AI 스마트폰’이 관심을 모으고 있습니다. 하지만 초거대언어모델(LLM)을 제대로 활용하려면 천문학적인 양의 연산을 처리하기 위한 병렬 처리 기술과 고성능 GPU로 무장한 서버/데이터 센터가 필수입니다. 그런데 여기서 의문이 생깁니다. 이 커다란 LLM 모델이 어떻게 그 작은 스마트폰 속으로 들어갈 수 있었을까요? 그 해답은 바로 ‘LLM 경량화 테크닉’에 있습니다.",url:"https://yozm.wishket.com/magazine/detail/2747/",logo:"https://yozm.wishket.com/favicon.ico",preview:"https://yozm.wishket.com/media/news/2747/3d-rendering-biorobots-concept__2_.jpg"}),t[29]||(t[29]=n("p",null,"최근 갤럭시 S24를 시작으로 서버나 클라우드에 연동할 필요 없이 스마트폰에서 직접 생성형 AI 기능을 활용할 수 있는 ‘AI 스마트폰’이 관심을 모으고 있습니다. 삼성전자는 갤럭시 스마트폰 기기에 자체 개발한 초거대언어모델(이하 LLM) 삼성 가우스(Gauss)를 탑재하여 실시간 자동 번역, 채팅, 사진 편집 등 다양한 기능을 제공하고 있습니다. 애플 역시 자사 AI 플랫폼 ‘애플 인텔리전스’에 GPT, 제미나이(Gemini), 클로드(Claude) 등 다양한 LLM을 활용한 서비스를 제공할 계획을 밝혔죠.",-1)),t[30]||(t[30]=n("p",null,"LLM은 그 이름에서 알 수 있듯 거대한 모델 사이즈와 방대한 양의 훈련 데이터에서 나오는 압도적인 성능이 특징입니다. 일반적으로 언어 모델은 심층신경망(DNN)의 가중치(weight)와 편향(bias)의 정보를 담고 있는 매개 변수(parameter)가 많으면 많을수록 성능이 좋습니다. 이를 통해 언어의 문맥과 세부적인 뉘앙스까지 파악해 추론할 수 있죠. 그에 따라 자연스럽게 LLM은 매개 변수의 양을 늘려가는 방식으로 발전해 왔습니다.",-1)),t[31]||(t[31]=n("p",null,"매개 변수 확대로 성능 개선을 노리는 경향은 비단 GPT-4o나 클로드 소넷(Sonnet)처럼 API로 제공되며 엄청난 크기를 가진 모델에서만 나타나지 않습니다. 얼마 전 출시된 메타의 LLaMA 3.1은 경량화와 가성비를 추구하던 오픈 소스 모델로서는 다소 파격적으로 4,050억 개(405B)의 매개 변수를 자랑하며 개발자들 사이에서 화제가 되었습니다.",-1)),n("figure",null,[t[9]||(t[9]=n("img",{src:"https://yozm.wishket.com/media/news/2747/image3.png",alt:'웬만한 대형 LLM보다 뛰어난 성능을 자랑하는 오픈 소스 기반 Llama3.1 405B 모델<br/><출처: <VPIcon icon="fa-brands fa-meta"/>Meta>',tabindex:"0",loading:"lazy"},null,-1)),n("figcaption",null,[t[5]||(t[5]=e("웬만한 대형 LLM보다 뛰어난 성능을 자랑하는 오픈 소스 기반 Llama3.1 405B 모델")),t[6]||(t[6]=n("br",null,null,-1)),t[7]||(t[7]=e("<출처: ")),n("a",w,[i(l,{icon:"fa-brands fa-meta"}),t[4]||(t[4]=e("Meta"))]),t[8]||(t[8]=e(">"))])]),t[32]||(t[32]=g('<p>이러한 LLM을 활용하려면 천문학적인 양의 연산을 처리하기 위한 병렬 처리 기술과 고성능 GPU로 무장한 서버/데이터 센터가 필수입니다. 그런데 여기서 의문이 생깁니다. 이 커다란 LLM 모델이 어떻게 그 작은 스마트폰 속으로 들어갈 수 있었을까요? 그 해답은 바로 ‘LLM 경량화 테크닉’에 있습니다.</p><hr><h2 id="대표적인-llm-경량화-테크닉" tabindex="-1"><a class="header-anchor" href="#대표적인-llm-경량화-테크닉"><span>대표적인 LLM 경량화 테크닉</span></a></h2><p>LLM의 커다란 사이즈로 발생하는 문제를 해결하기 위해 언어 모델 연구자와 개발자들은 모델의 사이즈를 줄이면서도 성능을 비슷하게 유지할 방법을 고심하기 시작했습니다. 곧 “매개 변수가 큰 모델을 경량화해서 사용하는 것이 대체로 매개 변수가 작은 모델을 그대로 사용하는 것보다 성능이 좋다.”라는 결론을 내리게 됩니다.</p><p>LLM 경량화 테크닉의 기초를 이해하려면 우선 변수 형태 변환으로 모델 성능을 거의 동일하게 유지하면서 크기를 줄이는 ‘양자화(quantization)’ 개념을 아는 것이 중요합니다. 그다음으로 최근 실전에서 많이 쓰이는 대표적인 경량화 테크닉인 파인튜닝 기술, ‘PEFT(Parameter Efficient Fine-Tuning)’ 개념을 소개하겠습니다.</p><h3 id="양자화-quantization-란" tabindex="-1"><a class="header-anchor" href="#양자화-quantization-란"><span>양자화(quantization)란?</span></a></h3><p>마블(Marble) 영화 ‘앤트맨’의 주인공 스콧은 슈트에 달린 버튼으로 ‘핌 입자’를 활성화해 몸의 크기를 줄일 수 있습니다. 왼손의 버튼으로 축소했다 오른손의 버튼으로 돌아가는 것이 가능하죠. 개미만한 크기로 작아진 앤트맨은 마치 초인과 같은 힘을 냅니다. 남성 한 명의 전체 질량을 유지한 상태에서 극도로 좁아진 타격 면적에 직격하면 엄청난 위력을 낼 수 있기 때문입니다.</p><p>LLM도 마치 앤트맨처럼 작게 줄일 수 있다는 사실을 아시나요? 양자화는 강력한 LLM을 보다 작은 크기로 축소해 활용할 수 있게 해줍니다. 정확한 이해를 위해 조금 더 기술적 관점에서 양자화에 대해 이야기해보도록 하겠습니다.</p><p>**양자화(quantization)**란 LLM의 매개 변수를 실수형 변수(floating-point type)에서 정수형 변수(integer or fixed point)로 변환해 더 작은 비트로 바꾸는 과정을 말합니다. 양자화를 거친 LLM은 거의 동일한 성능을 가지지만 실제 사이즈보다 작은 모델처럼 보이도록 변환됩니다. 예를 들어 원래 32비트 부동 소수점을 가지고 있는 모델의 매개 변수를 8비트 정수로 변환하는 것처럼, 언어 모델이 가진 각각의 매개 변수 비트 수를 줄여주는 것입니다.</p>',9)),n("figure",null,[t[15]||(t[15]=n("img",{src:"https://yozm.wishket.com/media/news/2747/image1.png",alt:'FP32에서 INT8로 변환 <br/><출처: <VPIcon icon="fa-brands fa-medium"/>Florian June 미디엄 블로그>',tabindex:"0",loading:"lazy"},null,-1)),n("figcaption",null,[t[11]||(t[11]=e("FP32에서 INT8로 변환 ")),t[12]||(t[12]=n("br",null,null,-1)),t[13]||(t[13]=e("<출처: ")),n("a",I,[i(l,{icon:"fa-brands fa-medium"}),t[10]||(t[10]=e("Florian June 미디엄 블로그"))]),t[14]||(t[14]=e(">"))])]),t[33]||(t[33]=n("p",null,"이렇게 양자화를 거친 LLM은 크기가 줄어들 뿐만 아니라 계산 효율성 역시 좋아집니다. 보통 비트 수를 N배 줄이면 곱셈 복잡도는 NxN 만큼 줄어들죠. 예시처럼 float32 대신 int8을 쓰면 단순 계산만으로도 모델 크기가 1/4까지 작아집니다. 이에 따라 추론 속도 역시 2배에서 4배까지 빨라집니다. 마찬가지로 2배에서 4배까지 적은 메모리로도 비슷한 성능의 LLM 연산이 가능해집니다.",-1)),t[34]||(t[34]=n("p",null,"이처럼 양자화는 LLM 크기를 줄여 주고 계산 효율성을 향상시켜 줍니다. 그러나 양자화로 모델의 성능 저하가 생길 수 있다는 점을 염두에 둬야 합니다. 양자화의 목적은 훈련 및 계산 비용을 줄이면서도 성능을 유지하는 것으로 매개 변수의 일부만을 조정하여 활용하기 때문에 원래 모델과 동일한 성능을 보장하지는 않습니다. 따라서 양자화 모델은 어디까지나 가성비에 중점을 두어야 하며 사용 전에 철저한 모델 평가가 필요합니다.",-1)),n("figure",null,[t[21]||(t[21]=n("img",{src:"https://yozm.wishket.com/media/news/2747/image2.png",alt:'양자화에 따른 성능 저하가 발생할 수 있다 <br/><출처: <VPIcon icon="fa-brands fa-medium"/>Olga Zem 미디엄 블로그>',tabindex:"0",loading:"lazy"},null,-1)),n("figcaption",null,[t[17]||(t[17]=e("양자화에 따른 성능 저하가 발생할 수 있다 ")),t[18]||(t[18]=n("br",null,null,-1)),t[19]||(t[19]=e("<출처: ")),n("a",A,[i(l,{icon:"fa-brands fa-medium"}),t[16]||(t[16]=e("Olga Zem 미디엄 블로그"))]),t[20]||(t[20]=e(">"))])]),t[35]||(t[35]=n("h3",{id:"peft-parameter-efficient-fine-tuning-란",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#peft-parameter-efficient-fine-tuning-란"},[n("span",null,"PEFT(Parameter Efficient Fine-Tuning)란?")])],-1)),t[36]||(t[36]=n("p",null,"생성형 AI 열풍으로 최근 ‘파인튜닝(Fine-tuning)’이라는 단어를 한 번쯤 들어봤을 것입니다. GPT와 같은 LLM 기초 모델은 두 가지 단계를 거쳐 만들어집니다. 첫째, 백지상태 모델에 방대한 양의 텍스트 데이터를 학습시켜 전반적인 언어 능력 및 생성 능력을 향상시키는 사전학습(pre-training)과 둘째, 비교적 적은 데이터셋을 활용해 LLM을 특정 분야의 전문 지식과 태스크에 특화시키는 파인튜닝이죠.",-1)),t[37]||(t[37]=n("p",null,"PEFT(Parameter Efficient Fine-Tuning)란 적은 수의 매개 변수 학습만으로 빠른 시간에 새로운 문제를 효과적으로 해결하는 파인튜닝 기법을 말합니다. 최근 적게는 수천억 개에서 많게는 조 단위 이상의 매개 변수를 가진 LLM도 파라미터 일부만 조절하여 유사한 성능을 낼 수 있다는 연구 결과가 있었습니다. 이를 기반으로 PEFT 방법론 연구가 활발히 진행되는 중이죠.",-1)),n("figure",null,[t[27]||(t[27]=n("img",{src:"https://yozm.wishket.com/media/news/2747/image4.png",alt:'PEFT 기법 <br/><출처: <VPIcon icon="fa-brands fa-medium"/>kanika adik 미디엄 블로그>',tabindex:"0",loading:"lazy"},null,-1)),n("figcaption",null,[t[23]||(t[23]=e("PEFT 기법 ")),t[24]||(t[24]=n("br",null,null,-1)),t[25]||(t[25]=e("<출처: ")),n("a",T,[i(l,{icon:"fa-brands fa-medium"}),t[22]||(t[22]=e("kanika adik 미디엄 블로그"))]),t[26]||(t[26]=e(">"))])]),t[38]||(t[38]=n("p",null,"PEFT의 중요한 특징은 파인튜닝 시 전체 모델을 조정하는 것보다 적은 계산 자원과 데이터만을 사용한다는 점입니다. 이는 다양한 언어와 도메인 데이터에 모델을 적용할 때 특히 유용합니다. 각 언어 또는 도메인별로 작은 체크포인트만 로컬에 저장하면 효율적으로 작동하기 때문이죠. 따라서 AI 스마트폰처럼 다운로드나 업데이트의 제한이 있는 환경에서 모델의 다양성을 유지하는 데에 큰 도움이 됩니다.",-1)),t[39]||(t[39]=n("p",null,"또한 PEFT는 다양한 태스크에 모델을 신속히 적용하고자 하는 연구자나 개발자에게 특히 유용합니다. 기존에 학습된 LLM 위에 추가 레이어를 덧붙여 미세 조정하며 새로운 작업에 대한 모델 적용 및 평가가 가능하기 때문입니다.",-1)),t[40]||(t[40]=n("hr",null,null,-1)),t[41]||(t[41]=n("h2",{id:"마치며-llm-경량화-테크닉은-만능이-아니다",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#마치며-llm-경량화-테크닉은-만능이-아니다"},[n("span",null,"마치며: LLM 경량화 테크닉은 만능이 아니다")])],-1)),t[42]||(t[42]=n("p",null,"이처럼 LLM 경량화 테크닉은 모바일 기기나 기타 제한된 자원을 사용하는 환경에서 생성형 AI 기능을 효율적으로 활용하는 데 매우 효과적입니다. 특히 실시간 자동 번역, 채팅, 사진 편집 등 다양한 기능을 제공해야 하는 AI 스마트폰에서는 모델 경량화 테크닉이 필수죠. 이 기술이 거대한 모델 사이즈라는 LLM의 고질적인 문제점을 해결해 생성형 AI 확산에 기여할 수 있을 것으로 기대됩니다.",-1)),t[43]||(t[43]=n("p",null,"그러나 양자화나 PEFT 같은 LLM 경량화 테크닉은 만능이 아닙니다. 기술을 활용할 때 그 한계점을 이해하는 것은 매우 중요합니다.",-1)),t[44]||(t[44]=n("p",null,"예를 들어 양자화의 경우, 모델의 매개 변수를 정수형으로 변환하는 과정에서 일부 정보 손실이 발생할 수 있습니다. 따라서 특히 복잡한 언어 이해나 생성 작업에서는 이로 인한 성능 저하가 생길 수 있음을 충분히 인지하고 활용해야 합니다. 마찬가지로 PEFT 역시 적은 수의 매개 변수만을 조정하기 때문에 전체 모델을 튜닝하는 것보다 특정 태스크에서의 최적 성능을 달성하기 어려울 수 있습니다. 경량화 테크닉을 내 제품에 활용할 예정인 개발자라면 이 사실을 꼭 염두에 두고 적용 방법을 모색하기 바랍니다.",-1))])}const h=d(k,[["render",E]]),N=JSON.parse('{"path":"/yozm.wishket.com/2747.html","title":"AI 스마트폰의 핵심, LLM 경량화 테크닉","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"AI 스마트폰의 핵심, LLM 경량화 테크닉","description":"Article(s) > AI 스마트폰의 핵심, LLM 경량화 테크닉","icon":"fas fa-language","category":["AI","LLM","Article(s)"],"tag":["blog","yozm.wishket.com","ai","artificial-intelligence","llm","large-language-model"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"AI 스마트폰의 핵심, LLM 경량화 테크닉\\",\\"image\\":[\\"https://ai.meta.com/blog/meta-llama-3-1/\\",\\"https://medium.com/@florian_algo/model-quantization-1-basic-concepts-860547ec6aa9\\",\\"https://medium.com/@olga.zem/exploring-the-impact-of-quantization-on-llm-performance-5698e16c5564\\",\\"https://medium.com/@kanikaadik07/peft-parameter-efficient-fine-tuning-55e32c60c799\\"],\\"datePublished\\":\\"2024-09-05T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/yozm.wishket.com/2747.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"AI 스마트폰의 핵심, LLM 경량화 테크닉"}],["meta",{"property":"og:description","content":"Article(s) > AI 스마트폰의 핵심, LLM 경량화 테크닉"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://yozm.wishket.com/media/news/2747/3d-rendering-biorobots-concept__2_.jpg"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://yozm.wishket.com/media/news/2747/3d-rendering-biorobots-concept__2_.jpg"}],["meta",{"name":"twitter:image:alt","content":"AI 스마트폰의 핵심, LLM 경량화 테크닉"}],["meta",{"property":"article:tag","content":"large-language-model"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"artificial-intelligence"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:tag","content":"yozm.wishket.com"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:published_time","content":"2024-09-05T00:00:00.000Z"}],[{"meta":null},{"property":"og:title","content":"Article(s) > AI 스마트폰의 핵심, LLM 경량화 테크닉"},{"property":"og:description","content":"AI 스마트폰의 핵심, LLM 경량화 테크닉"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/yozm.wishket.com/2747.html"}]],"prev":"/ai/llm/articles/README.md","date":"2024-09-05T00:00:00.000Z","isOriginal":false,"cover":"https://yozm.wishket.com/media/news/2747/3d-rendering-biorobots-concept__2_.jpg"},"git":{},"readingTime":{"minutes":1.19,"words":358},"filePathRelative":"yozm.wishket.com/2747.md"}');export{h as comp,N as data};
