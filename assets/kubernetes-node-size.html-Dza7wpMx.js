import{_ as v}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as k,d as n,f as t,b as p,a as y,t as w,n as h,g as b,w as a,e as s,r as d,o as P}from"./app-BItykJLQ.js";const x={},I={id:"frontmatter-title-관련",tabindex:"-1"},C={class:"header-anchor",href:"#frontmatter-title-관련"},A={class:"table-of-contents"},B={href:"https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#scheduling-cycle-binding-cycle",target:"_blank",rel:"noopener noreferrer"},T={href:"https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/",target:"_blank",rel:"noopener noreferrer"},G={href:"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#should-i-use-a-cpu-usage-based-node-autoscaler-with-kubernetes",target:"_blank",rel:"noopener noreferrer"},z={href:"https://hub.docker.com/_/openjdk",target:"_blank",rel:"noopener noreferrer"},K={href:"https://github.com/rpardini/docker-registry-proxy",target:"_blank",rel:"noopener noreferrer"},U={href:"https://github.com/XenitAB/spegel",target:"_blank",rel:"noopener noreferrer"},S={href:"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/#:~:text=%2D%2Dnode%2Dstatus%2Dupdate%2Dfrequency",target:"_blank",rel:"noopener noreferrer"},q={href:"https://kubernetes.io/blog/2023/05/15/speed-up-pod-startup/#raised-default-api-query-per-second-limits-for-kubelet",target:"_blank",rel:"noopener noreferrer"},R={href:"https://kubernetes.io/docs/setup/best-practices/cluster-large/",target:"_blank",rel:"noopener noreferrer"},D={href:"https://cloud.google.com/blog/products/containers-kubernetes/google-kubernetes-engine-clusters-can-have-up-to-15000-nodes",target:"_blank",rel:"noopener noreferrer"},W={href:"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#:~:text=Default%3A%20%22promiscuous%2Dbridge%22-,maxPods,-int32",target:"_blank",rel:"noopener noreferrer"},L={href:"https://learnkube.com/kubernetes-instance-calculator/",target:"_blank",rel:"noopener noreferrer"};function N(g,e){const c=d("VPCard"),r=d("router-link"),f=d("SiteInfo"),l=d("VPIcon"),m=d("RouteLink"),u=d("Tabs");return P(),k("div",null,[n("h1",I,[n("a",C,[n("span",null,w(g.$frontmatter.title)+" 관련",1)])]),t(c,h(b({title:"Kubernetes > Article(s)",desc:"Article(s)",link:"/devops/k8s/articles/README.md",logo:"https://chanhi2000.github.io/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),n("nav",A,[n("ul",null,[n("li",null,[t(r,{to:"#cluster-capacity"},{default:a(()=>[...e[0]||(e[0]=[s("Cluster capacity",-1)])]),_:1})]),n("li",null,[t(r,{to:"#reserved-resource-in-kubernetes-worker-nodes"},{default:a(()=>[...e[1]||(e[1]=[s("Reserved resource in Kubernetes worker nodes",-1)])]),_:1})]),n("li",null,[t(r,{to:"#resource-allocations-and-efficiency-in-worker-nodes"},{default:a(()=>[...e[2]||(e[2]=[s("Resource allocations and efficiency in worker nodes",-1)])]),_:1})]),n("li",null,[t(r,{to:"#resiliency-and-replication"},{default:a(()=>[...e[3]||(e[3]=[s("Resiliency and replication",-1)])]),_:1})]),n("li",null,[t(r,{to:"#scaling-increments-and-lead-time"},{default:a(()=>[...e[4]||(e[4]=[s("Scaling increments and lead time",-1)])]),_:1})]),n("li",null,[t(r,{to:"#pulling-containers-images"},{default:a(()=>[...e[5]||(e[5]=[s("Pulling containers images",-1)])]),_:1})]),n("li",null,[t(r,{to:"#kubelet-and-scaling-the-kubernetes-api"},{default:a(()=>[...e[6]||(e[6]=[s("Kubelet and scaling the Kubernetes API",-1)])]),_:1})]),n("li",null,[t(r,{to:"#node-and-cluster-limits"},{default:a(()=>[...e[7]||(e[7]=[s("Node and cluster limits",-1)])]),_:1})]),n("li",null,[t(r,{to:"#storage"},{default:a(()=>[...e[8]||(e[8]=[s("Storage",-1)])]),_:1})]),n("li",null,[t(r,{to:"#summary-and-conclusions"},{default:a(()=>[...e[9]||(e[9]=[s("Summary and conclusions",-1)])]),_:1})])])]),e[81]||(e[81]=n("hr",null,null,-1)),t(f,{name:"Architecting Kubernetes clusters — choosing a worker node size",desc:"What type of worker nodes should I use for my Kubernetes cluster? And how many of them?. This article looks at the pros and cons.",url:"https://learnkube.com/kubernetes-node-size",logo:"https://static.learnkube.com/f7e5160d4744cf05c46161170b5c11c9.svg",preview:"https://static.learnkube.com/a102852d1e938e7c95a134501111ed92.png"}),e[82]||(e[82]=p('<div class="hint-container important"><p class="hint-container-title">TL;DR</p><p>Should you have a Kubernetes cluster with fewer larger nodes or many smaller nodes? This article discusses the pros and cons.</p></div><p><strong>When you create a Kubernetes cluster, one of the first questions you may have is: &quot;What type of worker nodes should I use, and how many of them?&quot;</strong></p><p><em>If you&#39;re building an on-premises cluster, should you order some last-generation power servers or use the dozen or so old machines that are lying around in your data centre?</em></p><p>Or if you&#39;re using a managed Kubernetes service like Google Kubernetes Engine (GKE), should you use eight <code>n1-standard-1</code> or two <code>n1-standard-4</code> instances to achieve your desired computing capacity?</p><hr><h2 id="cluster-capacity" tabindex="-1"><a class="header-anchor" href="#cluster-capacity"><span>Cluster capacity</span></a></h2><p>In general, a Kubernetes cluster can be seen as abstracting a set of individual nodes as a big &quot;super node&quot;.</p><p>This super node&#39;s total compute capacity (CPU and memory) is the sum of all the constituent nodes&#39; capacities.</p><p>There are multiple ways to achieve this.</p><p>For example, imagine you need a cluster with a total capacity of 8 CPU cores and 32 GB of RAM.</p><p>Here are just two of the possible ways to design your cluster:</p><figure><img src="https://learnkube.com/a/c642b260295b87df85d97a6e8c20be48.svg" alt="Small vs. large nodes in a Kubernetes cluster" tabindex="0" loading="lazy"><figcaption>Small vs. large nodes in a Kubernetes cluster</figcaption></figure><p>Both options result in a cluster with the same capacity — but the left option uses four smaller nodes, whereas the right one uses two larger nodes.</p><p><em>Which is better?</em></p><p>Let&#39;s start by reviewing how resources are allocated in a worker node.</p><hr><h2 id="reserved-resource-in-kubernetes-worker-nodes" tabindex="-1"><a class="header-anchor" href="#reserved-resource-in-kubernetes-worker-nodes"><span>Reserved resource in Kubernetes worker nodes</span></a></h2><p>Each worker node in a Kubernetes cluster is a compute unit that runs the kubelet — the Kubernetes agent.</p><p>The kubelet is a binary that connects to the control plane and keeps the node&#39;s current state in sync with the state of the cluster.</p><p>For example, when the Kubernetes scheduler assigns a pod to a particular node, it doesn&#39;t send a message to the kubelet.</p>',20)),n("p",null,[e[11]||(e[11]=s("Instead, ",-1)),n("a",B,[t(l,{icon:"iconfont icon-k8s"}),e[10]||(e[10]=s("it writes a Binding object and stores it in etcd.",-1))])]),e[83]||(e[83]=p('<p>The kubelet checks the state of the cluster on a regular schedule, and as soon as it notices a new pod assigned to its node, it proceeds to download the pod specification and create it.</p><p>The kubelet is often deployed as a SystemD service and runs as part of the operating system.</p><p>Kubelet, SystemD, and operating system need resources such as CPU and memory to function correctly.</p><p><strong>Consequently, not all resources from your worker nodes are available for running pods.</strong></p><p>CPU and memory resources are usually reparted as follows:</p><ol><li>Operating system.</li><li>Kubelet.</li><li>Pods.</li><li>Eviction threshold.</li></ol><figure><img src="https://learnkube.com/a/d627f4247a50662c83a2a40703a8b693.svg" alt="Resource allocations a in a Kubernetes node" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node</figcaption></figure><p>You might wonder what resources are assigned to each of those.</p><p>While those tend to be configurable, most of the time, the CPU is reserved with the following allocations:</p><ul><li>6% of the first core.</li><li>1% of the following core (up to 2 cores).</li><li>0.5% of the following two cores (up to 4).</li><li>0.25% of any cores above four cores.</li></ul><p>For the memory, it could look like this:</p><ul><li>255 MiB of memory for machines with less than 1 GB.</li><li>25% of the first 4GB of memory.</li><li>20% of the following 4GB of memory (up to 8GB).</li><li>10% of the following 8GB of memory (up to 16GB).</li><li>6% of the next 112GB of memory (up to 128GB).</li><li>2% of any memory above 128GB.</li></ul><p>Finally, the eviction threshold is usually 100MB.</p><p><em>What&#39;s the eviction threshold?</em></p>',14)),n("p",null,[n("a",T,[t(l,{icon:"iconfont icon-k8s"}),e[12]||(e[12]=s("It's a threshold for memory usage",-1))]),e[13]||(e[13]=s(" — if the node crosses that threshold, the kubelet starts evicting pods because there isn't enough memory in the current node.",-1))]),e[84]||(e[84]=p(`<p><em>Let&#39;s have a look at an example.</em></p><p>For an 8GB and 2 vCPU instance, the available resources are reparted as follows:</p><ol><li>70m vCPU and 1.8GB for the kubelet and operating system (those are usually bundled together).</li><li>100MB for the eviction threshold.</li><li>The remaining 6.1GB memory and 1930 millicores can be used by pods.</li></ol><p>Only 75% of the total memory is used to run workloads.</p><figure><img src="https://learnkube.com/a/3de0f4647a4b0d71b196d4e394aa5451.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 8GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 8GB of memory</figcaption></figure><p><em>But it doesn&#39;t end there.</em></p><p>Your node may need to run pods on every node (e.g. DaemonSets) to function correctly, and those consume memory and CPU too.</p><p>Examples include Kube-proxy, a log agent such as Fluentd or Fluent Bit, NodeLocal DNSCache or a CSI driver.</p><p><strong>This is a fixed cost you must pay regardless of the node size.</strong></p><figure><img src="https://learnkube.com/a/e77f8c687be1e38bd35470e177e4290a.svg" alt="Resource allocations a in a Kubernetes node with DaemonSets" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with DaemonSets</figcaption></figure><p>With this in mind, let&#39;s examine the pros and cons of the two opposing directions of &quot;few large nodes&quot; and &quot;many small nodes&quot;.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>Note that &quot;nodes&quot; in this article always refers to worker nodes. The choice of number and size of control plane nodes is an entirely different topic.</p></div><hr><h2 id="resource-allocations-and-efficiency-in-worker-nodes" tabindex="-1"><a class="header-anchor" href="#resource-allocations-and-efficiency-in-worker-nodes"><span>Resource allocations and efficiency in worker nodes</span></a></h2><p>Resources reserved by the kubelet decrease with larger instances.</p><p>Let&#39;s have a look at two extreme scenarios.</p><p>You want to deploy seven replicas for an application with requests of 0.3 vCPU and 2GB of memory.</p><ol><li>In the first scenario, you provision a single worker node to deploy all replicas.</li><li>In the second scenario, you deploy a replica per node.</li></ol><div class="hint-container note"><p class="hint-container-title">Note</p><p>For the sake of simplicity, we will assume that no DaemonSets are running on those nodes.</p></div><p>The total resources needed by seven replicas are 2.1 vCPU and 14GB of memory (i.e. <code>7 x 300m = 2.1 vCPU</code> and <code>7 x 2GB = 14GB</code>).</p><p><em>Can a 4 vCPU and 16GB instance run the workloads?</em></p><p>Let&#39;s do the math for the CPU reserved:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">6% of the first core        = 60m +</span>
<span class="line">1% of the second core       = 10m +</span>
<span class="line">0.5% of the remaining cores = 10m</span>
<span class="line">---------------------------------</span>
<span class="line">total                       = 80m</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The available CPU for running pods is 3.9 vCPU (i.e. <code>4000m - 80m</code>) — more than enough.</p><p>Let&#39;s check the memory reserved for the kubelet:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">25% of the first 4GB of memory = 1GB</span>
<span class="line">20% of the following 4GB of memory  = 0.8GB</span>
<span class="line">10% of the following 8GB of memory  = 0.8GB</span>
<span class="line">--------------------------------------</span>
<span class="line">total                          = 2.8GB</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The total memory available to pods is <code>16GB - (2.8GB + 0.1GB)</code> — where 0.1GB takes into account the 100MB of eviction threshold.</p><p>Finally, pods can consume up to 13.1GB of memory.</p><figure><img src="https://learnkube.com/a/c72727b60ee6387d73c40004f9003561.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory</figcaption></figure><p><strong>Unfortunately, this is not enough (i.e. 7 replicas require 14GB of memory, but you have only 13.1GB), and you should provision a compute unit with more memory to deploy the workloads.</strong></p><p>If you use a cloud provider, the next available increment for the compute unit is 4 vCPU and 32GB of memory.</p><figure><img src="https://learnkube.com/a/c1be9a8de0824f6e5abcba5371bb9b4e.svg" alt="A node with 2 vCPU and 16GB of memory is insufficient to run seven replicas" tabindex="0" loading="lazy"><figcaption>A node with 2 vCPU and 16GB of memory is insufficient to run seven replicas</figcaption></figure><p><em>Excellent!</em></p><p>Let&#39;s look at the other scenario where we try to find the smallest instance that could fit a single replica with requests equal to 0.3 vCPU and 2GB of memory.</p><p><strong>Let&#39;s try with an instance type with 1 vCPU and 4GB of memory.</strong></p><p>The total reserved CPU is 6% or 60m, and the available CPU to pods is 940m.</p><p>Since the app only requires 300m of CPU, this is enough.</p><p>The reserved memory for the kubelet is 25% or 1GB plus an additional 0.1GB of eviction threshold.</p><p>The total available memory for pods is 2.9GB; since the app only requires 2GB, this value is sufficient.</p><p><em>Great!</em></p><figure><img src="https://learnkube.com/a/f97ca286843a3ff5ee0fbc9a55630829.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory</figcaption></figure><p><em>Let&#39;s compare the two setups.</em></p><p>The total resources for the first cluster are just a single node — 4vCPU and 32 GB.</p><p>The second cluster has seven instances with 1 vCPU and 4GB of memory (for a total of 7 vCPU and 28 GB of memory).</p><p>In the first example, 2.9GB of memory and 80m of CPU are reserved for Kubernetes.</p><p>In the second, 7.7GB (1.1GB x 7 instances) of memory and 360m of CPU (60m x 7 instances) are reserved.</p><p><strong>You can already notice how resources are utilized more efficiently when provisioning larger nodes.</strong></p><figure><img src="https://learnkube.com/a/23885fd9cefd09159d3bab9618a55aae.svg" alt="Comparing resource allocations between a cluster with a single node and another with multiple nodes" tabindex="0" loading="lazy"><figcaption>Comparing resource allocations between a cluster with a single node and another with multiple nodes</figcaption></figure><p><em>But there&#39;s more to it.</em></p><p>The larger instance still has space to run more replicas <em>— but how many?</em></p><ul><li>The reserved memory is 3.66GB (3.56GB kubelet + 0.1GB eviction threshold), and the total available memory to pods is 28.44GB.</li><li>The reserved CPU is still 80m, and pods can use 3920m.</li></ul><p>At this point, you can find the max number of replicas for memory and CPU with the following division:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">Total CPU   3920 /</span>
<span class="line">Pod CPU      300</span>
<span class="line">------------------</span>
<span class="line">Max Pod       13.1</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can repeat the calculation for the memory:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">Total memory  28.44 /</span>
<span class="line">Pod memory     2</span>
<span class="line">---------------------</span>
<span class="line">Max Pod       14.22</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The above numbers suggest you run out of CPU before memory and can host up to 13 pods in the 4 vCPU and 32GB worker node.</p><figure><img src="https://learnkube.com/a/8166b554c895b638a68d2ef76475f943.svg" alt="Calculating the pod capacity for a 2 vCPU and 32GB worker node" tabindex="0" loading="lazy"><figcaption>Calculating the pod capacity for a 2 vCPU and 32GB worker node</figcaption></figure><p><em>What about the second scenario?</em></p><p><em>Is there any room to scale?</em></p><p>Not really.</p><p><strong>While the instances still have more CPU, they only have 0.9GB of memory available after you deploy the first pod.</strong></p><figure><img src="https://learnkube.com/a/2e379971f411f7ca2908ebed0798c020.svg" alt="Calculating the pod capacity for a 1 vCPU and 4GB worker node" tabindex="0" loading="lazy"><figcaption>Calculating the pod capacity for a 1 vCPU and 4GB worker node</figcaption></figure><p>In conclusion, not only does the larger node utilise resources better, but it can also minimise the fragmentation of resources and increase efficiency.</p><p><em>Does this mean that you should always provision larger instances?</em></p><p><em>Let&#39;s look at another extreme scenario: what happens when a node is lost unexpectedly?</em></p><hr><h2 id="resiliency-and-replication" tabindex="-1"><a class="header-anchor" href="#resiliency-and-replication"><span>Resiliency and replication</span></a></h2><p>A small number of nodes may limit your applications&#39; effective degree of replication.</p><p>For example, if you have a high-availability application consisting of 5 replicas but only two nodes, then the effective degree of replication is reduced to 2. This is because the five replicas can be distributed only across two nodes, and if one of them fails, it may take down multiple replicas at once.</p><figure><img src="https://learnkube.com/a/4db4fb1e1af84fcbcec51b23f9b9e77f.svg" alt="The replication factor for a cluster with two nodes and five replicas is two" tabindex="0" loading="lazy"><figcaption>The replication factor for a cluster with two nodes and five replicas is two</figcaption></figure><p>On the other hand, if you have at least five nodes, each replica can run on a separate node, and a failure of a single node takes down at most one replica.</p><p><strong>Thus, you might require a certain minimum number of nodes in your cluster if you have high-availability requirements.</strong></p><figure><img src="https://learnkube.com/a/4b0ddbe420f754323537c3bde7f938e2.svg" alt="The replication factor for a cluster with five nodes and five replicas is five" tabindex="0" loading="lazy"><figcaption>The replication factor for a cluster with five nodes and five replicas is five</figcaption></figure><p>You should also take into account the size of the node.</p><p>When a larger node is lost, several replicas are eventually rescheduled to other nodes.</p><p>If the node is smaller and hosts only a few workloads, the scheduler reassigns only a handful of pods.</p><p>While you are unlikely to hit any limits in the scheduler, redeploying many replicas might trigger the Cluster Autoscaler.</p><p>And depending on your setup, this could lead to further slowdowns.</p><p><em>Let&#39;s explore why.</em></p><hr><h2 id="scaling-increments-and-lead-time" tabindex="-1"><a class="header-anchor" href="#scaling-increments-and-lead-time"><span>Scaling increments and lead time</span></a></h2>`,81)),n("p",null,[e[15]||(e[15]=s("You can scale applications deployed on Kubernetes ",-1)),t(m,{to:"/learnkube.com/kubernetes-autoscaling-strategies.html"},{default:a(()=>[...e[14]||(e[14]=[n("strong",null,"using a combination of a horizontal scaler (i.e. increasing the number of replicas) and cluster autoscaler (i.e. increasing the nodes count)",-1)])]),_:1}),e[16]||(e[16]=s(".",-1))]),e[85]||(e[85]=n("p",null,[n("em",null,"Assuming you have a cluster at total capacity, how does the node size impact your autoscaling?")],-1)),n("p",null,[e[20]||(e[20]=s("First, you should know that ",-1)),n("a",G,[e[17]||(e[17]=s("the Cluster Autoscaler doesn't look at the memory or CPU available (",-1)),t(l,{icon:"iconfont icon-github"}),e[18]||(e[18]=n("code",null,"kubernetes/autoscaler",-1)),e[19]||(e[19]=s(")",-1))]),e[21]||(e[21]=s(" when it triggers the autoscaling.",-1))]),e[86]||(e[86]=n("p",null,"In other words, a cluster being utilised in total does not trigger the Cluster Autoscaler.",-1)),e[87]||(e[87]=n("p",null,"Instead, the Cluster Autoscaler creates more nodes when a pod is unschedulable due to a lack of resources.",-1)),e[88]||(e[88]=n("p",null,"At that point, the autoscaler calls the cloud provider API to provision more nodes for that cluster.",-1)),t(u,{data:[{id:"1/"},{id:"2/2"}],active:0},{title0:a(({value:o,isActive:i})=>[...e[22]||(e[22]=[s("1/",-1)])]),title1:a(({value:o,isActive:i})=>[...e[23]||(e[23]=[s("2/2",-1)])]),tab0:a(({value:o,isActive:i})=>[...e[24]||(e[24]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/a26ee4d5052d327089d62b6da143f914.svg",alt:"The Cluster Autoscaler provisions new nodes when pods are pending due to lack of resources.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"The Cluster Autoscaler provisions new nodes when pods are pending due to lack of resources.")],-1)])]),tab1:a(({value:o,isActive:i})=>[...e[25]||(e[25]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/6a29222a3225ebb38f4e758210ca85b5.svg",alt:"When the node is provisioned, pods can be deployed.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"When the node is provisioned, pods can be deployed.")],-1)])]),_:1}),e[89]||(e[89]=p('<p><strong>Unfortunately, provisioning nodes is usually slow.</strong></p><p>It might take several minutes to provision a new virtual machine.</p><p><em>Does the provisioning time change for larger or smaller nodes?</em></p><p>No, it&#39;s usually constant regardless of the instance size.</p><p><strong>Also, the cluster autoscaler isn&#39;t limited to adding a single node at a time; it could add several at once</strong>.</p><p><em>Let&#39;s have a look at an example.</em></p><p>There are two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>An application with 0.3 vCPU and 2GB of memory is deployed in the cluster and scaled to 13 replicas.</p><p>Both setups are running at total capacity — they don&#39;t have any extra space for pods left.</p><figure><img src="https://learnkube.com/a/f9fd8d7bfbc21dbf4e2f34d68489d1f7.svg" alt="Two clusters with: one pod per node and all pods into a single node" tabindex="0" loading="lazy"><figcaption>Two clusters with: one pod per node and all pods into a single node</figcaption></figure><p><em>What happens when the deployment scales to 15 replicas (i.e. two more)?</em></p><p>In both clusters, the Cluster Autoscaler detects that the extra pods are un-schedulable due to a lack of resources and provisions:</p><ul><li>An extra node of 4 vCPU and 32GB for the first cluster.</li><li>Two 1 vCPU and 4GB for the second cluster.</li></ul><p>Since there isn&#39;t any time difference between provisioning large or small instances, the nodes will be available simultaneously in both scenarios.</p><figure><img src="https://learnkube.com/a/ab3adead1eea7f1615bff64488318317.svg" alt="Pending pods triggering the autoscaler regardless of their size" tabindex="0" loading="lazy"><figcaption>Pending pods triggering the autoscaler regardless of their size</figcaption></figure><p><em>However, can you spot another difference?</em></p><p>The first cluster has space for 11 more pods since the total capacity is 13. Instead, the second cluster is still maxed out.</p><p>You could argue that smaller increments are more efficient and cheaper because you add only what you need.</p><figure><img src="https://learnkube.com/a/4b5b12047d321e34a6c2455677fd99a0.svg" alt="Autoscaling increments in large and small nodes" tabindex="0" loading="lazy"><figcaption>Autoscaling increments in large and small nodes</figcaption></figure><p>But let&#39;s observe what happens when you scale the deployment again — this time to 17 replicas (i.e. two more).</p><ul><li>The first cluster creates two extra pods in the existing node.</li><li>The second cluster is running at capacity. The pods are Pending, and the Cluster Autoscaler is triggered. Finally, two more worker nodes are provisioned.</li></ul><figure><img src="https://learnkube.com/a/be78be3cd35dc7387e2352c4630c5ae9.svg" alt="Trade-offs for autoscaling increments in Kubernetes nodes" tabindex="0" loading="lazy"><figcaption>Trade-offs for autoscaling increments in Kubernetes nodes</figcaption></figure><p><strong>In the first cluster, the scaling is almost instantaneous.</strong></p><p>In the second, you must wait for the nodes to be provisioned before the pods can serve requests.</p><p>In other words, scaling is quicker in the former case and takes more time in the latter.</p><p><strong>In general, since provisioning time is in the range of minutes, you should think carefully about triggering the Cluster Autoscaler sparingly not to incur longer pod lead time.</strong></p><p>In other words, you can have quicker scaling with larger nodes if you are okay with (potentially) having resources not fully utilised.</p><p><em>But it doesn&#39;t end there.</em></p><p>Pulling container images also affects how quickly you can scale your workloads — and that is related to the number of nodes in the cluster.</p><hr><h2 id="pulling-containers-images" tabindex="-1"><a class="header-anchor" href="#pulling-containers-images"><span>Pulling containers images</span></a></h2><p>When a pod is created in Kubernetes, its definition is stored in etcd.</p><p>It&#39;s the kubelet&#39;s job to detect that the pod is assigned to its node and create it.</p><p>The kubelet will:</p><ul><li>Download the definition from the control plane.</li><li><a href="/learnkube.com/kubernetes-network-packets#how-linux-network-namespaces-work-in-a-pod.md"><strong>Invoke the Container Runtime Interface (CRI) to create the Pod sandbox. The CRI invokes the Container Network Interface (CNI) to attach the Pod to the network</strong></a>.</li><li>Invoke the Container Storage Interface (CSI) to mount any container volume.</li></ul><p>At the end of those steps, the Pod is alive, and the kubelet can move on to checking liveness and readiness probes and update the control plane with the state of the new Pod.</p><figure><img src="https://learnkube.com/a/b321498cc64bfd4bc424abd7a8a8f461.svg" alt="The Kubelet and the CRI, CSI and CNI interfaces" tabindex="0" loading="lazy"><figcaption>The Kubelet and the CRI, CSI and CNI interfaces</figcaption></figure><p><strong>It&#39;s essential to notice that when the CRI creates the container in the pod, it must first download the container image.</strong></p><p>That&#39;s unless the container image is already cached on the current node.</p><p>Let&#39;s have a look at how this affects scaling with two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>Let&#39;s deploy 13 replicas of an app with 0.3 vCPU and 2GB of memory.</p>',43)),n("p",null,[e[27]||(e[27]=s("The app uses a container image ",-1)),n("a",z,[t(l,{icon:"fa-brands fa-docker"}),e[26]||(e[26]=s("based on OpenJDK",-1))]),e[28]||(e[28]=s(" and weighs 1GB (the base image alone is 775MB).",-1))]),e[90]||(e[90]=p('<p>What happens to the two clusters?</p><ul><li>In the first cluster, the Container Runtime downloads the image once and runs 13 replicas.</li><li>In the second cluster, each Container Runtime downloads and runs the image.</li></ul><p>In the first scenario, only 1GB is downloaded.</p><figure><img src="https://learnkube.com/a/352ea906f7800c90ed71deea01eac4f1.svg" alt="The container runtime download the container image once and runs 13 replicas" tabindex="0" loading="lazy"><figcaption>The container runtime download the container image once and runs 13 replicas</figcaption></figure><p>However, you download 13GB of container images in the second scenario.</p><p>Since downloading takes time, the second cluster is slower at creating replicas than the first.</p><p>It also uses more bandwidth and makes more requests (i.e. at least one request for each image layer, 13 times), making it more prone to network glitches.</p><figure><img src="https://learnkube.com/a/d4dfe6e68938734993dd6b5bd2498e7b.svg" alt="Each of the 13 container runtimes download one image" tabindex="0" loading="lazy"><figcaption>Each of the 13 container runtimes download one image</figcaption></figure><p>It&#39;s essential to notice that this issue compounds with the Cluster Autoscaler.</p><p>If you have smaller nodes:</p><ul><li>The Cluster Autoscaler provisions several nodes at once.</li><li>Once ready, each starts to download the container image.</li><li>Finally, the pod is created.</li></ul><p>When you provision larger nodes, the image is likely cached on the node, and the pod can start immediately.</p>',12)),t(u,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:o,isActive:i})=>[...e[29]||(e[29]=[s("1/4",-1)])]),title1:a(({value:o,isActive:i})=>[...e[30]||(e[30]=[s("2/4",-1)])]),title2:a(({value:o,isActive:i})=>[...e[31]||(e[31]=[s("3/4",-1)])]),title3:a(({value:o,isActive:i})=>[...e[32]||(e[32]=[s("4/4",-1)])]),tab0:a(({value:o,isActive:i})=>[...e[33]||(e[33]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/c77ad0aa9e0afa24e57788948ee1b12c.svg",alt:"Imagine having a cluster with 8 nodes, one replica per node.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"Imagine having a cluster with 8 nodes, one replica per node.")],-1)])]),tab1:a(({value:o,isActive:i})=>[...e[34]||(e[34]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/d3b49aee1e2a56dfe0509c3874c308a8.svg",alt:"The cluster is full; scaling to 16 replicas triggers the cluster autoscaler.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"The cluster is full; scaling to 16 replicas triggers the cluster autoscaler.")],-1)])]),tab2:a(({value:o,isActive:i})=>[...e[35]||(e[35]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/6b09aa74b75b631b7be55c492d80c084.svg",alt:"As soon as the nodes are provisioned, the Container Runtime downloads the container image.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"As soon as the nodes are provisioned, the Container Runtime downloads the container image.")],-1)])]),tab3:a(({value:o,isActive:i})=>[...e[36]||(e[36]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/4f053af905c5ceb1f89c9a70dcabebd9.svg",alt:"Finally, the pods are created in the nodes.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"Finally, the pods are created in the nodes.")],-1)])]),_:1}),e[91]||(e[91]=n("p",null,[n("em",null,"So, should you always provision larger nodes?")],-1)),e[92]||(e[92]=n("p",null,"Not necessarily.",-1)),n("p",null,[e[40]||(e[40]=s("You could mitigate nodes downloading the same container image with ",-1)),n("a",K,[e[37]||(e[37]=s("a container registry proxy (",-1)),t(l,{icon:"iconfont icon-github"}),e[38]||(e[38]=n("code",null,"rpardini/docker-registry-proxy",-1)),e[39]||(e[39]=s(")",-1))]),e[41]||(e[41]=s(".",-1))]),e[93]||(e[93]=n("p",null,"In this case, the image is still downloaded but from a local registry in the current network.",-1)),n("p",null,[e[43]||(e[43]=s("Or you could warm up the cache for the nodes with tools such as ",-1)),n("a",U,[t(l,{icon:"iconfont icon-github"}),e[42]||(e[42]=n("code",null,"XenitAB/spegel",-1))]),e[44]||(e[44]=s(".",-1))]),e[94]||(e[94]=p('<p>With Spegel, nodes are peers who can advertise and share container image layers.</p><p>In this other case, container images are downloaded from other worker nodes, and pods can start almost immediately.</p><p>But container bandwidth isn&#39;t the only bandwidth you must keep under control.</p><hr><h2 id="kubelet-and-scaling-the-kubernetes-api" tabindex="-1"><a class="header-anchor" href="#kubelet-and-scaling-the-kubernetes-api"><span>Kubelet and scaling the Kubernetes API</span></a></h2><p>The <code>kubelet</code> is designed to pull information from the control plane.</p><p>So on a regular interval, the <code>kubelet</code> issues a request to the Kubernetes API to check the status of the cluster.</p><p><em>But doesn&#39;t the control plane send instructions to the <code>kubelet</code>?</em></p><p>The pull model is easier to scale because:</p><ul><li>The control plane doesn&#39;t have to push messages to each worker node.</li><li>Nodes can independently query the API server at their own pace.</li><li>The control plane doesn&#39;t have to keep connections with the kubelets open.</li></ul><blockquote><p>Please note that there are notable exceptions. Commands such as <code>kubectl logs</code> and <code>kubectl exec</code> require the control plane to connect to the kubelet (i.e. push model).</p></blockquote><p>But the Kubelet doesn&#39;t just query for info.</p><p>It also reports information back to the master.</p>',13)),n("p",null,[e[48]||(e[48]=s("For example, ",-1)),n("a",S,[t(l,{icon:"iconfont icon-k8s"}),e[45]||(e[45]=s("the ",-1)),e[46]||(e[46]=n("code",null,"kubelet",-1)),e[47]||(e[47]=s(" reports the node's status to the cluster every ten seconds.",-1))])]),e[95]||(e[95]=n("p",null,[s("Also, the "),n("code",null,"kubelet"),s(" informs the control plane when a readiness probe fails (and the pod endpoint should be removed from the service).")],-1)),e[96]||(e[96]=n("p",null,[s("And the "),n("code",null,"kubelet"),s(" keeps the control plane up to date with container metrics.")],-1)),e[97]||(e[97]=n("p",null,[s("In other words, several requests in both directions (i.e. from and to the control plane) are made by the "),n("code",null,"kubelet"),s(" to the control plane to keep the node functioning correctly.")],-1)),n("p",null,[e[52]||(e[52]=s("In Kubernetes 1.26 and earlier, ",-1)),n("a",q,[t(l,{icon:"iconfont icon-k8s"}),e[49]||(e[49]=s("the ",-1)),e[50]||(e[50]=n("code",null,"kubelet",-1)),e[51]||(e[51]=s(" could issue up to 5 requests per second for this (this has been relaxed with Kubernetes >1.27).",-1))])]),e[98]||(e[98]=p('<p><em>So, assuming your kubelet is running at full capacity (i.e. 5rps), what happens when you run several smaller nodes versus a single large node?</em></p><p>Let&#39;s have a look at our two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>The first generates 5 requests per second.</p><figure><img src="https://learnkube.com/a/cb5baa1a4b290bff10e3f693fb5fce97.svg" alt="A single kubelet issueing 5 requests per second" tabindex="0" loading="lazy"><figcaption>A single kubelet issueing 5 requests per second</figcaption></figure><p>The second 65 requests per second (i.e. <code>13 x 5</code>).</p><figure><img src="https://learnkube.com/a/106f96256da416271d46d757e912c983.svg" alt="13 kubelets issueing 5 requests per second each" tabindex="0" loading="lazy"><figcaption>13 kubelets issueing 5 requests per second each</figcaption></figure><p>You should scale your API server to cope with more frequent requests when you run clusters with many smaller nodes.</p><p>And in turn, that usually means running a control plane on a larger instance or running multiple control planes.</p><hr><h2 id="node-and-cluster-limits" tabindex="-1"><a class="header-anchor" href="#node-and-cluster-limits"><span>Node and cluster limits</span></a></h2><p><em>Is there a limit on the number of nodes a Kubernetes cluster can have?</em></p>',12)),n("p",null,[n("a",R,[t(l,{icon:"iconfont icon-k8s"}),e[53]||(e[53]=s("Kubernetes is designed to support up to 5000 nodes.",-1))])]),n("p",null,[e[55]||(e[55]=s("However, this is not a hard constraint, as the team at Google demonstrated by allowing you to ",-1)),n("a",D,[t(l,{icon:"iconfont icon-gcp"}),e[54]||(e[54]=s("run GKE clusters with 15,000 nodes.",-1))])]),e[99]||(e[99]=n("p",null,"For most use cases, 5000 nodes is already a large number and might not be a factor that could steer your decision towards larger or smaller nodes.",-1)),e[100]||(e[100]=n("p",null,"Instead, the max number of pods that you can run in a node could drive you to rethink your cluster architecture.",-1)),e[101]||(e[101]=n("p",null,[n("em",null,"So, how many Pods can you run in a Kubernetes node?")],-1)),e[102]||(e[102]=n("p",null,"Most cloud providers let you run between 110 and 250 pods per node.",-1)),n("p",null,[e[57]||(e[57]=s("If you provision a cluster yourself, ",-1)),n("a",W,[t(l,{icon:"iconfont icon-k8s"}),e[56]||(e[56]=s("the default from is 110.",-1))])]),e[103]||(e[103]=n("p",null,"In most cases, this number is not a limitation of the kubelet but the cloud provider's proneness to the risk of double booking IP addresses.",-1)),e[104]||(e[104]=n("p",null,"To understand what that means, let's take a step back and look at how the cluster network is constructed.",-1)),e[105]||(e[105]=n("p",null,[s("In most cases, each worker node is assigned a subnet with 256 addresses (e.g. "),n("code",null,"10.0.1.0/24"),s(").")],-1)),e[106]||(e[106]=n("figure",null,[n("img",{src:"https://learnkube.com/a/cc3274f66d411c87546f1eb88a42511d.svg",alt:"Each worker node has a subnet assigned",tabindex:"0",loading:"lazy"}),n("figcaption",null,"Each worker node has a subnet assigned")],-1)),n("p",null,[e[59]||(e[59]=s("Of those, ",-1)),t(m,{to:"/freecodecamp.org/subnet-cheat-sheet-24-subnet-mask-30-26-27-29-and-other-ip-address-cidr-network-references.html"},{default:a(()=>[...e[58]||(e[58]=[n("strong",null,"two are restricted",-1)])]),_:1}),e[60]||(e[60]=s(" and you can use 254 for running your Pods.",-1))]),e[107]||(e[107]=p("<p>Consider the scenario where you have 254 pods in the same node.</p><p>You create one more pod but exhausted the available IP addresses, and it stays pending.</p><p>To fix the issue, you decide to decrease the number of replicas to 253. <em>Is the pending pod created in the cluster?</em></p><p>Probably not.</p><p>When you delete the pod, its state changes to &quot;Terminating&quot;.</p><p>The kubelet sends the SIGTERM to the Pod (as well as calling the <code>preStop</code> lifecycle hook, if present) and waits for the containers to shut down gracefully.</p><p>If the containers don&#39;t terminate within 30 seconds, the kubelet sends a SIGKILL signal to the container and forces the process to terminate.</p><p>During this period, the Pod still hasn&#39;t released the IP address, and traffic can still reach it.</p><p>When the pod is finally deleted, the IP address is released.</p>",9)),t(u,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:o,isActive:i})=>[...e[61]||(e[61]=[s("1/4",-1)])]),title1:a(({value:o,isActive:i})=>[...e[62]||(e[62]=[s("2/4",-1)])]),title2:a(({value:o,isActive:i})=>[...e[63]||(e[63]=[s("3/4",-1)])]),title3:a(({value:o,isActive:i})=>[...e[64]||(e[64]=[s("4/4",-1)])]),tab0:a(({value:o,isActive:i})=>[...e[65]||(e[65]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/01b4a22bf8aa92a7998540f6ac1516e0.svg",alt:"When a Pod is deleted, the kubelet is notified of the change.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"When a Pod is deleted, the kubelet is notified of the change.")],-1)])]),tab1:a(({value:o,isActive:i})=>[...e[66]||(e[66]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/ba13194ea5b9ef63597e8fb059355887.svg",alt:"If the Pod has a preStop hook, it is invoked first. Then, the kubelet sends the SIGTERM signal to the container.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"If the Pod has a preStop hook, it is invoked first. Then, the kubelet sends the SIGTERM signal to the container.")],-1)])]),tab2:a(({value:o,isActive:i})=>[...e[67]||(e[67]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/eaba36dda660dd289189979c3b6ce3c4.svg",alt:"By default, the process has 30 seconds to exit, including the preStop hook. If the process isn't exited by then, the kubelet sends the SIGKILL signal and forces killing the process.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"By default, the process has 30 seconds to exit, including the preStop hook. If the process isn't exited by then, the kubelet sends the SIGKILL signal and forces killing the process.")],-1)])]),tab3:a(({value:o,isActive:i})=>[...e[68]||(e[68]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/c05c50cd03dba4465b38b95a016b3712.svg",alt:"The kubelet notifies the control plane that the Pod was deleted successfully. The IP address is finally released.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"The kubelet notifies the control plane that the Pod was deleted successfully. The IP address is finally released.")],-1)])]),_:1}),e[108]||(e[108]=n("p",null,"At this point, the pending pod can be created, and it is assigned the same IP address as the last.",-1)),e[109]||(e[109]=n("p",null,[n("em",null,"Is this a good idea?")],-1)),e[110]||(e[110]=n("p",null,"Well, there isn't any other IP available — so you don't have a choice.",-1)),t(u,{data:[{id:"1/3"},{id:"2/3"},{id:"3/3"}],active:0},{title0:a(({value:o,isActive:i})=>[...e[69]||(e[69]=[s("1/3",-1)])]),title1:a(({value:o,isActive:i})=>[...e[70]||(e[70]=[s("2/3",-1)])]),title2:a(({value:o,isActive:i})=>[...e[71]||(e[71]=[s("3/3",-1)])]),tab0:a(({value:o,isActive:i})=>[...e[72]||(e[72]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/e038d5441bbab5e27bff91fa567517e0.svg",alt:"Imagine your node is using all available IP addresses.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"Imagine your node is using all available IP addresses.")],-1)])]),tab1:a(({value:o,isActive:i})=>[...e[73]||(e[73]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/f0bdd2ddeb962d9e3303965eef0006d3.svg",alt:"When a pod is deleted, the IP address is not released immediately. You have to wait for the graceful shutdown.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"When a pod is deleted, the IP address is not released immediately. You have to wait for the graceful shutdown.")],-1)])]),tab2:a(({value:o,isActive:i})=>[...e[74]||(e[74]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/362ab748b014147011e2a723ae8fa71b.svg",alt:"As soon as the pod is deleted, the IP address can be reused.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"As soon as the pod is deleted, the IP address can be reused.")],-1)])]),_:1}),e[111]||(e[111]=n("p",null,[n("em",null,"What are the consequences?")],-1)),e[112]||(e[112]=n("p",null,"Remember when we mentioned that the pod should gracefully shut down and handle all pending requests?",-1)),e[113]||(e[113]=n("p",null,"Well, if the pod is terminated abruptly (i.e. no graceful shutdown) and the IP address is immediately assigned to a different pod, all existing apps and kubernetes components might still not be aware of the change.",-1)),e[114]||(e[114]=n("p",null,"As a result, some of the existing traffic could be erroneously sent to the new Pod because it has the same IP address as the old one.",-1)),t(u,{data:[{id:"1/2"},{id:"2/2"}],active:0},{title0:a(({value:o,isActive:i})=>[...e[75]||(e[75]=[s("1/2",-1)])]),title1:a(({value:o,isActive:i})=>[...e[76]||(e[76]=[s("2/2",-1)])]),tab0:a(({value:o,isActive:i})=>[...e[77]||(e[77]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/7b2d466a8559f46f477801ae2ca257fd.svg",alt:"The ingress controller routes traffic to an IP address.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"The ingress controller routes traffic to an IP address.")],-1)])]),tab1:a(({value:o,isActive:i})=>[...e[78]||(e[78]=[n("figure",null,[n("img",{src:"https://learnkube.com/a/a785b7150b052e5f5f6906c17c7312b8.svg",alt:"If the IP address is recycled and used by a new Pod without waiting for the graceful shutdown, the ingress controller might still route traffic to that IP address.",tabindex:"0",loading:"lazy"}),n("figcaption",null,"If the IP address is recycled and used by a new Pod without waiting for the graceful shutdown, the ingress controller might still route traffic to that IP address.")],-1)])]),_:1}),e[115]||(e[115]=p(`<p>To avoid this issue, you can have lesser IP addresses assigned (e.g. 110) and use the remaining ones as buffers.</p><p>That way, you can be reasonably sure that the same IP address isn&#39;t immediately reused.</p><hr><h2 id="storage" tabindex="-1"><a class="header-anchor" href="#storage"><span>Storage</span></a></h2><p>Compute units have restrictions on the number of disks that can be attached.</p><p>For example, a Standard_D2_v5 with 2 vCPU and 8GB of memory can have up to 4 data disks attached on Azure.</p><p>If you wish to deploy a StatefulSet to a worker node that uses the Standard_D2_v5 instance type, you won&#39;t be able to create more than four replicas.</p><p>That&#39;s because each replica in a StatefulSet has a disk attached.</p><p>As soon as you create the fifth, the Pod will stay pending because the Persistent Volume Claim can&#39;t be bound to a Persistent Volume.</p><p><em>And why not?</em></p><p>Because each Persistent Volume is an attached disk, you can have only 4 for that instance.</p><p><em>So, what are your options?</em></p><p>You can provision a larger instance.</p><p>Or you could be reusing the same disk with a different <code>subPath</code> field.</p><p>Let&#39;s have a look at an example.</p><p>The following persistent volume requires a disk with 16GB of space:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="pvc.yaml"><span>pvc.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> shared</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> default</span>
<span class="line">  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> ReadWriteOnce</span>
<span class="line">  <span class="token key atrule">resources</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">requests</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 16Gi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>If you submit this resource to the cluster, you&#39;ll observe that a Persistent Volume is created and bound to it.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pv,pvc</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>There is a one-to-one relationship between Persistent Volume and Persistent Volume Claims, so you won&#39;t be able to have more Persistent Volume Claims to use the same disk.</p><p>If you want to use the claim in your pods, you can do so with:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You could have another deployment using the same Persistent Volume Claim:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>However, with this configuration, both pods will write their data in the same folder.</p><p>You could have them working on subdirectories with <code>subPath</code> to work around the issue.</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">              <span class="token key atrule">subPath</span><span class="token punctuation">:</span> app2</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>The deployments will write their data on the following paths:</p><ul><li><code>/data/app1</code> for the first deployment and</li><li><code>/data/app2</code> for the second.</li></ul><p>This workaround is not a perfect solution and has a few limitations:</p><ul><li>All deployments have to remember to use the <code>subPath</code>.</li><li>If you need to write to the volume, you should opt for a Read-Write-Many volume that can be accessed from multiple nodes. Those are usually expensive to provision.</li></ul><p>Also, the same workaround won&#39;t work with a StatefulSet since this will create a brand new Persistent Volume Claim (and persistent Volume) for each replica.</p><hr><h2 id="summary-and-conclusions" tabindex="-1"><a class="header-anchor" href="#summary-and-conclusions"><span>Summary and conclusions</span></a></h2><p><em>So, should you use a few large nodes or many small nodes in your cluster?</em></p><p>It depends.</p><p><em>What&#39;s small or large, anyway?</em></p><p>It comes down to the workloads that you deploy in your cluster.</p><p>For example, if your application requires 10 GB of memory, running an instance with 16GB of memory equals to &quot;running a smaller node&quot;.</p><p>The same instance with an app that requires only 64MB of memory could be considered &quot;large&quot; since you can fit several of them.</p><p><em>And what about a mix of workloads with different resource requirements?</em></p><p>In Kubernetes, there is no rule that all your nodes must have the same size.</p><p>Nothing stops you from using a mix of different node sizes in your cluster.</p><p>This might allow you to trade off the pros and cons of both approaches.</p><p>While you might find the answer through trial and error, we&#39;ve also built a tool to help you with the process.</p>`,45)),n("p",null,[n("a",L,[t(l,{icon:"fas fa-globe"}),e[79]||(e[79]=s("The Kubernetes instance calculator",-1))]),e[80]||(e[80]=s(" lets you explore the best instance type for a given workload.",-1))]),e[116]||(e[116]=n("p",null,"Make sure you give it a try.",-1)),y(" TODO: add ARTICLE CARD "),t(c,h(b({title:"Architecting Kubernetes clusters — choosing a worker node size",desc:"What type of worker nodes should I use for my Kubernetes cluster? And how many of them?. This article looks at the pros and cons.",link:"https://chanhi2000.github.io/bookshelf/learnkube.com/kubernetes-node-size.html",logo:"https://static.learnkube.com/f7e5160d4744cf05c46161170b5c11c9.svg",background:"rgba(102,152,204,0.2)"})),null,16)])}const E=v(x,[["render",N]]),F=JSON.parse('{"path":"/learnkube.com/kubernetes-node-size.html","title":"Architecting Kubernetes clusters — choosing a worker node size","lang":"en-US","frontmatter":{"lang":"en-US","title":"Architecting Kubernetes clusters — choosing a worker node size","description":"Article(s) > Architecting Kubernetes clusters — choosing a worker node size","icon":"iconfont icon-k8s","category":["DevOps","Kubernetes","Article(s)"],"tag":["blog","learnkube.com","devops","kubernetes","k8s"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Architecting Kubernetes clusters — choosing a worker node size\\",\\"image\\":[\\"https://learnkube.com/a/c642b260295b87df85d97a6e8c20be48.svg\\",\\"https://learnkube.com/a/d627f4247a50662c83a2a40703a8b693.svg\\",\\"https://learnkube.com/a/3de0f4647a4b0d71b196d4e394aa5451.svg\\",\\"https://learnkube.com/a/e77f8c687be1e38bd35470e177e4290a.svg\\",\\"https://learnkube.com/a/c72727b60ee6387d73c40004f9003561.svg\\",\\"https://learnkube.com/a/c1be9a8de0824f6e5abcba5371bb9b4e.svg\\",\\"https://learnkube.com/a/f97ca286843a3ff5ee0fbc9a55630829.svg\\",\\"https://learnkube.com/a/23885fd9cefd09159d3bab9618a55aae.svg\\",\\"https://learnkube.com/a/8166b554c895b638a68d2ef76475f943.svg\\",\\"https://learnkube.com/a/2e379971f411f7ca2908ebed0798c020.svg\\",\\"https://learnkube.com/a/4db4fb1e1af84fcbcec51b23f9b9e77f.svg\\",\\"https://learnkube.com/a/4b0ddbe420f754323537c3bde7f938e2.svg\\",\\"https://learnkube.com/a/a26ee4d5052d327089d62b6da143f914.svg\\",\\"https://learnkube.com/a/6a29222a3225ebb38f4e758210ca85b5.svg\\",\\"https://learnkube.com/a/f9fd8d7bfbc21dbf4e2f34d68489d1f7.svg\\",\\"https://learnkube.com/a/ab3adead1eea7f1615bff64488318317.svg\\",\\"https://learnkube.com/a/4b5b12047d321e34a6c2455677fd99a0.svg\\",\\"https://learnkube.com/a/be78be3cd35dc7387e2352c4630c5ae9.svg\\",\\"https://learnkube.com/a/b321498cc64bfd4bc424abd7a8a8f461.svg\\",\\"https://learnkube.com/a/352ea906f7800c90ed71deea01eac4f1.svg\\",\\"https://learnkube.com/a/d4dfe6e68938734993dd6b5bd2498e7b.svg\\",\\"https://learnkube.com/a/c77ad0aa9e0afa24e57788948ee1b12c.svg\\",\\"https://learnkube.com/a/d3b49aee1e2a56dfe0509c3874c308a8.svg\\",\\"https://learnkube.com/a/6b09aa74b75b631b7be55c492d80c084.svg\\",\\"https://learnkube.com/a/4f053af905c5ceb1f89c9a70dcabebd9.svg\\",\\"https://learnkube.com/a/cb5baa1a4b290bff10e3f693fb5fce97.svg\\",\\"https://learnkube.com/a/106f96256da416271d46d757e912c983.svg\\",\\"https://learnkube.com/a/cc3274f66d411c87546f1eb88a42511d.svg\\",\\"https://learnkube.com/a/01b4a22bf8aa92a7998540f6ac1516e0.svg\\",\\"https://learnkube.com/a/ba13194ea5b9ef63597e8fb059355887.svg\\",\\"https://learnkube.com/a/eaba36dda660dd289189979c3b6ce3c4.svg\\",\\"https://learnkube.com/a/c05c50cd03dba4465b38b95a016b3712.svg\\",\\"https://learnkube.com/a/e038d5441bbab5e27bff91fa567517e0.svg\\",\\"https://learnkube.com/a/f0bdd2ddeb962d9e3303965eef0006d3.svg\\",\\"https://learnkube.com/a/362ab748b014147011e2a723ae8fa71b.svg\\",\\"https://learnkube.com/a/7b2d466a8559f46f477801ae2ca257fd.svg\\",\\"https://learnkube.com/a/a785b7150b052e5f5f6906c17c7312b8.svg\\"],\\"datePublished\\":\\"2023-08-15T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Daniele Polencic\\",\\"url\\":\\"https://linkedin.com/in/danielepolencic\\"}]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnkube.com/kubernetes-node-size.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"Architecting Kubernetes clusters — choosing a worker node size"}],["meta",{"property":"og:description","content":"Article(s) > Architecting Kubernetes clusters — choosing a worker node size"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://static.learnkube.com/a102852d1e938e7c95a134501111ed92.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://static.learnkube.com/a102852d1e938e7c95a134501111ed92.png"}],["meta",{"name":"twitter:image:alt","content":"Architecting Kubernetes clusters — choosing a worker node size"}],["meta",{"property":"article:author","content":"Daniele Polencic"}],["meta",{"property":"article:tag","content":"k8s"}],["meta",{"property":"article:tag","content":"kubernetes"}],["meta",{"property":"article:tag","content":"devops"}],["meta",{"property":"article:tag","content":"learnkube.com"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:published_time","content":"2023-08-15T00:00:00.000Z"}],[{"meta":null},{"property":"og:title","content":"Article(s) > Architecting Kubernetes clusters — choosing a worker node size"},{"property":"og:description","content":"Architecting Kubernetes clusters — choosing a worker node size"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnkube.com/kubernetes-node-size.html"}]],"prev":"/devops/k8s/articles/README.md","date":"2023-08-15T00:00:00.000Z","isOriginal":false,"author":[{"name":"Daniele Polencic","url":"https://linkedin.com/in/danielepolencic"}],"cover":"https://static.learnkube.com/a102852d1e938e7c95a134501111ed92.png"},"git":{},"readingTime":{"minutes":17.57,"words":5270},"filePathRelative":"learnkube.com/kubernetes-node-size.md","copyright":{"author":"Daniele Polencic"}}');export{E as comp,F as data};
