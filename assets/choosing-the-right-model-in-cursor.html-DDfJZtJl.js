import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as m,d as t,f as n,b as u,a as f,t as y,n as d,g as h,w as s,e as o,r,o as w}from"./app-BItykJLQ.js";const b={},v={id:"frontmatter-title-·ÑÄ·Ö™·Ü´·ÑÖ·Öß·Ü´",tabindex:"-1"},k={class:"header-anchor",href:"#frontmatter-title-·ÑÄ·Ö™·Ü´·ÑÖ·Öß·Ü´"},C={class:"table-of-contents"},T={href:"https://openai.com/codex/",target:"_blank",rel:"noopener noreferrer"},I={href:"https://anthropic.com/claude-code",target:"_blank",rel:"noopener noreferrer"},x={href:"https://cloud.google.com/gemini/docs/codeassist/gemini-cli",target:"_blank",rel:"noopener noreferrer"},G={href:"https://docs.cursor.com/en/models#auto",target:"_blank",rel:"noopener noreferrer"},P={class:"hint-container note"},A={href:"https://frontendmasters.com/courses/pro-ai/",target:"_blank",rel:"noopener noreferrer"};function M(c,e){const l=r("VPCard"),i=r("router-link"),g=r("SiteInfo"),a=r("VPIcon");return w(),m("div",null,[t("h1",v,[t("a",k,[t("span",null,y(c.$frontmatter.title)+" Í¥ÄÎ†®",1)])]),n(l,d(h({title:"Cursor > Article(s)",desc:"Article(s)",link:"/tool/cursor/articles/README.md",logo:"/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),n(l,d(h({title:"LLM > Article(s)",desc:"Article(s)",link:"/ai/llm/articles/README.md",logo:"/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),t("nav",C,[t("ul",null,[t("li",null,[n(i,{to:"#a-word-on-auto-mode"},{default:s(()=>[...e[0]||(e[0]=[o("A Word on ‚ÄúAuto‚Äù Mode",-1)])]),_:1})]),t("li",null,[n(i,{to:"#claude-models-sonnet-opus-opus-4-1"},{default:s(()=>[...e[1]||(e[1]=[o("Claude Models (Sonnet, Opus, Opus 4.1)",-1)])]),_:1})]),t("li",null,[n(i,{to:"#gpt-models-gpt-3-5-gpt-4-gpt-4o-o3-gpt-5"},{default:s(()=>[...e[2]||(e[2]=[o("GPT Models (GPT-3.5, GPT-4, GPT-4o, o3, GPT-5)",-1)])]),_:1})]),t("li",null,[n(i,{to:"#gemini-models-gemini-2-5-pro"},{default:s(()=>[...e[3]||(e[3]=[o("Gemini Models (Gemini 2.5 Pro)",-1)])]),_:1})]),t("li",null,[n(i,{to:"#deepseek-coder"},{default:s(()=>[...e[4]||(e[4]=[o("DeepSeek Coder",-1)])]),_:1})]),t("li",null,[n(i,{to:"#local-models-llama2-derivatives-etc"},{default:s(()=>[...e[5]||(e[5]=[o("Local Models (LLaMa2 Derivatives, etc.)",-1)])]),_:1})]),t("li",null,[n(i,{to:"#model-selection-strategy"},{default:s(()=>[...e[6]||(e[6]=[o("Model Selection Strategy",-1)])]),_:1})]),t("li",null,[n(i,{to:"#evaluating-new-models"},{default:s(()=>[...e[7]||(e[7]=[o("Evaluating New Models",-1)])]),_:1})]),t("li",null,[n(i,{to:"#wrapping-up"},{default:s(()=>[...e[8]||(e[8]=[o("Wrapping Up",-1)])]),_:1})])])]),e[25]||(e[25]=t("hr",null,null,-1)),n(g,{name:"Choosing the Right Model in Cursor",desc:"Cursor has an ",url:"https://frontendmasters.com/blog/choosing-the-right-model-in-cursor/",logo:"https://frontendmasters.com/favicon.ico",preview:"https://frontendmasters.com/blog/wp-json/social-image-generator/v1/image/7083"}),t("p",null,[e[12]||(e[12]=o("A number of the big players are coming out with their own AI coding assistants (e.g., ",-1)),t("a",T,[n(a,{icon:"iconfont icon-openai"}),e[9]||(e[9]=o("OpenAI‚Äôs Codex",-1))]),e[13]||(e[13]=o(", ",-1)),t("a",I,[n(a,{icon:"iconfont icon-claude"}),e[10]||(e[10]=o("Anthropic‚Äôs Claude Code",-1))]),e[14]||(e[14]=o(", and ",-1)),t("a",x,[n(a,{icon:"iconfont icon-gcp"}),e[11]||(e[11]=o("Google Gemini CLI",-1))]),e[15]||(e[15]=o("). However, one of the advantages of using a third-party tool like Cursor is that you have the option to choose from a wide selection of models. The downside‚Äîof course‚Äîis that, like Uncle Ben would always say, ‚ÄúWith great power comes great responsibility.‚Äù",-1))]),e[26]||(e[26]=t("p",null,"Cursor doesn‚Äôt just give you a single AI model and call it a day‚Äîit hands you a buffet. You‚Äôve got heavy hitters like OpenAI‚Äôs GPT series (now including the newly-released GPT-5), Anthropic‚Äôs Claude models (including the shiny new Opus 4.1), Google‚Äôs Gemini, along with Cursor‚Äôs own hosted options and even local models you can run on your machine.",-1)),e[27]||(e[27]=t("p",null,[o("Different models excel in different areas, and selecting wisely has a significant impact on quality, latency, and cost. Think of it like picking the right guitar for the gig‚Äîyou "),t("em",null,"could"),o(" play metal riffs on a nylon-string classical, but wouldn‚Äôt you rather have the right tool for the job?")],-1)),e[28]||(e[28]=t("hr",null,null,-1)),e[29]||(e[29]=t("h2",{id:"a-word-on-auto-mode",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#a-word-on-auto-mode"},[t("span",null,"A Word on ‚ÄúAuto‚Äù Mode")])],-1)),t("p",null,[e[17]||(e[17]=o("Cursor also offers Auto mode, which will pick a model for you based on the complexity of your query and current server reliability. It‚Äôs like autopilot‚Äîbut if you care about cost or predictability, it‚Äôs worth picking models manually. ",-1)),t("a",G,[n(a,{icon:"iconfont icon-cursor"}),e[16]||(e[16]=o("Cursor‚Äôs documentation",-1))]),e[18]||(e[18]=o(" describes it as selecting ‚Äúthe premium model best fit for the immediate task‚Äù and ‚Äúautomatically switching models‚Äù when output quality or availability dips. In practice, it‚Äôs a reliability‚Äëfirst, hands‚Äëoff default so you can keep coding without thinking about providers.",-1))]),e[30]||(e[30]=u('<p>Use Auto when you want to stay in flow and avoid babysitting model choice. It‚Äôs especially handy for day‚Äëto‚Äëday edits, smaller refactors, explanation/QA over the codebase, and any situation where provider hiccups would otherwise force you to switch models manually. Because Auto can detect degraded performance and hop to a healthier model, it reduces stalls during outages or rate‚Äëlimit blips.</p><p>Auto is also a good ‚Äúfirst try‚Äù when you‚Äôre unsure which model style fits‚ÄîCursor‚Äôs guidance explicitly calls it a safe default. If you later notice the conversation needs a different behavior (more initiative vs. tighter instruction‚Äëfollowing), you can switch and continue. But, with that said, let‚Äôs dive into the differences between the models themselves for those situations where you want to take control of the wheel.</p><div class="hint-container note"><p class="hint-container-title">Nota bene</p><p>A lot of evaluating how ‚Äúgood‚Äù a model is for a given task is a subjective art. So, for this post, we‚Äôre going to be juggling a careful balance between my own experience and a requisite amount of reading other people‚Äôs hot takes on Reddit so that you don‚Äôt have to subject yourself to that.</p></div><hr><h2 id="claude-models-sonnet-opus-opus-4-1" tabindex="-1"><a class="header-anchor" href="#claude-models-sonnet-opus-opus-4-1"><span>Claude Models (Sonnet, Opus, Opus 4.1)</span></a></h2><p>Claude has become a fan favorite in Cursor, especially for frontend work, UI/UX refactoring, and code simplification. I will say, I like to think that I am pretty good at this whole front-end engineering schtick, but even sometimes, I am impressed.</p><ul><li><strong>Claude 3.5 Sonnet</strong>: Often the ‚Äúdefault choice‚Äù for coding tasks. It‚Äôs fast, reliable, and has a knack for simplifying messy code without losing nuance.</li><li><strong>Claude 4 Opus</strong>: Anthropic‚Äôs flagship for deep reasoning. Excellent for architectural planning and critical refactors, though slower and pricier.</li><li><strong>Claude 4.1 Opus</strong>: The newest version, with sharper reasoning and longer context windows. This is the model you pull out when you‚Äôre dealing with a sprawling repo or thorny system design and you want answers that feel almost like a senior architect wrote them.</li></ul><div class="hint-container info"><p class="hint-container-title">Trade-off</p><p>Claude models are sometimes cautious‚Äîthey‚Äôll decline tasks that a GPT model might at least attempt. But the output is usually more focused and aligned with best practices. I‚Äôve also noticed that Claude has a tendency to get side-tracked and work on other tangentially-related tasks that I didn‚Äôt explicitly ask for. That said, I‚Äôm guilty of this too.</p></div><hr><h2 id="gpt-models-gpt-3-5-gpt-4-gpt-4o-o3-gpt-5" tabindex="-1"><a class="header-anchor" href="#gpt-models-gpt-3-5-gpt-4-gpt-4o-o3-gpt-5"><span>GPT Models (GPT-3.5, GPT-4, GPT-4o, o3, GPT-5)</span></a></h2><p>OpenAI‚Äôs GPT line has been the workhorse of AI coding.</p><ul><li><strong>GPT-3.5</strong>: Blazing fast and cheap, perfect for boilerplate generation and small tasks.</li><li><strong>GPT-4 / GPT-4o</strong>: Solid all-rounders. Great for logic-heavy work, nuanced refactors, and design patterns. GPT-4o is especially nice as a ‚Äúdaily driver‚Äù because it balances cost, speed, and capability.</li><li><strong>o3</strong>: A variant tuned for better reasoning and structured answers. Handy for debugging or step-by-step problem solving.</li><li><strong>GPT-5</strong>: The new heavyweight. Think GPT-4 but with significantly deeper reasoning, longer context, and a much better grasp of codebases at scale. It‚Äôs particularly strong at handling multi-file architectural changes and design discussions. If GPT-4 was like working with a diligent senior dev, GPT-5 feels closer to having a staff engineer who can keep the whole system in their head.</li></ul><div class="hint-container info"><p class="hint-container-title">Trade-off</p><p>GPT models sometimes get ‚Äúlazy‚Äù‚Äîthey‚Äôll sketch a partial solution instead of finishing the job. But when you want factual grounding or logic-intensive brainstorming, they‚Äôre hard to beat. GPT-5 in particular tends to go slower and check in more often. So, it‚Äôs a bit more of a hands-on experience than the Claude models. That said, given Claude‚Äôs tendency to go on side quests, I am not sure this is a bad thing. GPT-5 will often do the bare minimum but then come to you with suggestions for what it ought to do next and I find myself either agreeing or choosing a subset of its suggestions.</p></div><hr><h2 id="gemini-models-gemini-2-5-pro" tabindex="-1"><a class="header-anchor" href="#gemini-models-gemini-2-5-pro"><span>Gemini Models (Gemini 2.5 Pro)</span></a></h2><p>Google‚Äôs Gemini slots in nicely for certain tasks: complex design, deep bug-hunting, and rapid completions. It‚Äôs more of a specialist tool‚Äîless universal than Claude or GPT, but very effective when you hit the right workload. Historically, one of the perks of Gemini is that it had a massive context window (around 2 million tokens). In the months since it was released, however, other models have caught up‚Äînamely Opus and GPT-5. Even Sonnet 4 now rocks a 1 million token context window.</p><p>I typically find myself using Gemini for research tasks. ‚ÄúHey Gemini, look over my code base and come up with some suggestions for how I can make my tests less flaky and go write them to this file.‚Äù Its large context window makes it great for these kinds of tasks. It‚Äôs no slouch in your day-to-day coding tasks either. I just typically find myself reaching for something lighter‚Äîand cheaper.</p><hr><h2 id="deepseek-coder" tabindex="-1"><a class="header-anchor" href="#deepseek-coder"><span>DeepSeek Coder</span></a></h2><p>Cursor also offers DeepSeek Coder, a leaner, cost-effective option hosted directly by Cursor. It‚Äôs good for troubleshooting and analysis, and useful if you want more privacy and predictable costs. That said, it doesn‚Äôt quite match the top-tier frontier models for heavy generative work.</p><hr><h2 id="local-models-llama2-derivatives-etc" tabindex="-1"><a class="header-anchor" href="#local-models-llama2-derivatives-etc"><span>Local Models (LLaMa2 Derivatives, etc.)</span></a></h2><p>Sometimes you just need to keep everything on your own machine. Cursor supports local models, which are slower and less powerful but guarantee maximum privacy. These shine if you‚Äôre working with highly sensitive code or under strict compliance requirements. This is not my area of expertise. Mainly because my four-year-old MacBook can‚Äôt run these models at the same speed as one of OpenAI‚Äôs datacenters can.</p><hr><h2 id="model-selection-strategy" tabindex="-1"><a class="header-anchor" href="#model-selection-strategy"><span>Model Selection Strategy</span></a></h2><p>Here are some general heuristics I‚Äôve found useful:</p><ul><li><strong>For small stuff</strong> (boilerplate, stubs, quick utilities): GPT-4o or a local model keeps things fast and cheap.</li><li><strong>For day-to-day coding</strong>: Claude Sonnet 4 and GPT-4.1 are solid defaults. They balance reliability with performance. Gemini 2.5 Flash is also a strong contender in this department.</li><li><strong>For heavy lifting</strong> (large refactors, architecture, critical business logic): GPT-5 or Claude Opus 4.1 are the power tools. They‚Äôre not cheap, but often it costs less to get it right the first time. What I‚Äôll typically do is have them write their plan to a Markdown file, review it, and then let a lighter weight model take over from there.</li><li><strong>When stuck</strong>: Swap models. If Claude hesitates, try GPT. If GPT spins in circles, Claude often cuts to the chase. This is not a super scientific approach, but it‚Äôs wildly effective‚Äîor at least it <em>feels</em> that way.</li><li><strong>Privacy first</strong>: Use local models or Cursor-hosted DeepSeek when your code should never leave your machine. I‚Äôve traditionally worked on open-source stuff. So, this hasn‚Äôt been a huge concern of mine, personally.</li></ul>',27)),t("div",P,[e[24]||(e[24]=t("p",{class:"hint-container-title"},"Editor‚Äôs note",-1)),t("p",null,[e[20]||(e[20]=o("If you ",-1)),e[21]||(e[21]=t("em",null,"really",-1)),e[22]||(e[22]=o(" want to level up with your AI coding skills, you should go from here right to Steve‚Äôs course: ",-1)),t("a",A,[n(a,{icon:"fas fa-globe"}),e[19]||(e[19]=o("Cursor & Claude Code: Professional AI Setup",-1))]),e[23]||(e[23]=o(".",-1))])]),e[31]||(e[31]=u('<hr><h2 id="evaluating-new-models" tabindex="-1"><a class="header-anchor" href="#evaluating-new-models"><span>Evaluating New Models</span></a></h2><p>New models drop all of the time, which raises the question: How should you think about evaluating a new model release to see if it‚Äôs a good fit for your workflow?</p><p><strong>Capability</strong>‚ÄîCan it actually ship fixes in your codebase, not just talk about them? Reasoning‚Äëforward models like OpenAI‚Äôs o3 and hybrid ‚Äúthinking‚Äù models like Claude 3.7 Sonnet are pitched for deeper analysis; use them when you expect layered reasoning or ambiguous requirements.</p><p><strong>Behavior</strong>‚ÄîDoes it take initiative or wait for explicit instructions? Cursor‚Äôs model guide groups ‚Äúthinking models‚Äù (e.g., o3, Gemini 2.5 Pro) versus ‚Äúnon‚Äëthinking models‚Äù (e.g., Claude‚Äë4‚ÄëSonnet, GPT‚Äë4.1) and spells out when each style helps. Assertive models are great for exploration and refactors; obedient models shine on surgical edits.</p><p><strong>Context</strong>‚ÄîDo you need a lot of context right now? If you‚Äôre touching broad cross‚Äëcutting concerns, enable Max Mode on models that support 1M‚Äëtoken windows and observe whether plan quality improves enough to justify the slower, pricier runs. Having a bigger context window isn‚Äôt always a good thing. Regardless of what the model‚Äôs maximum context window size is, the more you load into that window, the longer it‚Äôs going to take to process all of those tokens. Generally speaking, having the <em>right</em> context is way better than having <em>more</em> context.</p><p><strong>Cost and reliability</strong>‚ÄîCursor bills at provider API rates; Auto exists to keep you moving when a provider hiccups. New models often carry different throughput/price curves‚Äîcompare under your real workload, not just benchmarks. Cost is a tricky thing to evaluate because if a model costs more per token, but can accomplish the task in few tokens, it might end up being a bit cheaper when all is said and done.</p><p>Here is my pseudo-scientific guide for kicking the tires on a new model.</p><ol><li>Freeze variables. Use the same branch, same repo state, and the same prompt for each run. Turn Auto off when you‚Äôre pinning a candidate so you‚Äôre not measuring routing noise. Cursor‚Äôs guide confirms Auto isn‚Äôt task‚Äëaware and excludes o3‚Äîso when you test o3 or any very new model, pin it.</li><li>Pick three task archetypes. Choose one surgical edit, one bug‚Äëhunt, and one broader refactor. That trio exposes obedience, reasoning, and context behavior in a single pass. Cursor‚Äôs ‚Äúmodes‚Äù page clarifies that Agent can run commands and do multi‚Äëfile edits‚Äîideal for these trials.</li><li>As Peter Drucker (or John Doerr, but I digress) used to say: Measure what matters. For each task and model, record: did tests pass; how much did it modify; did it follow constraints; how many agent tool calls and shell runs; and wall‚Äëclock duration. Cursor‚Äôs headless CLI can stream structured events that include the chosen model and per‚Äërequest timing‚Äîperfect for quick logging.</li></ol><p>Repeat this process with Max Mode if the model you‚Äôre evaluating advertises giant context. You‚Äôre testing whether the larger window yields better plans or just slower ones.</p><hr><h2 id="wrapping-up" tabindex="-1"><a class="header-anchor" href="#wrapping-up"><span>Wrapping Up</span></a></h2><p>Model choice in Cursor isn‚Äôt just about ‚Äúwhich AI is best‚Äù‚Äîit‚Äôs about matching the right tool to the task. Claude excels at simplifying and clarifying, GPT shines at reasoning and factual grounding, Gemini offers design chops, and local models guard your privacy.</p><p>And with GPT-5 and Opus 4.1 now in the mix, we‚Äôre entering a phase where models can reason about your codebase almost like a human teammate. The trick is knowing when to bring in the heavy artillery and when a lighter model will do the job faster and cheaper.</p>',14)),f(" TODO: add ARTICLE CARD "),n(l,d(h({title:"Choosing the Right Model in Cursor",desc:"Cursor has an ",link:"https://chanhi2000.github.io/bookshelf/frontendmasters.com/choosing-the-right-model-in-cursor.html",logo:"https://frontendmasters.com/favicon.ico",background:"rgba(188,75,52,0.2)"})),null,16)])}const j=p(b,[["render",M]]),O=JSON.parse('{"path":"/frontendmasters.com/choosing-the-right-model-in-cursor.html","title":"Choosing the Right Model in Cursor","lang":"en-US","frontmatter":{"lang":"en-US","title":"Choosing the Right Model in Cursor","description":"Article(s) > Choosing the Right Model in Cursor","icon":"iconfont icon-cursor","category":["Tool","Cursor","IDE","Productivity","AI","LLM","Article(s)"],"tag":["blog","frontendmasters.com","tool","cursor","ide","productivity","ai","artificial-intelligence","llm","large-language-models"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Choosing the Right Model in Cursor\\",\\"image\\":[\\"https://frontendmasters.com/blog/wp-json/social-image-generator/v1/image/7083\\"],\\"datePublished\\":\\"2025-09-10T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Steve Kinney\\",\\"url\\":\\"https://frontendmasters.com/blog/author/stevekinney/\\"}]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/frontendmasters.com/choosing-the-right-model-in-cursor.html"}],["meta",{"property":"og:site_name","content":"üìöBookshelf"}],["meta",{"property":"og:title","content":"Choosing the Right Model in Cursor"}],["meta",{"property":"og:description","content":"Article(s) > Choosing the Right Model in Cursor"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://frontendmasters.com/blog/wp-json/social-image-generator/v1/image/7083"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://frontendmasters.com/blog/wp-json/social-image-generator/v1/image/7083"}],["meta",{"name":"twitter:image:alt","content":"Choosing the Right Model in Cursor"}],["meta",{"property":"article:author","content":"Steve Kinney"}],["meta",{"property":"article:tag","content":"large-language-models"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"artificial-intelligence"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:tag","content":"productivity"}],["meta",{"property":"article:tag","content":"ide"}],["meta",{"property":"article:tag","content":"cursor"}],["meta",{"property":"article:tag","content":"tool"}],["meta",{"property":"article:tag","content":"frontendmasters.com"}],["meta",{"property":"article:tag","content":"blog"}],["meta",{"property":"article:published_time","content":"2025-09-10T00:00:00.000Z"}],[{"meta":null},{"property":"og:title","content":"Article(s) > Choosing the Right Model in Cursor"},{"property":"og:description","content":"Choosing the Right Model in Cursor"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/frontendmasters.com/choosing-the-right-model-in-cursor.html"}]],"prev":"/tool/cursor/articles/README.md","date":"2025-09-10T00:00:00.000Z","isOriginal":false,"author":[{"name":"Steve Kinney","url":"https://frontendmasters.com/blog/author/stevekinney/"}],"cover":"https://frontendmasters.com/blog/wp-json/social-image-generator/v1/image/7083"},"git":{},"readingTime":{"minutes":7.79,"words":2338},"filePathRelative":"frontendmasters.com/choosing-the-right-model-in-cursor.md","copyright":{"author":"Steve Kinney"}}');export{j as comp,O as data};
