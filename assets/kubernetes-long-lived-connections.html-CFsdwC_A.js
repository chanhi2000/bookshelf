import{_ as g}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as k,d as s,f as l,b as o,a as f,t as y,n as h,g as m,w as a,e as n,r as d,o as w}from"./app-BVguHYKu.js";const P={},x={id:"frontmatter-title-관련",tabindex:"-1"},I={class:"header-anchor",href:"#frontmatter-title-관련"},E={class:"table-of-contents"},T={href:"https://kubernetes.io/docs/reference/networking/virtual-ips/",target:"_blank",rel:"noopener noreferrer"},S={href:"https://iximiuz.com/en/posts/laymans-iptables-101/",target:"_blank",rel:"noopener noreferrer"},A={href:"https://commons.wikimedia.org/wiki/File:Iptables_diagram.png",target:"_blank",rel:"noopener noreferrer"},N={href:"https://lwn.net/Articles/267140/",target:"_blank",rel:"noopener noreferrer"},K={href:"https://stackrox.io/blog/kubernetes-networking-demystified/#iptables",target:"_blank",rel:"noopener noreferrer"},U={href:"https://stackoverflow.com/questions/68180239/iptables-rules-for-kube-dns",target:"_blank",rel:"noopener noreferrer"},R={href:"https://reddit.com/r/eero/comments/6we6er/comment/dm7ek4l/",target:"_blank",rel:"noopener noreferrer"},B={href:"https://docs.tigera.io/calico/latest/network-policy/services/services-cluster-ips",target:"_blank",rel:"noopener noreferrer"},L={href:"https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-type-clusterip",target:"_blank",rel:"noopener noreferrer"},C={href:"https://kubernetes.io/blog/2022/12/30/advancements-in-kubernetes-traffic-engineering/",target:"_blank",rel:"noopener noreferrer"},q={href:"https://linkedin.com/in/michael-w-oleary/",target:"_blank",rel:"noopener noreferrer"};function O(b,e){const u=d("VPCard"),r=d("router-link"),v=d("SiteInfo"),p=d("VPIcon"),c=d("Tabs");return w(),k("div",null,[s("h1",x,[s("a",I,[s("span",null,y(b.$frontmatter.title)+" 관련",1)])]),l(u,h(m({title:"Kubernetes > Article(s)",desc:"Article(s)",link:"/devops/k8s/articles/README.md",logo:"https://chanhi2000.github.io/images/ico-wind.svg",background:"rgba(10,10,10,0.2)"})),null,16),s("nav",E,[s("ul",null,[s("li",null,[l(r,{to:"#deploying-a-two-tier-application"},{default:a(()=>e[0]||(e[0]=[n("Deploying a two-tier application")])),_:1,__:[0]})]),s("li",null,[l(r,{to:"#deploying-the-backend-pods"},{default:a(()=>e[1]||(e[1]=[n("Deploying the Backend Pods")])),_:1,__:[1]})]),s("li",null,[l(r,{to:"#inspecting-the-backend-deployment"},{default:a(()=>e[2]||(e[2]=[n("Inspecting the backend deployment")])),_:1,__:[2]})]),s("li",null,[l(r,{to:"#exposing-the-backend-pods-within-the-cluster-with-a-service"},{default:a(()=>e[3]||(e[3]=[n("Exposing the backend pods within the cluster with a Service")])),_:1,__:[3]})]),s("li",null,[l(r,{to:"#dns-resolution-for-the-backend-service"},{default:a(()=>e[4]||(e[4]=[n("DNS Resolution for the backend service")])),_:1,__:[4]})]),s("li",null,[l(r,{to:"#endpoints-and-services"},{default:a(()=>e[5]||(e[5]=[n("Endpoints and Services")])),_:1,__:[5]})]),s("li",null,[l(r,{to:"#kube-proxy-translating-service-ip-to-pod-ip"},{default:a(()=>e[6]||(e[6]=[n("kube-proxy: translating Service IP to Pod IP")])),_:1,__:[6]})]),s("li",null,[l(r,{to:"#kube-proxy-and-iptables-rules"},{default:a(()=>e[7]||(e[7]=[n("kube-proxy and iptables rules")])),_:1,__:[7]})]),s("li",null,[l(r,{to:"#following-traffic-from-a-pod-to-service"},{default:a(()=>e[8]||(e[8]=[n("Following traffic from a Pod to Service")])),_:1,__:[8]})]),s("li",null,[l(r,{to:"#deploying-and-exposing-the-frontend-pods"},{default:a(()=>e[9]||(e[9]=[n("Deploying and exposing the frontend Pods")])),_:1,__:[9]})]),s("li",null,[l(r,{to:"#exposing-the-frontend-pods"},{default:a(()=>e[10]||(e[10]=[n("Exposing the frontend pods")])),_:1,__:[10]})]),s("li",null,[l(r,{to:"#load-balancer-service"},{default:a(()=>e[11]||(e[11]=[n("Load Balancer Service")])),_:1,__:[11]})]),s("li",null,[l(r,{to:"#extra-hop-with-kube-proxy-and-intra-cluster-load-balancing"},{default:a(()=>e[12]||(e[12]=[n("Extra hop with kube-proxy and intra-cluster load balancing")])),_:1,__:[12]})]),s("li",null,[l(r,{to:"#externaltrafficpolicy-local-preserving-the-source-ip-in-kubernetes"},{default:a(()=>e[13]||(e[13]=[n("ExternalTrafficPolicy: Local, preserving the source IP in Kubernetes")])),_:1,__:[13]})]),s("li",null,[l(r,{to:"#proxyterminatingendpoints-in-kubernetes"},{default:a(()=>e[14]||(e[14]=[n("ProxyTerminatingEndpoints in Kubernetes")])),_:1,__:[14]})]),s("li",null,[l(r,{to:"#how-can-the-pod-s-ip-address-be-routable-from-the-load-balancer"},{default:a(()=>e[15]||(e[15]=[n("How can the Pod's IP address be routable from the load balancer?")])),_:1,__:[15]})]),s("li",null,[l(r,{to:"#summary"},{default:a(()=>e[16]||(e[16]=[n("Summary")])),_:1,__:[16]})])])]),e[131]||(e[131]=s("hr",null,null,-1)),l(v,{name:"Kubernetes networking: service, kube-proxy, load balancing",desc:"Master Kubernetes networking with Services and load balancing. Learn how traffic flows within clusters and from external sources.",url:"https://learnk8s.com/learnk8s.iokubernetes-services-and-load-balancing",logo:"https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg",preview:"https://static.learnk8s.io/1fd1550ac4cb6e44a8f57d11d45ad42f.png"}),e[132]||(e[132]=o('<div class="hint-container important"><p class="hint-container-title">TL;DR</p><p>This article explores Kubernetes networking, focusing on Services, kube-proxy, and load balancing.</p></div><p>It covers how pods communicate within a cluster, how Services direct traffic, and how external access is managed.</p><p>You will explore ClusterIP, NodePort, and LoadBalancer service types and dive into their implementations using iptables rules.</p><p>You will also discuss advanced topics like preserving source IPs, handling terminating endpoints, and integrating with cloud load balancers.</p><hr><h2 id="deploying-a-two-tier-application" tabindex="-1"><a class="header-anchor" href="#deploying-a-two-tier-application"><span>Deploying a two-tier application</span></a></h2><p>Consider a two-tier application consisting of two tiers: the frontend tier, which is a web server that serves HTTP responses to browser requests, and the backend tier, which is a stateful API containing a list of job titles.</p><figure><img src="https://learnk8s.io/a/a72a5e5ce63b136493614bfc156afdb5.svg" alt="A front-end and backend application in Kubernetes" tabindex="0" loading="lazy"><figcaption>A front-end and backend application in Kubernetes</figcaption></figure><p>The front end calls the backend to display a job title and logs which pod processed the request.</p><p><em>Let&#39;s deploy and expose those applications in Kubernetes.</em></p><hr><h2 id="deploying-the-backend-pods" tabindex="-1"><a class="header-anchor" href="#deploying-the-backend-pods"><span>Deploying the Backend Pods</span></a></h2>',12)),s("p",null,[e[17]||(e[17]=n("This is what ")),l(p,{icon:"iconfont icon-yaml"}),e[18]||(e[18]=s("code",null,"backend-deployment.yaml",-1)),e[19]||(e[19]=n(" looks like."))]),e[133]||(e[133]=o(`<p>Notice that we will include <code>replicas: 1</code> to indicate that I want to deploy only one pod.</p><p>backend-deployment.yaml</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="backend-deployment.yaml"><span>backend-deployment.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> backend<span class="token punctuation">-</span>deployment</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">app</span><span class="token punctuation">:</span> backend</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">app</span><span class="token punctuation">:</span> backend</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> backend</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> ghcr.io/learnk8s/jobs<span class="token punctuation">-</span>api</span>
<span class="line">        <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">          <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">3000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You can submit the file to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> backend-deployment.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># deployment.apps/backend-deployment created</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><em>Great!</em></p><p>Now, you have a deployment of a single pod running the backend API.</p><p>Verify this:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get deployment</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                 READY   UP-TO-DATE   AVAILABLE</span></span>
<span class="line"><span class="token comment"># backend-deployment   1/1     1            1</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The command above provides deployment information, but it&#39;d be great to get information about the individual pod, like the IP address or node it was assigned to.</p><hr><h2 id="inspecting-the-backend-deployment" tabindex="-1"><a class="header-anchor" href="#inspecting-the-backend-deployment"><span>Inspecting the backend deployment</span></a></h2><p>You can retrieve the pod&#39;s IP address by appending <code>-l app=backend</code> to get only pods matching our deployment and <code>-o wide</code> so that the output includes the pod IP address.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>backend <span class="token parameter variable">-o</span> wide</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                                  READY   STATUS    IP           NODE</span></span>
<span class="line"><span class="token comment"># backend-deployment-6c84d55bc6-v7tcq   1/1     Running   10.244.1.2   minikube-m02</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><em>Great!</em></p><p>Now you know that the pod IP address is <code>10.244.1.2</code>.</p><p><em>But how will the frontend pods reach this IP address when they need to call the backend API?</em></p><hr><h2 id="exposing-the-backend-pods-within-the-cluster-with-a-service" tabindex="-1"><a class="header-anchor" href="#exposing-the-backend-pods-within-the-cluster-with-a-service"><span>Exposing the backend pods within the cluster with a Service</span></a></h2><p><strong>A Service in Kubernetes allows pods to be easily discoverable and reachable across the pod network.</strong></p><figure><img src="https://learnk8s.io/a/449d83d8882e5ded09a64ffa1c49bf88.svg" alt="Exposing the backend application with a ClusterIP Service" tabindex="0" loading="lazy"><figcaption>Exposing the backend application with a ClusterIP Service</figcaption></figure><p>To enable the frontend pods to discover and reach the backend, let&#39;s expose the backend pod through a Service.</p><p>This is what the service looks like:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="backend-service.yaml"><span>backend-service.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> backend<span class="token punctuation">-</span>service</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> backend</span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> backend</span>
<span class="line">      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP</span>
<span class="line">      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">3000</span></span>
<span class="line">      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">3000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You can create the resource with the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> backend-service.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># service/backend-service created</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Verify the creation of this service:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get <span class="token function">service</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)</span></span>
<span class="line"><span class="token comment"># backend-service   ClusterIP   10.96.5.81   &lt;none&gt;        3000/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The service&#39;s IP address is <code>10.96.5.81</code>, exposing a single port: <code>3000</code>.</p><p><em>But how do the frontend pods know they should reach that IP address?</em></p><p><em>And what if the IP address changes?</em></p><hr><h2 id="dns-resolution-for-the-backend-service" tabindex="-1"><a class="header-anchor" href="#dns-resolution-for-the-backend-service"><span>DNS Resolution for the backend service</span></a></h2><p><strong>Instead of reaching the Service by its IP address, you can assign a friendly name and rely on the DNS to translate it to an IP address.</strong></p><p>And that&#39;s precisely what happens when you create a Service in Kubernetes: a DNS record is created with the Fully Qualified Domain Name (FQDN) of <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>.</p><p>You can access services and pods using DNS names instead of IP addresses.</p><p>CoreDNS is the component that resolves these DNS names to their corresponding IP addresses.</p><p>It is deployed as a <code>ClusterIP</code> service named <code>kube-dns</code> and managed by a <code>Deployment</code> in the <code>kube-system</code> namespace.</p><p>When a pod needs to resolve a service name, it sends a DNS query to the <code>kube-dns</code> service.</p><p><strong>CoreDNS processes the request and resolves the service name to the appropriate <code>ClusterIP</code>.</strong></p>`,40)),l(c,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:t,isActive:i})=>e[20]||(e[20]=[n("1/4")])),title1:a(({value:t,isActive:i})=>e[21]||(e[21]=[n("2/4")])),title2:a(({value:t,isActive:i})=>e[22]||(e[22]=[n("3/4")])),title3:a(({value:t,isActive:i})=>e[23]||(e[23]=[n("4/4")])),tab0:a(({value:t,isActive:i})=>e[24]||(e[24]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/e4f22a3358e5cf13ba9c26bb5ca5c563.svg",alt:"The front-end pod doesn't know the IP address of the Service, but all services can be called using their Fully Qualified Domain Name (FQDN).",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The front-end pod doesn't know the IP address of the Service, but all services can be called using their Fully Qualified Domain Name (FQDN).")],-1)])),tab1:a(({value:t,isActive:i})=>e[25]||(e[25]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/90c0d1ef18f196de9b7936300b885060.svg",alt:"The application will query CoreDNS and swap the FQDN for an IP address.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The application will query CoreDNS and swap the FQDN for an IP address.")],-1)])),tab2:a(({value:t,isActive:i})=>e[26]||(e[26]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/7cc292393948f1019ec0cc3e197952e7.svg",alt:"Depending on the type of Service, CoreDNS will return the appropriate IP address.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Depending on the type of Service, CoreDNS will return the appropriate IP address.")],-1)])),tab3:a(({value:t,isActive:i})=>e[27]||(e[27]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/0c9be1e6d56a3df58dd4d8f12d229e33.svg",alt:"Finally, the application can use that IP address to connect to the Service.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Finally, the application can use that IP address to connect to the Service.")],-1)])),_:1}),e[134]||(e[134]=o(`<p>You can inspect the <code>kube-dns</code> service with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get svc <span class="token parameter variable">-n</span> kube-system kube-dns</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME       TYPE        CLUSTER-IP   PORT(S)</span></span>
<span class="line"><span class="token comment"># kube-dns   ClusterIP   10.96.0.10   53/UDP,53/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2)),s("p",null,[e[28]||(e[28]=n("Kubelet configures each pod's ")),l(p,{icon:"fas fa-folder-open"}),e[29]||(e[29]=s("code",null,"/etc/",-1)),l(p,{icon:"fas fa-file-lines"}),e[30]||(e[30]=s("code",null,"resolv.conf",-1)),e[31]||(e[31]=n(" file."))]),e[135]||(e[135]=s("p",null,"This file specifies how DNS queries are resolved, including the nameservers to use and the search domains to help expand queries.",-1)),s("p",null,[e[32]||(e[32]=n("Check the contents of a pod's ")),l(p,{icon:"fas fa-folder-open"}),e[33]||(e[33]=s("code",null,"/etc/",-1)),l(p,{icon:"fas fa-file-lines"}),e[34]||(e[34]=s("code",null,"resolv.conf",-1)),e[35]||(e[35]=n(" file:"))]),e[136]||(e[136]=o(`<div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> pod-name -- <span class="token function">cat</span> /etc/resolv.conf</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># nameserver 10.96.0.10</span></span>
<span class="line"><span class="token comment"># search default.svc.cluster.local svc.cluster.local cluster.local</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Typically, this file contains a DNS search list, including the pod&#39;s namespace and the cluster&#39;s default domain.</strong></p><p>For example, if a pod in the default namespace queries for <code>kubernetes</code>, the system appends <code>default.svc.cluster.local</code>, which resolves to the <code>ClusterIP</code> of the Kubernetes service.</p><p>After CoreDNS resolves the service name to a <code>ClusterIP</code>, the application can communicate with the service using that IP address.</p><p><em>Let&#39;s review the above with an example.</em></p><p>Create a pod to perform a DNS lookup of <code>backend-service</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run <span class="token parameter variable">-i</span> dnsutils <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--image</span><span class="token operator">=</span>gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--rm</span> <span class="token punctuation">\\</span></span>
<span class="line">  -- <span class="token function">nslookup</span> backend-service.jobs.svc.cluster.local</span>
<span class="line"><span class="token comment">#   </span></span>
<span class="line"><span class="token comment"># Server:   10.96.0.10</span></span>
<span class="line"><span class="token comment"># Address:  10.96.0.10#53</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># Name: backend-service.jobs.svc.cluster.local</span></span>
<span class="line"><span class="token comment"># Address: 10.96.5.81</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The service can be resolved by its name &quot;backend-service&quot; to its IP address <code>10.96.5.81</code>.</p><p><strong>Any other pod in Kubernetes can target this service using its name.</strong></p><p>If you request that IP address, the traffic reaches the pod:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run curl-client <span class="token parameter variable">--rm</span> <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span> <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--image</span><span class="token operator">=</span>curlimages/curl -- /bin/sh</span>
<span class="line"></span>
<span class="line"><span class="token function">curl</span> <span class="token number">10.96</span>.5.81:3000</span>
<span class="line"><span class="token comment"># {&quot;job&quot;:&quot;Instructor at Learnk8s&quot;,&quot;pod&quot;:&quot;backend-deployment-5df766bf5c-xfdng&quot;}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The backend responds with a JSON object containing a job title and the backend pod that processed the request.</p><p>Service discovery and DNS resolution work, but it is unclear how traffic is directed to the service and forwarded to the backend pods.</p><hr><h2 id="endpoints-and-services" tabindex="-1"><a class="header-anchor" href="#endpoints-and-services"><span>Endpoints and Services</span></a></h2><p><strong>Kubernetes services don&#39;t exist in the infrastructure: there&#39;s no process for listening to incoming traffic and distributing it to the pods.</strong></p><p>There&#39;s no load balancer.</p><p>Services are just definitions of how traffic should be forwarded to pods.</p><p>To confirm it, you can SSH into your cluster nodes and execute:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">netstat</span> <span class="token parameter variable">-ntlp</span> <span class="token operator">|</span> <span class="token function">grep</span> <span class="token number">10.96</span>.5.81</span>
<span class="line"><span class="token function">netstat</span> <span class="token parameter variable">-ntlp</span> <span class="token operator">|</span> <span class="token function">grep</span> <span class="token number">3000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div>`,20)),s("p",null,[e[37]||(e[37]=n("These commands will not return results because ")),e[38]||(e[38]=s("code",null,"10.96.5.81",-1)),e[39]||(e[39]=n(" is a ")),s("a",T,[l(p,{icon:"iconfont icon-k8s"}),e[36]||(e[36]=n("virtual IP managed by Kubernetes"))]),e[40]||(e[40]=n(" and is not tied to a specific process."))]),e[137]||(e[137]=s("p",null,[s("em",null,"So, how does it work?")],-1)),e[138]||(e[138]=s("p",null,"When you submit a Service to the control plane, the Endpoint controller evaluates the service's selector and notes all pods' IP addresses that match.",-1)),e[139]||(e[139]=s("p",null,[s("strong",null,"The result is stored in an Endpoint object.")],-1)),l(c,{data:[{id:"1/3"},{id:"2/3"},{id:"3/3"}],active:0},{title0:a(({value:t,isActive:i})=>e[41]||(e[41]=[n("1/3")])),title1:a(({value:t,isActive:i})=>e[42]||(e[42]=[n("2/3")])),title2:a(({value:t,isActive:i})=>e[43]||(e[43]=[n("3/3")])),tab0:a(({value:t,isActive:i})=>e[44]||(e[44]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/ae247003c4d93f1266b312e57e89145f.svg",alt:"Usually, you don't create endpoints manually. Instead, you create Services which are stored in etcd.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Usually, you don't create endpoints manually. Instead, you create Services which are stored in etcd.")],-1)])),tab1:a(({value:t,isActive:i})=>e[45]||(e[45]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/3c8b40578b0403dd2092f0d5c1331e2a.svg",alt:"For each Service, the Endpoint controller evaluates the service selector and collects the matching pod IP addresses.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"For each Service, the Endpoint controller evaluates the service selector and collects the matching pod IP addresses.")],-1)])),tab2:a(({value:t,isActive:i})=>e[46]||(e[46]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/8c6ab3c2ea0ff2cd43cec519c4235377.svg",alt:"An Endpoint object is created in etcd with all the IP addresses and port pairs.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"An Endpoint object is created in etcd with all the IP addresses and port pairs.")],-1)])),_:1}),e[140]||(e[140]=o(`<p>Let&#39;s verify that this is the case:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get endpoints,services</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                        ENDPOINTS</span></span>
<span class="line"><span class="token comment"># endpoints/backend-service   10.244.1.2:3000</span></span>
<span class="line"><span class="token comment"># endpoints/kubernetes        192.168.49.2:8443</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                      TYPE        CLUSTER-IP     PORT(S)</span></span>
<span class="line"><span class="token comment"># service/backend-service   ClusterIP   10.96.5.81     3000/TCP</span></span>
<span class="line"><span class="token comment"># service/kubernetes        ClusterIP   10.96.0.1      443/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The above output shows that an Endpoints object named <code>backend-service</code> was created.</p><p>This list has a single endpoint: <code>10.244.1.2:3000</code>.</p><p>This is our backend pod&#39;s IP address and port.</p><p><strong>The Endpoint controller created an IP address and port pair and stored it in the Endpoint object.</strong></p><p>The IP address and port pair are also called an endpoint to make everything more confusing.</p><p>However, in this article (and the rest of the Learnk8s material), we differentiate endpoints in this manner:</p><ul><li>&quot;endpoints&quot; (lowercase e) are IP addresses and port pairs.</li><li>&quot;Endpoint object&quot; (upper case E) is the object created by the Endpoint controller that contains a list of endpoints.</li></ul><p>Understanding how endpoints are collected is crucial, but it still doesn&#39;t explain how the traffic reaches the pods behind the Service if those don&#39;t exist.</p><p><strong>Kubernetes uses a clever workaround to implement a distributed load balancer using endpoints.</strong></p><hr><h2 id="kube-proxy-translating-service-ip-to-pod-ip" tabindex="-1"><a class="header-anchor" href="#kube-proxy-translating-service-ip-to-pod-ip"><span>kube-proxy: translating Service IP to Pod IP</span></a></h2><p><strong><code>kube-proxy</code> programs the Linux kernel to intercept connections made to the service IP.</strong></p><p>Then, it rewrites the destination and forwards the traffic to the available pods.</p>`,15)),l(c,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:t,isActive:i})=>e[47]||(e[47]=[n("1/4")])),title1:a(({value:t,isActive:i})=>e[48]||(e[48]=[n("2/4")])),title2:a(({value:t,isActive:i})=>e[49]||(e[49]=[n("3/4")])),title3:a(({value:t,isActive:i})=>e[50]||(e[50]=[n("4/4")])),tab0:a(({value:t,isActive:i})=>e[51]||(e[51]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/87d750f27c7ace0cb43ea848d56350d7.svg",alt:"Services don't exist, but we must pretend they exist.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Services don't exist, but we must pretend they exist.")],-1)])),tab1:a(({value:t,isActive:i})=>e[52]||(e[52]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/fe96c9b4869a132f0925e33e7adea509.svg",alt:"When the traffic reaches the node, it is intercepted and rewritten.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When the traffic reaches the node, it is intercepted and rewritten.")],-1)])),tab2:a(({value:t,isActive:i})=>e[53]||(e[53]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/f91bb1224eabbd0d088e703ac6594f29.svg",alt:"The destination IP address is replaced with one of the pod IP addresses.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The destination IP address is replaced with one of the pod IP addresses.")],-1)])),tab3:a(({value:t,isActive:i})=>e[54]||(e[54]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/224fd24a0326acd79376409da625a5c5.svg",alt:"The traffic is then forwarded to the pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The traffic is then forwarded to the pod.")],-1)])),_:1}),e[141]||(e[141]=o("<p><strong>You can think of kube-proxy and its rules as a mail redirection service.</strong></p><p>Imagine you want to move house but worry that a few friends might still send you letters to the old address.</p><p>To work around it, you set up a redirection service: when the mail reaches the post office, it is redirected and forwarded to your newer address.</p><p>Services work similarly: since they don&#39;t exist, when traffic reaches the node (e.g., the post office), it has to be redirected to a real pod.</p><p><strong>These redirection rules are set up by kube-proxy.</strong></p><p><em>But how does kube-proxy know when traffic should be intercepted and redirected?</em></p><p>Kube-proxy is a pod deployed as a DaemonSet in the cluster and subscribes to changes to Endpoint and Services.</p><p>When an Endpoint or Service is created, updated, or deleted, kube-proxy refreshes its internal state for the current node.</p><p>Then, it proceeds to update its interception and redirect rules.</p>",9)),l(c,{data:[{id:"1/3"},{id:"2/3"},{id:"3/3"}],active:0},{title0:a(({value:t,isActive:i})=>e[55]||(e[55]=[n("1/3")])),title1:a(({value:t,isActive:i})=>e[56]||(e[56]=[n("2/3")])),title2:a(({value:t,isActive:i})=>e[57]||(e[57]=[n("3/3")])),tab0:a(({value:t,isActive:i})=>e[58]||(e[58]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/2ce88554447b53c9032c40d3a1624041.svg",alt:"When a Service is created, the endpoint controller collects the IP addresses and stores them in etcd.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When a Service is created, the endpoint controller collects the IP addresses and stores them in etcd.")],-1)])),tab1:a(({value:t,isActive:i})=>e[59]||(e[59]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/53443260a7571b226099d352a2eba210.svg",alt:"Kube-proxy subscribes to updates to etcd. It has been notified of new services and endpoints.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Kube-proxy subscribes to updates to etcd. It has been notified of new services and endpoints.")],-1)])),tab2:a(({value:t,isActive:i})=>e[60]||(e[60]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/e6678fdbb6d550b76a20647acbdc9057.svg",alt:"Then it updates the iptables on the current node.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Then it updates the iptables on the current node.")],-1)])),_:1}),e[142]||(e[142]=o('<p><strong><code>kube-proxy</code> primarily sets up iptables rules to route traffic between services and pods on each node to achieve this.</strong></p><p>However, other popular tools use different options like IPVS, eBPF, and nftables.</p><p>Regardless of the underlying technology, these rules instruct the kernel to rewrite the destination IP address from the service IP to the IP of one of the pods backing the service.</p><p><em>Let&#39;s see how this works in practice.</em></p><hr><h2 id="kube-proxy-and-iptables-rules" tabindex="-1"><a class="header-anchor" href="#kube-proxy-and-iptables-rules"><span><code>kube-proxy</code> and iptables rules</span></a></h2><p>Iptables is a tool that operates at the network layer. It allows you to configure rules to control incoming and outgoing network traffic.</p><p>It&#39;s worth taking a step back and looking at how the traffic reaches the pod to understand how it works.</p>',8)),s("p",null,[s("a",S,[l(p,{icon:"fas fa-globe"}),e[61]||(e[61]=n("When traffic first arrives at the node, it's intercepted by the Linux Kernel's networking stack, and it's then forwarded to the pod."))])]),e[143]||(e[143]=s("figure",null,[s("img",{src:"https://learnk8s.io/a/b013f255aedbf3fd185b2f84c70ed28a.svg",alt:"How packets flow in a Kubernetes node",tabindex:"0",loading:"lazy"}),s("figcaption",null,"How packets flow in a Kubernetes node")],-1)),s("p",null,[e[63]||(e[63]=n("The Linux Kernel offers several hooks to customize how the traffic is handled ")),s("a",A,[l(p,{icon:"fas fa-globe"}),e[62]||(e[62]=n("depending on the stage of the network stack."))])]),e[144]||(e[144]=o('<figure><img src="https://learnk8s.io/a/fbb6a8a566625387c201b5d2d4616fac.svg" alt="Hooks in the Linux Kernel&#39;s networking stack" tabindex="0" loading="lazy"><figcaption>Hooks in the Linux Kernel&#39;s networking stack</figcaption></figure><p><strong>Iptables is a user-space application that allows you to configure these hooks and create rules to filter and manipulate the traffic.</strong></p><p>The most notable example for iptables is firewall rules.</p><p>You can define rules to allow or block traffic based on criteria such as source or destination IP address, port, protocol, and more.</p><p>For example, you might say, I want to block all traffic coming from a specific IP address or range.</p><p><em>But how does it help us with load-balancing traffic to pods?</em></p><p><strong>iptables can also be used to rewrite the destination for a packet.</strong></p><p>For example, you might say, I want to redirect all traffic from a specific IP address to another IP address.</p><p>And that&#39;s precisely what happens in Kubernetes.</p>',9)),s("p",null,[s("strong",null,[e[65]||(e[65]=n("iptables has five modes of operations (i.e. tables): filter, nat, mangle, raw and ")),s("a",N,[l(p,{icon:"fas fa-globe"}),e[64]||(e[64]=n("security"))]),e[66]||(e[66]=n("."))])]),e[145]||(e[145]=o(`<figure><img src="https://learnk8s.io/a/672ab62c81b38aac14c03156104ba120.svg" alt="Filter, Nat, mangle and raw tables in iptables" tabindex="0" loading="lazy"><figcaption>Filter, Nat, mangle and raw tables in iptables</figcaption></figure><p><strong>It&#39;s important to note that tables have different hooks available.</strong></p><p>The Nat table, primarily used in Kubernetes, has only three hooks: PREROUTING, OUTPUT, and POSTROUTING.</p><p>You can create custom rules in this table and group them into chains.</p><p>Each rule has a target and an action.</p><figure><img src="https://learnk8s.io/a/8227d773c076787d1ce4dbc9dddf15b8.svg" alt="Chains, rules, targets and actions in iptables" tabindex="0" loading="lazy"><figcaption>Chains, rules, targets and actions in iptables</figcaption></figure><p>Chains are linked to each other and the hooks to create complex workflows.</p><figure><img src="https://learnk8s.io/a/dba4e17f5635219d3935dedc40cca5b2.svg" alt="You can link iptables chains to each other and to the hooks" tabindex="0" loading="lazy"><figcaption>You can link iptables chains to each other and to the hooks</figcaption></figure><p>In Kubernetes, <strong><code>kube-proxy</code> programs iptables so that when a packet arrives at the node, its destination IP address is matched against all service IP addresses.</strong></p><p>If there&#39;s a match, the traffic is forwarded to a specific chain that handles load balancing for that service.</p><p>As pods are added or removed from the service, <code>kube-proxy</code> dynamically updates these chains.</p><p>To see how chains interact in kube-proxy, let&#39;s follow the traffic from pod to service.</p><hr><h2 id="following-traffic-from-a-pod-to-service" tabindex="-1"><a class="header-anchor" href="#following-traffic-from-a-pod-to-service"><span>Following traffic from a Pod to Service</span></a></h2><p>First, deploy a <code>curl</code> client pod in the cluster:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run curl-client <span class="token parameter variable">--rm</span> <span class="token parameter variable">-i</span> <span class="token parameter variable">--tty</span> <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--image</span><span class="token operator">=</span>curlimages/curl -- /bin/sh</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>Inside the <code>curl-client</code> pod, send a request to the <code>backend-service</code> using its <code>ClusterIP</code> <code>10.96.5.81</code> on port <code>3000</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">curl</span> http://10.96.5.81:3000</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>On another terminal, launch a privileged container that has access to the host&#39;s network stack:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run <span class="token parameter variable">-it</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--privileged</span> <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--image</span><span class="token operator">=</span>ubuntu <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--overrides</span><span class="token operator">=</span><span class="token string">&#39;{&quot;spec&quot;: {&quot;hostNetwork&quot;: true, &quot;hostPID&quot;: true}}&#39;</span> <span class="token punctuation">\\</span></span>
<span class="line">  ubuntu -- <span class="token function">bash</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Inside the container, update the package list and install <code>iptables</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">apt</span> update <span class="token operator">&amp;&amp;</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> iptables</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div>`,22)),s("p",null,[s("strong",null,[e[68]||(e[68]=n("When a request is sent to the Service, it enters the node's network stack, where it is ")),s("a",K,[l(p,{icon:"fas fa-globe"}),e[67]||(e[67]=n("intercepted by the iptables rules set by kube-proxy."))])])]),e[146]||(e[146]=o(`<p>The process begins in the <code>PREROUTING</code> chain of the <code>nat</code> table, where incoming packets are matched to service IPs.</p><p>Since the destination IP matches <code>10.96.5.81</code>, the <code>KUBE-SERVICES</code> chain processes the packet.</p><p>Let&#39;s inspect the <code>PREROUTING</code> chain with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> PREROUTING --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-SERVICES  all  anywhere      anywhere             /* kubernetes service portals */</span></span>
<span class="line"><span class="token comment"># 2  DOCKER_OUTPUT  all  anywhere      host.minikube.internal</span></span>
<span class="line"><span class="token comment"># 3  DOCKER         all  anywhere      anywhere             ADDRTYPE match dst-type LOCAL</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://learnk8s.io/a/cbb58cedcff254bc18e574b8abb06727.svg" alt="The PREROUTING chain is followed by the KUBE-SERVICE chain" tabindex="0" loading="lazy"><figcaption>The PREROUTING chain is followed by the KUBE-SERVICE chain</figcaption></figure><p>You can explore what happens next by inspecting the <code>KUBE-SERVICES</code> chain.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SERVICES <span class="token parameter variable">-n</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-SVC-NPX46M4PTMTKRN6Y  /* default/kubernetes:https cluster IP */ tcp dpt:443</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SVC-TCOU7JCQXEZGVUNU  /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span></span>
<span class="line"><span class="token comment"># 3  KUBE-SVC-ERIFXISQEP7F7OF4  /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span></span>
<span class="line"><span class="token comment"># 4  KUBE-SVC-JD5MR3NA4I4DYORP  /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span></span>
<span class="line"><span class="token comment"># 5  KUBE-SVC-6R7RAWWNQI6ZLKMO  /* default/backend-service:backend cluster IP */ tcp dpt:3000</span></span>
<span class="line"><span class="token comment"># 6  KUBE-NODEPORTS             /* kubernetes service nodeports;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Don&#39;t get scared by the long list of IDs.</p><p><strong>You are seeing a list of redirection rules: one for each service.</strong></p><p><em>But you only created one service; how come there are so many?</em></p><p>Kubernetes already has services for CoreDNS, the API server, and more; each is implemented as a chain in iptables.</p><p>You can verify (and match) those chains to their respective Service by listing all services in your cluster:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get services <span class="token parameter variable">-A</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAMESPACE     NAME              TYPE        CLUSTER-IP      PORT(S)</span></span>
<span class="line"><span class="token comment"># default       kubernetes        ClusterIP   10.96.0.1       443/TCP</span></span>
<span class="line"><span class="token comment"># kube-system   kube-dns          ClusterIP   10.96.0.10      53/UDP,53/TCP</span></span>
<span class="line"><span class="token comment"># kube-system   metrics-server    ClusterIP   10.96.10.5      443/TCP</span></span>
<span class="line"><span class="token comment"># jobs          backend-service   ClusterIP   10.96.5.81      3000/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now that the output makes more sense, let&#39;s continue.</p><p>The <code>KUBE-SERVICE</code> chain is a collection of chains.</p><p>Only the <code>KUBE-SVC-6R7RAWWNQI6ZLKMO</code> chain matches the destination IP <code>10.96.5.81</code>.</p><figure><img src="https://learnk8s.io/a/d01bf84aa5efab3dbdda8750cc90609e.svg" alt="The KUBE-SERVICE chain invokes the KUBE-SVC-6R7RAWWNQI6ZLKMO chain" tabindex="0" loading="lazy"><figcaption>The KUBE-SERVICE chain invokes the KUBE-SVC-6R7RAWWNQI6ZLKMO chain</figcaption></figure><p>Let&#39;s inspect this chain:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SVC-6R7RAWWNQI6ZLKMO <span class="token parameter variable">-n</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-MARK-MASQ             --  /* default/backend-service:backend cluster IP */ tcp dpt:3000</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SEP-O3HWD4DESFNXEYL6  --  /* default/backend-service:backend -&gt; 10.244.1.2:3000 */</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The first chain is <code>KUBE-MARK-MASQ</code>, which patches the source IP when the destination is external to the cluster.</p><p>The second chain, <code>KUBE-SEP-O3HWD4DESFNXEYL6</code>, is the Service Endpoint chain.</p><figure><img src="https://learnk8s.io/a/3969fa46f374a9dd0262b595d706f347.svg" alt="Inspecting the KUBE-SEP chain" tabindex="0" loading="lazy"><figcaption>Inspecting the KUBE-SEP chain</figcaption></figure><p>If you inspect this chain, you will find two rules:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SEP-O3HWD4DESFNXEYL6 <span class="token parameter variable">-n</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1    KUBE-MARK-MASQ    10.244.1.2    0.0.0.0/0    /* default/backend-service:backend */</span></span>
<span class="line"><span class="token comment"># 2    DNAT                                         /* default/backend-service:backend */ tcp to:10.244.1.2:3000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,24)),s("p",null,[e[70]||(e[70]=n("The first rule marks the packet for masquerading if ")),s("a",U,[l(p,{icon:"fa-brands fa-stack-overflow"}),e[69]||(e[69]=n("the pod requesting the service is also chosen as the destination."))])]),e[147]||(e[147]=o(`<p>The <code>DNAT</code> rule changes the destination IP from the service IP (<code>10.96.5.81</code>) to the pod&#39;s IP (<code>10.244.1.2</code>).</p><figure><img src="https://learnk8s.io/a/e9f3eb841484634102d40ffbc02d72c7.svg" alt="Inspecting the KUBE-SEP chain" tabindex="0" loading="lazy"><figcaption>Inspecting the KUBE-SEP chain</figcaption></figure><p><em>What happens when you scale the deployments to three replicas?</em></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl scale deployment backend-deployment <span class="token parameter variable">--replicas</span><span class="token operator">=</span><span class="token number">3</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>The <code>KUBE-SVC-6R7RAWWNQI6ZLKMO</code> chain now has three <code>KUBE-SEP</code> chains:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SVC-6R7RAWWNQI6ZLKMO <span class="token parameter variable">-n</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-MARK-MASQ              /* default/backend-service:backend cluster IP */ tcp dpt:3000</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SEP-O3HWD4DESFNXEYL6   /* default/backend-service:backend -&gt; 10.244.1.2:3000 */</span></span>
<span class="line"><span class="token comment"># 3  KUBE-SEP-C2Y64IBVPH4YIBGX   /* default/backend-service:backend -&gt; 10.244.1.3:3000 */</span></span>
<span class="line"><span class="token comment"># 4  KUBE-SEP-MRYDKJV5U7PLF5ZN   /* default/backend-service:backend -&gt; 10.244.1.4:3000 */</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Each rule points to a chain that changes the destination IP to one of the three pods.</strong></p><figure><img src="https://learnk8s.io/a/1da21ccdaad3cbd0fd81ac99ca230f8e.svg" alt="Scaling to three replicas creates more KUBE-SEP chains" tabindex="0" loading="lazy"><figcaption>Scaling to three replicas creates more KUBE-SEP chains</figcaption></figure><p>You finally got to the bottom of how Kubernetes Services works.</p><p>Let&#39;s create a deployment for the frontend app that will consume the API exposed by the backend.</p><hr><h2 id="deploying-and-exposing-the-frontend-pods" tabindex="-1"><a class="header-anchor" href="#deploying-and-exposing-the-frontend-pods"><span>Deploying and exposing the frontend Pods</span></a></h2><p>This is what <code>frontend-deployment.yaml</code> looks like:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="frontend-deployment.yaml"><span>frontend-deployment.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>deployment</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> ghcr.io/learnk8s/jobs<span class="token punctuation">-</span>api</span>
<span class="line">        <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">          <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You can submit the definition to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> frontend-deployment.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># deployment.apps/frontend-deployment created</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let&#39;s verify the deployment was successful:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pod <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>frontend <span class="token parameter variable">-o</span> wide</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                                   READY   STATUS    IP           NODE</span></span>
<span class="line"><span class="token comment"># frontend-deployment-66dd585966-2bjtt   1/1     Running   10.244.1.7   minikube-m02</span></span>
<span class="line"><span class="token comment"># frontend-deployment-66dd585966-rxtxt   1/1     Running   10.244.2.6   minikube-m03</span></span>
<span class="line"><span class="token comment"># frontend-deployment-66dd585966-w8szs   1/1     Running   10.244.0.5   minikube</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Unlike the backend service, the frontend service does not need to be reachable from other pods in the cluster.</p><p><strong>It must be reached by clients using web browsers outside of the cluster.</strong></p><hr><h2 id="exposing-the-frontend-pods" tabindex="-1"><a class="header-anchor" href="#exposing-the-frontend-pods"><span>Exposing the frontend pods</span></a></h2><p>The Service you used until now was the default Kubernetes service: ClusterIP.</p><p><strong>However, Kubernetes has a second option for when you wish to expose your pods externally to the cluster: NodePort.</strong></p><p>Let&#39;s create a NodePort service:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="frontend-service.yaml"><span>frontend-service.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>service</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort</span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend</span>
<span class="line">      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP</span>
<span class="line">      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You can submit it to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> frontend-service.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># service/frontend-service created</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Next, let&#39;s verify the creation of the Endpoints resource by running:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get endpoints frontend-service</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME               ENDPOINTS</span></span>
<span class="line"><span class="token comment"># frontend-service   10.244.0.5:8080,10.244.1.7:8080,10.244.2.6:8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The <code>frontend-service</code> is associated with three pod IPs: <code>10.244.0.5</code>, <code>10.244.1.7</code>, and <code>10.244.2.6</code>, each listening on port <code>8080</code>.</p><p>These IPs correspond to the individual frontend deployment pods.</p><p>If you get the <code>frontend-service</code>, you can verify the <code>TYPE</code> column shows <code>NodePort</code> with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get <span class="token function">service</span> frontend-service</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)</span></span>
<span class="line"><span class="token comment"># frontend-service   NodePort    10.96.154.187   &lt;none&gt;        80:32073/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>When you create a service of this type, the API server allocates a unique port from the service node port range.</strong></p><p>This range typically defaults to 30000-32767, and in this case is 32073.</p><figure><img src="https://learnk8s.io/a/d68ed61b2d2abb2353e2ab6627317f98.svg" alt="The current setup with a NodePort Service, 3 front-end pods and 3 backend pods" tabindex="0" loading="lazy"><figcaption>The current setup with a NodePort Service, 3 front-end pods and 3 backend pods</figcaption></figure><p><strong>This NodePort is open on all nodes in the cluster, not just the node where the pod is running.</strong></p><p>As per the ClusterIP service, NodePort doesn&#39;t require a specific process to be listening on that port.</p><p>When traffic reaches the port, it gets intercepted and redirected by iptables rules.</p><p>The chain for the NodePorts is called <code>KUBE-NODEPORTS</code> and is the last chain in <code>KUBE-SERVICES</code>.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SERVICES <span class="token parameter variable">-n</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-SVC-NPX46M4PTMTKRN6Y  /* default/kubernetes:https cluster IP */ tcp dpt:443</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SVC-TCOU7JCQXEZGVUNU  /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span></span>
<span class="line"><span class="token comment"># 3  KUBE-SVC-ERIFXISQEP7F7OF4  /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span></span>
<span class="line"><span class="token comment"># 4  KUBE-SVC-JD5MR3NA4I4DYORP  /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span></span>
<span class="line"><span class="token comment"># 5  KUBE-SVC-6R7RAWWNQI6ZLKMO  /* default/backend-service:backend cluster IP */ tcp dpt:3000</span></span>
<span class="line"><span class="token comment"># 6  KUBE-NODEPORTS             /* kubernetes service nodeports;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://learnk8s.io/a/d09ea0f6efecf91e396f755108fb29a9.svg" alt="The KUBE-NODEPORTS chain is the last chain in KUBE-SERVICES" tabindex="0" loading="lazy"><figcaption>The KUBE-NODEPORTS chain is the last chain in KUBE-SERVICES</figcaption></figure><p><em>What&#39;s inside this chain?</em></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-NODEPORTS <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span> --line-numbers <span class="token operator">|</span> <span class="token function">grep</span> <span class="token number">32073</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-EXT-6XQSOA4B4HLF6UNI /* jobs/frontend-service:frontend */ tcp dpt:32073</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The <code>KUBE-NODEPORTS</code> chain lists all NodePorts in the cluster.</p><p>For each NodePort, there&#39;s a rule that redirects traffic to the corresponding service.</p><figure><img src="https://learnk8s.io/a/c97765bc42ce6a32119b8432a68c3a83.svg" alt="the KUBE-NODEPORTS chain lists all NodePorts in the cluster" tabindex="0" loading="lazy"><figcaption>the KUBE-NODEPORTS chain lists all NodePorts in the cluster</figcaption></figure><p>Let&#39;s inspect the <code>KUBE-EXT-6XQSOA4B4HLF6UNI</code> chain:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-EXT-6XQSOA4B4HLF6UNI <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-MARK-MASQ      /* masquerade traffic for jobs/frontend-service:frontend external destinations */</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SVC-6XQSOA4B4HLF6UNI</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>You might recognize the second chain: it&#39;s the Service&#39;s chain!</strong></p><p>So, if the incoming traffic is destined for the NodePort, it gets redirected to the correct Service chain.</p><figure><img src="https://learnk8s.io/a/1c9c1988dfaa8060e1b083e7e729575b.svg" alt="The KUBE-EXT chain links back to the Service" tabindex="0" loading="lazy"><figcaption>The KUBE-EXT chain links back to the Service</figcaption></figure><p>As you&#39;ve already experienced, this chain will have a <code>KUBE-SEP-*</code> chain for each pod in the Service.</p><figure><img src="https://learnk8s.io/a/4a105e698aa894ec20307ad19ee5e4ec.svg" alt="The KUBE-SEP chains will load balance the traffic to the destination pods" tabindex="0" loading="lazy"><figcaption>The KUBE-SEP chains will load balance the traffic to the destination pods</figcaption></figure><p>This is the third time you have found the <code>KUBE-MARK-MASQ</code> chain.</p>`,56)),s("ol",null,[s("li",null,[e[72]||(e[72]=n("In the first case (in ")),e[73]||(e[73]=s("code",null,"KUBE-SEP",-1)),e[74]||(e[74]=n("), it was used for making sure that the ")),e[75]||(e[75]=s("strong",null,"traffic originated from a pod could use a service that has the same pod as the destination",-1)),e[76]||(e[76]=n()),s("a",R,[l(p,{icon:"fa-brands fa-reddit"}),e[71]||(e[71]=n("(hairpin NAT)"))]),e[77]||(e[77]=n("."))]),s("li",null,[e[79]||(e[79]=n("The second case (in ")),e[80]||(e[80]=s("code",null,"KUBE-SVC",-1)),e[81]||(e[81]=n(") ensured that ")),s("a",B,[l(p,{icon:"fas fa-globe"}),e[78]||(e[78]=n("traffic external to the cluster using the ClusterIP could be routed correctly."))])]),e[82]||(e[82]=s("li",null,[n("The third case is just now in the NodePort chain (in "),s("code",null,"KUBE-EXT"),n(").")],-1))]),e[148]||(e[148]=s("p",null,"To understand why those are necessary, consider the following scenario.",-1)),e[149]||(e[149]=s("p",null,"You make a request to a NodePort.",-1)),e[150]||(e[150]=s("p",null,"The request has been redirected to one of the pods in the service.",-1)),e[151]||(e[151]=s("p",null,[s("em",null,"The Pod processes the request and replies, but where should the traffic go back to?")],-1)),e[152]||(e[152]=s("p",null,[s("strong",null,"The source IP address is your IP address; however, you are not expecting a response from a Pod but from a node.")],-1)),e[153]||(e[153]=s("p",null,"When the traffic reaches the node port, the source IP address is changed to that of the local node to work around the issue.",-1)),l(c,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:t,isActive:i})=>e[83]||(e[83]=[n("1/4")])),title1:a(({value:t,isActive:i})=>e[84]||(e[84]=[n("2/4")])),title2:a(({value:t,isActive:i})=>e[85]||(e[85]=[n("3/4")])),title3:a(({value:t,isActive:i})=>e[86]||(e[86]=[n("4/4")])),tab0:a(({value:t,isActive:i})=>e[87]||(e[87]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/4883ae2bdfc783eb69cae3d10b37c163.svg",alt:"As soon as you make a request to a NodePort service, the traffic is intercepted.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"As soon as you make a request to a NodePort service, the traffic is intercepted.")],-1)])),tab1:a(({value:t,isActive:i})=>e[88]||(e[88]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/0ce896813f5427428f79ec25f107c990.svg",alt:"We know that iptables rewrites the request to target a pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"We know that iptables rewrites the request to target a pod.")],-1)])),tab2:a(({value:t,isActive:i})=>e[89]||(e[89]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/3caa033d537256d0159e6fa6475951f2.svg",alt:"When the traffic is external to the cluster, the source IP is also replaced with the Node's IP address.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When the traffic is external to the cluster, the source IP is also replaced with the Node's IP address.")],-1)])),tab3:a(({value:t,isActive:i})=>e[90]||(e[90]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/e4624dcd5da128da8e1d9b0479c8b73b.svg",alt:"The traffic finally reaches the pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The traffic finally reaches the pod.")],-1)])),_:1}),e[154]||(e[154]=s("p",null,"When the traffic returns, the source IP address is amended to the real IP.",-1)),l(c,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:t,isActive:i})=>e[91]||(e[91]=[n("1/4")])),title1:a(({value:t,isActive:i})=>e[92]||(e[92]=[n("2/4")])),title2:a(({value:t,isActive:i})=>e[93]||(e[93]=[n("3/4")])),title3:a(({value:t,isActive:i})=>e[94]||(e[94]=[n("4/4")])),tab0:a(({value:t,isActive:i})=>e[95]||(e[95]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/8ea46f052f0a5bfbae37660e0122f0c7.svg",alt:"As soon as the request exits the node, it is intercepted.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"As soon as the request exits the node, it is intercepted.")],-1)])),tab1:a(({value:t,isActive:i})=>e[96]||(e[96]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/746f1d97820da2ae3166bd9f833e732f.svg",alt:"The source IP address is amended again to be the original requester.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The source IP address is amended again to be the original requester.")],-1)])),tab2:a(({value:t,isActive:i})=>e[97]||(e[97]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/c34fd75980cb0a38dd94a302aad8cceb.svg",alt:"Iptables also help us pretend the traffic comes from a service.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Iptables also help us pretend the traffic comes from a service.")],-1)])),tab3:a(({value:t,isActive:i})=>e[98]||(e[98]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/2aadcb00882dab22a8d30e3117868618.svg",alt:"The traffic finally reaches the original requester.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The traffic finally reaches the original requester.")],-1)])),_:1}),e[155]||(e[155]=o(`<p>Let&#39;s inspect the <code>KUBE-MARK-MASQ</code> chain to understand the details.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-MARK-MASQ <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 542 28196 MARK    *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>When the <code>KUBE-MARK-MASQ</code> is invoked, it applies a mark with the 0x4000 value to all packets.</p><p><em>And that&#39;s it.</em></p><p>When the packet is about to exit the node, the <code>KUBE-POSTROUTING</code> chain will check for the mark.</p><p>Let&#39;s inspect the <code>POSTROUTING</code> chain:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> POSTROUTING <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># KUBE-POSTROUTING      --  0.0.0.0/0            0.0.0.0/0/* kubernetes postrouting rules */</span></span>
<span class="line"><span class="token comment"># IP-MASQ-AGENT         --  0.0.0.0/0            0.0.0.0/0  ADDRTYPE match dst-type !LOCAL</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>And let&#39;s inspect the <code>KUBE-POSTROUTING</code> chain:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-POSTROUTING <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># RETURN     --  *      *       0.0.0.0/0   0.0.0.0/0   mark match ! 0x4000/0x4000</span></span>
<span class="line"><span class="token comment"># MARK       --  *      *       0.0.0.0/0   0.0.0.0/0   MARK xor 0x4000</span></span>
<span class="line"><span class="token comment"># MASQUERADE --  *      *       0.0.0.0/0   0.0.0.0/0   random-fully</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The three lines read:</p><ol><li>If there is no MARK, there is no need to apply SNAT. Return to the caller.</li><li>If there&#39;s a MARK, remove it.</li><li>Finally, apply SNAT so that the traffic is correctly handled.</li></ol><figure><img src="https://learnk8s.io/a/02d0869171f6756052514e83b250b2cc.svg" alt=" chain" tabindex="0" loading="lazy"><figcaption><code>KUBE-POSTROUTING</code> chain</figcaption></figure><p><strong>But there&#39;s a drawback: the pods see the Node&#39;s IP as the source of the request and not the original client.</strong></p>`,13)),s("p",null,[e[100]||(e[100]=n("You can set ")),e[101]||(e[101]=s("code",null,"externalTrafficPolicy: Local",-1)),e[102]||(e[102]=n(" to ")),s("a",L,[l(p,{icon:"iconfont icon-k8s"}),e[99]||(e[99]=n("preserve the client's original IP."))])]),e[156]||(e[156]=o(`<p><strong>However, this change comes with its trade-offs, which we&#39;ll explore shortly.</strong></p><p>Before discussing those, let&#39;s reflect on the two Services you have used:</p><ul><li>ClusterIP has a virtual IP address used by kube-proxy as a placeholder to redirect traffic.</li><li>NodePort extends ClusterIP, inheriting all of its inner workings and exposing an external port to route external traffic.</li></ul><p>ClusterIP and NodePort aren&#39;t the only two Service types in a Kubernetes cluster.</p><p>A popular option is the <code>type: LoadBalancer</code> service.</p><hr><h2 id="load-balancer-service" tabindex="-1"><a class="header-anchor" href="#load-balancer-service"><span>Load Balancer Service</span></a></h2><p>In a typical cloud environment, Kubernetes&#39; cloud controller manager automatically provisions a load balancer from the cloud provider and assigns an external IP to the service of <code>type: LoadBalancer</code>.</p><p>The load balancer forwards requests to the service&#39;s NodePort, which kube-proxy directs to the appropriate pod.</p><figure><img src="https://learnk8s.io/a/adee2fea9f2eac5443960964d015173c.svg" alt="A LoadBalancer service in Kubernetes is a combination of a NodePort and a cloud load balancer" tabindex="0" loading="lazy"><figcaption>A LoadBalancer service in Kubernetes is a combination of a NodePort and a cloud load balancer</figcaption></figure><p><strong>Some load balancers preserve the source IP, but by default, the NodePort setup might hide it.</strong></p><p>Let&#39;s use Minikube to demonstrate this behaviour; the same principles apply to other load balancers, including cloud-based ones.</p><p>To test it, amend the frontend service file as follows:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="frontend-service.yaml"><span>frontend-service.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>service</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line highlighted">  <span class="token key atrule">type</span><span class="token punctuation">:</span> LoadBalancer</span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend</span>
<span class="line">      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP</span>
<span class="line">      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>Resubmit the resource to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> frontend-service.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># service/fronted-service configured</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now you can see the TYPE column shows <code>LoadBalancer</code>.</p><p>Let&#39;s inspect the service with the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get <span class="token function">service</span> frontend-service</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME               TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)</span></span>
<span class="line"><span class="token comment"># frontend-service   LoadBalancer   10.107.34.82   &lt;pending&gt;   80:32073/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let&#39;s execute <code>minikube tunnel</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">minikube tunnel</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME               TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE</span></span>
<span class="line"><span class="token comment"># frontend-service   LoadBalancer   10.107.34.82   10.107.34.82   80:32073/TCP   4d12h</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>When a LoadBalancer controller is installed in a cloud environment, it assigns a unique external IP from its pool of addresses to each LoadBalancer service.</strong></p><p>This IP is often publicly accessible and differs from the <code>ClusterIP</code>, which belongs to the cluster network.</p><p>When using Minikube, if you make a browser request to the IP address <code>10.107.34.82</code> on port <code>80</code>, the <code>minikube tunnel</code> intercepts the request.</p><p>The tunnel forwards the request to the corresponding <code>nodePort</code>, such as <code>32073</code>, on the Minikube node.</p><p>Once the request reaches the node, <code>kube-proxy</code> translates the <code>nodePort</code> to one of the frontend pod IPs (<code>10.244.0.5</code>, <code>10.244.1.7</code>, or <code>10.244.2.6</code>) and port <code>8080</code>.</p><p><strong>Whether you use <code>minikube tunnel</code> or provision a cloud load balancer, in both cases, the traffic is forwarded to the NodePort first.</strong></p><p>In other words, the LoadBalancer service extends NodePort with a mechanism to load balance requests to multiple NodePorts.</p><p>From there onwards, the traffic is intercepted and redirected as usual.</p><p>Until now, we&#39;ve discussed NodePort and LoadBalancer services, where traffic is always routed to a node and then to a pod running on that node.</p><p><em>But what happens when the traffic hits a NodePort that doesn&#39;t have a pod?</em></p><hr><h2 id="extra-hop-with-kube-proxy-and-intra-cluster-load-balancing" tabindex="-1"><a class="header-anchor" href="#extra-hop-with-kube-proxy-and-intra-cluster-load-balancing"><span>Extra hop with kube-proxy and intra-cluster load balancing</span></a></h2><p><strong>When a load balancer sends traffic to a NodePort, the node receiving the request may not always run the pod.</strong></p><p>In such cases, the iptables rules forward the traffic to another node where the target pod is located.</p><p>Let&#39;s test this in the two-node Minikube cluster, where a frontend application pod runs only on one node, <code>minikube-m02</code> with IP <code>192.168.49.3</code>.</p><p>However, the traffic is routed through <code>minikube-m03</code> (IP <code>192.168.49.4</code>) via the NodePort service.</p><figure><img src="https://learnk8s.io/a/763e3989f0a2e781c6ab537b3d4e9167.svg" alt="A three node cluster with a single pod exposed via a NodePort" tabindex="0" loading="lazy"><figcaption>A three node cluster with a single pod exposed via a NodePort</figcaption></figure><p><code>kube-proxy</code>&#39;s rules on <code>minikube-m03</code> forward the traffic to <code>minikube-m02</code>, where the pod is running.</p><p>With a tool like tcpdump, you can observe this extra hop capturing the traffic on <code>minikube-m02</code>.</p><p>First, identify the nodes in the cluster:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get <span class="token function">node</span> <span class="token parameter variable">-o</span> wide</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME           STATUS   VERSION   INTERNAL-IP</span></span>
<span class="line"><span class="token comment"># minikube-m02   Ready    v1.30.0   192.168.49.3</span></span>
<span class="line"><span class="token comment"># minikube-m03   Ready    v1.30.0   192.168.49.4</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Then, ssh into <code>minikube-m02</code> and install <code>tcpdump</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">minikube <span class="token function">ssh</span> <span class="token parameter variable">-n</span> minikube-m02</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># sudo apt update &amp;&amp; sudo apt install tcpdump -y</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>A network interface is a connection point for communication.</p><p>In Minikube, you usually see <code>eth0</code>, <code>docker0</code>, <code>bridge</code>, and <code>veth</code> interfaces.</p><p>The following command captures and displays network packets from all interfaces present on the minikube nodes, filtering traffic from <code>minikube-m03</code> (<code>192.168.49.4</code>) and targeting NodePort <code>32073</code> for the <code>frontend-service</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">tcpdump <span class="token parameter variable">-i</span> any <span class="token function">host</span> <span class="token number">192.168</span>.49.4 and port <span class="token number">32073</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Lastly, on another terminal, ssh into <code>minikube-m03</code> and send a curl request to <code>minikube-m02</code> on NodePort <code>32073</code>.</p><p>This may seem like you&#39;re sending a request to &quot;yourself&quot;, but the request is intended for the NodePort service, which is available on all nodes:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">minikube <span class="token function">ssh</span> <span class="token parameter variable">-n</span> minikube-m03</span>
<span class="line"><span class="token function">curl</span> http://192.168.49.3:32073</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>If you turn back to the tcpdump session, you&#39;ll see an output like this:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="output"><span>output</span></div><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">...</span>
<span class="line">09:53:06.968588 eth0  In  IP minikube-m03.minikube.39802 &gt; minikube-m02.32073: Flags [S], seq 3518088234,</span>
<span class="line">09:53:06.968613 eth0  Out IP minikube-m02.32073 &gt; minikube-m03.minikube.39802: Flags [S.], seq 4178857212,</span>
<span class="line">...</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>The curl request initially reaches <code>minikube-m03</code>, but the pod is not on <code>minikube-m03</code>.</p><p>Since there are no local endpoints, iptables rules forward the traffic to <code>minikube-m02</code> because the pod is running there.</p>`,55)),l(c,{data:[{id:"1/2"},{id:"2/2"}],active:0},{title0:a(({value:t,isActive:i})=>e[103]||(e[103]=[n("1/2")])),title1:a(({value:t,isActive:i})=>e[104]||(e[104]=[n("2/2")])),tab0:a(({value:t,isActive:i})=>e[105]||(e[105]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/131f3e7ff7cc0f48b86db0edccbefd4c.svg",alt:"When the traffic reaches a node that doesn't have any pod, it is intercepted by iptables rules.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When the traffic reaches a node that doesn't have any pod, it is intercepted by iptables rules.")],-1)])),tab1:a(({value:t,isActive:i})=>e[106]||(e[106]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/6c6e87844ed35382f332ce34a3467948.svg",alt:"The traffic is then forwarded to the node that has the pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The traffic is then forwarded to the node that has the pod.")],-1)])),_:1}),e[157]||(e[157]=o(`<p><strong>While this is great when you have fewer Pods than nodes, it also has drawbacks.</strong></p><p>Imagine having three pods, one on each node, exposed by a NodePort service.</p><p><em>If you send a request to the NodePort on the second node, does the traffic reach the pod on that node?</em></p><p>Not necessarily.</p><p>Let&#39;s inspect the iptables rules for the service again:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SVC-6XQSOA4B4HLF6UNI <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># KUBE-MARK-MASQ             /* jobs/frontend-service:frontend cluster IP */ tcp dpt:80</span></span>
<span class="line"><span class="token comment"># KUBE-SEP-IWKGAKCJPN2GDKZP  /* jobs/frontend-service:frontend -&gt; 10.244.0.3:8080 */ statistic mode random probability 0.3</span></span>
<span class="line"><span class="token comment"># KUBE-SEP-G5YGW63JRCEDVAQZ  /* jobs/frontend-service:frontend -&gt; 10.244.1.4:8080 */ statistic mode random probability 0.5</span></span>
<span class="line"><span class="token comment"># KUBE-SEP-SVT42V6SNWE7E3YK  /* jobs/frontend-service:frontend -&gt; 10.244.2.3:8080 */</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Kube-proxy uses the iptables &quot;statistic mode random probability&quot; mode to assign to each rule to ensure a roughly proportional distribution of connections.</strong></p><p>There&#39;s no rule that the pod should serve the traffic in the current node.</p>`,8)),l(c,{data:[{id:"1/4"},{id:"2/4"},{id:"3/4"},{id:"4/4"}],active:0},{title0:a(({value:t,isActive:i})=>e[107]||(e[107]=[n("1/4")])),title1:a(({value:t,isActive:i})=>e[108]||(e[108]=[n("2/4")])),title2:a(({value:t,isActive:i})=>e[109]||(e[109]=[n("3/4")])),title3:a(({value:t,isActive:i})=>e[110]||(e[110]=[n("4/4")])),tab0:a(({value:t,isActive:i})=>e[111]||(e[111]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/4e112e366eb0f0556af0c431b67fe6bb.svg",alt:"Traffic reaches a NodePort in the cluster, and the same node hosts a pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Traffic reaches a NodePort in the cluster, and the same node hosts a pod.")],-1)])),tab1:a(({value:t,isActive:i})=>e[112]||(e[112]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/ac20bc492fb835aa7a0f82445ca45efc.svg",alt:"The traffic is intercepted, and the iptables rules will use one of the KUBE-SEP-XXX chains to forward the traffic to a pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"The traffic is intercepted, and the iptables rules will use one of the KUBE-SEP-XXX chains to forward the traffic to a pod.")],-1)])),tab2:a(({value:t,isActive:i})=>e[113]||(e[113]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/7c70133c7cb861952218e1f258dd4587.svg",alt:"Since those chains use a statistic mode to distribute the traffic, there's no guarantee that the traffic will be forwarded to the pod in the current node.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Since those chains use a statistic mode to distribute the traffic, there's no guarantee that the traffic will be forwarded to the pod in the current node.")],-1)])),tab3:a(({value:t,isActive:i})=>e[114]||(e[114]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/31b5a35d7c62eb6fd1715aeca878ec5e.svg",alt:"In this case, the iptables rules selected the first pod.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"In this case, the iptables rules selected the first pod.")],-1)])),_:1}),e[158]||(e[158]=o(`<p><strong>As a consequence, when using NodePort or Loadbalancer, traffic might incur an extra hop as it is redirected elsewhere.</strong></p><p><em>Is this a problem?</em></p><p>Not really.</p><p>Latency within a cluster network might be a handful of milliseconds, and this strategy ensures that all pods will serve the same number of requests.</p><p>However, if you are running a latency-sensitive application, you might want to ensure that a pod always serves traffic on the local node.</p><p>In that case, you should use <code>externalTrafficPolicy: Local</code> in your service.</p><hr><h2 id="externaltrafficpolicy-local-preserving-the-source-ip-in-kubernetes" tabindex="-1"><a class="header-anchor" href="#externaltrafficpolicy-local-preserving-the-source-ip-in-kubernetes"><span>ExternalTrafficPolicy: Local, preserving the source IP in Kubernetes</span></a></h2><p>You have already encountered <code>externalTrafficPolicy: Local</code> twice so far:</p><ol><li>First, when you discussed NodePort, SNAT and preserving the IP address.</li><li>Then, when discussing intra-cluster load balancing (and the extra hop).</li></ol><p><em>What happens when you switch your services from <code>externalTrafficPolicy: Cluster</code> to <code>externalTrafficPolicy: Local</code>?</em></p><p>Let&#39;s find out.</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="frontend-service.yaml"><span>frontend-service.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>service</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">type</span><span class="token punctuation">:</span> NodePort</span>
<span class="line highlighted">  <span class="token key atrule">externalTrafficPolicy</span><span class="token punctuation">:</span> Local</span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend</span>
<span class="line">      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP</span>
<span class="line">      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>Resubmit the resource to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> frontend-service.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># service/fronted-service configured</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You&#39;ll notice a few differences if you compare the rules for the <code>externalTrafficPolicy: Cluster</code> and <code>Local</code> iptables.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-EXT-6XQSOA4B4HLF6UNI <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-MARK-MASQ               0.0.0.0/0            0.0.0.0/0</span></span>
<span class="line"><span class="token comment"># 2  KUBE-SVC-6XQSOA4B4HLF6UNI    0.0.0.0/0            0.0.0.0/0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>In <code>Cluster</code> mode, iptables always:</p><ol><li>Alters all external traffic source IPs to the node&#39;s IP through SNAT.</li><li>Invokes the Service&#39;s chain.</li></ol><p>Let&#39;s explore <code>Local</code> mode:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-EXT-6XQSOA4B4HLF6UNI <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span> --line-numbers</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1  KUBE-SVC-6XQSOA4B4HLF6UNI    10.244.0.0/16    0.0.0.0/0</span></span>
<span class="line"><span class="token comment"># 2  KUBE-MARK-MASQ               0.0.0.0/0        0.0.0.0/0 ADDRTYPE match src-type LOCAL</span></span>
<span class="line"><span class="token comment"># 3  KUBE-SVC-6XQSOA4B4HLF6UNI    0.0.0.0/0        0.0.0.0/0 ADDRTYPE match src-type LOCAL</span></span>
<span class="line"><span class="token comment"># 3  KUBE-SVL-6XQSOA4B4HLF6UNI    0.0.0.0/0        0.0.0.0/0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>In Local mode, there are many more rules and a new chain: <code>KUBE-SVL-6XQSOA4B4HLF6UNI</code>.</p><p>Inspecting the new chain reveals that the traffic reaches the pod directly without SNAT:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">iptables <span class="token parameter variable">-t</span> nat <span class="token parameter variable">-L</span> KUBE-SVL-6XQSOA4B4HLF6UNI <span class="token parameter variable">-n</span> <span class="token parameter variable">-v</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># 1 KUBE-SEP-4B2TTHBRUYTSCT32  0.0.0.0/0   0.0.0.0/0  /* default/frontend-service -&gt; 10.244.1.2:80 */</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The <code>KUBE-SEP-4B2TTHBRUYTSCT32</code> chain is the endpoint chain for the pod.</p><p>The only necessary translation is the destination NAT (DNAT) to the pod&#39;s IP — there is no SNAT.</p><p><em>What about all the other chains?</em></p><p>They are scoped to the local node so that traffic that consumes the service from the local node can still work (hence the <code>ADDRTYPE match src-type LOCAL</code>).</p><p>Those chains keep the Service running normally if the traffic originates from the local node and cluster.</p><p>While <code>externalTrafficPolicy: Local</code> preserves the source IP and routes traffic only to nodes with running pods, there are downsides:</p><ul><li><strong>The traffic is lost if no pods are on the node.</strong></li><li>Since we can&#39;t forward the load to other pods, one pod might receive more traffic than the others.</li></ul><p>While this answers the initial questions, it also raises another.</p><p><em>Does the <code>type: LoadBalancer</code> service know not to route traffic to nodes without a pod?</em></p><p>Yes.</p><p><strong>When you use a LoadBalancer service in Kubernetes, the cloud provider&#39;s load balancer needs to know which nodes are healthy and ready to serve traffic.</strong></p><p>It does this by regularly performing health checks on the nodes.</p><p>These checks typically target a NodePort and happen every 60 seconds.</p><p>If the node has a healthy pod running the service, it passes the check, and the load balancer routes traffic to it.</p><p>If the node does not have active pods for the service, it fails the check, and traffic stops being sent to that node.</p><p>When you set <code>externalTrafficPolicy: Local</code>, Kubernetes assigns a <code>healthCheckNodePort</code> to verify the health of the service&#39;s nodes.</p><p>You can check the assigned port with the following command:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get svc frontend-service <span class="token parameter variable">-o</span> yaml <span class="token operator">|</span> <span class="token function">grep</span> <span class="token parameter variable">-i</span> healthCheckNodePort</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># healthCheckNodePort: 31722</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To check if the node is healthy, you can curl the health check endpoint like this:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">curl</span> localhost:<span class="token operator">&lt;</span>healthCheckNodePort<span class="token operator">&gt;</span>/healthz</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>On a node with a healthy pod running the service, you&#39;ll see something like this:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="output"><span>output</span></div><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">&quot;localEndpoints&quot;: 1,</span>
<span class="line">&quot;serviceProxyHealthy&quot;: true</span>
<span class="line"># truncated output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>On a node without a pod, you might see:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="output"><span>output</span></div><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">&quot;localEndpoints&quot;: 0,</span>
<span class="line">&quot;serviceProxyHealthy&quot;: true</span>
<span class="line"># truncated output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>The load balancer uses this information to decide which nodes should receive traffic.</p>`,49)),l(c,{data:[{id:"1/2"},{id:"2/2"}],active:0},{title0:a(({value:t,isActive:i})=>e[115]||(e[115]=[n("1/2")])),title1:a(({value:t,isActive:i})=>e[116]||(e[116]=[n("2/2")])),tab0:a(({value:t,isActive:i})=>e[117]||(e[117]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/f6ec9453bf3587e715e68b9462805fdc.svg",alt:"Nodes have /healthz endpoints and track if they have local pods for the current NodePort.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"Nodes have /healthz endpoints and track if they have local pods for the current NodePort.")],-1)])),tab1:a(({value:t,isActive:i})=>e[118]||(e[118]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/96c6587f57d9c9c590c89c0496e45084.svg",alt:"When they don't, the response is still successful, but the count is zero.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When they don't, the response is still successful, but the count is zero.")],-1)])),_:1}),e[159]||(e[159]=o('<p>It will send traffic only to nodes with running pods for the service (in this example, the node with <code>localEndpoints: 1</code>).</p><p>Once the traffic reaches the NodePort, iptables&#39; rules forward the request to the pod.</p><p>Having the load balancer check the pod&#39;s existence works well, but it also introduces a new challenge.</p><p><em>What if kube-proxy removes the chain from iptables rules just after the load balancer probes the health check?</em></p><p><em>For the next 60 seconds, the load balancer isn&#39;t aware that the pod is gone.</em></p><hr><h2 id="proxyterminatingendpoints-in-kubernetes" tabindex="-1"><a class="header-anchor" href="#proxyterminatingendpoints-in-kubernetes"><span>ProxyTerminatingEndpoints in Kubernetes</span></a></h2><p>When you update the container image in your pods, Kubernetes replaces old pods with new ones with a rolling update.</p><p>As each new pod becomes ready, <code>kube-proxy</code> immediately updates the routing rules and removes the old pod&#39;s IP address from the iptables (or IPVS) NAT table to prevent traffic from reaching the terminating pod.</p><p><strong>However, if the cloud LoadBalancer still uses the old IP (because its health check interval isn&#39;t due yet), it may continue sending traffic to a node without a healthy pod.</strong></p><p>Requests are dropped until the next health check on the <code>healthCheckNodePort</code>.</p>',11)),s("p",null,[s("a",C,[l(p,{icon:"iconfont icon-k8s"}),e[119]||(e[119]=n("The ")),e[120]||(e[120]=s("code",null,"ProxyTerminatingEndpoints",-1)),e[121]||(e[121]=n(" feature, introduced in Kubernetes v1.26, handles this issue by allowing terminating pods to remain available for existing connections."))])]),e[160]||(e[160]=o(`<p>Before v1.26, once a pod was marked as &quot;terminating&quot;, it would stop serving traffic.</p><p><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1669-proxy-terminating-endpoints/README.md" target="_blank" rel="noopener noreferrer">With <code>ProxyTerminatingEndpoints</code>, when a pod starts terminating, it&#39;s not immediately removed from active endpoints. (&lt;VPIcon icon=&quot;iconfont icon-github&quot;/<code>kubernetes/enhancements</code>)</a></p><p>Instead, it is marked with the conditions <code>terminating</code> and <code>serving</code>.</p><p>This means the pod is shutting down but can still serve existing traffic until all tasks are complete.</p><p>If we take a look at the Endpoint object while the pod with IP <code>192.168.0.171</code> is terminating, it might look like this:</p><div class="language-json line-numbers-mode" data-highlighter="prismjs" data-ext="json"><pre><code class="language-json"><span class="line"><span class="token property">&quot;endpoints&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span></span>
<span class="line">  <span class="token punctuation">{</span></span>
<span class="line">    <span class="token property">&quot;addresses&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">&quot;192.168.0.171&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token property">&quot;conditions&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span></span>
<span class="line">      <span class="token property">&quot;ready&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span></span>
<span class="line">      <span class="token property">&quot;serving&quot;</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span></span>
<span class="line">      <span class="token property">&quot;terminating&quot;</span><span class="token operator">:</span> <span class="token boolean">true</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line">  <span class="token punctuation">}</span></span>
<span class="line"><span class="token punctuation">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>kube-proxy</code> has already removed the pod&#39;s IP from iptables to stop new traffic from reaching it.</p><p>However, because the pod is still marked as <code>serving</code>, it will continue processing existing connections until it is fully terminated.</p><p>After receiving the termination signal, the pod completes the request, performs a cleanup task, and only then shuts down gracefully.</p><p>Until now, you&#39;ve learned about <code>externalTrafficPolicy: Local</code> and discussed some drawbacks and workarounds.</p><p><strong>On the positive side, your pod knows the request&#39;s real source IP.</strong></p><p>On the less positive side:</p><ul><li>The traffic is dropped if there&#39;s no pod on the node.</li><li>LoadBalancers need an extra health check to know the pod&#39;s presence on the node.</li><li>Those same health checks are susceptible to the health check frequency and graceful shutdown of pods.</li></ul><p>With great power comes a significant burden.</p><p><em>However, what if you could skip that and load balance the traffic directly from the load balancer to the pods (while skipping the nodes)?</em></p><hr><h2 id="how-can-the-pod-s-ip-address-be-routable-from-the-load-balancer" tabindex="-1"><a class="header-anchor" href="#how-can-the-pod-s-ip-address-be-routable-from-the-load-balancer"><span>How can the Pod&#39;s IP address be routable from the load balancer?</span></a></h2><p><strong>Pod IPs are non-routable in most Kubernetes environments because they exist within an internal Pod CIDR block.</strong></p><p>To route traffic directly to the pods, their IPs must be part of a routable network that the external load balancer can see.</p><p>This can be achieved using certain Container Networking Interfaces (CNIs) that assign pod IPs from the same subnet as the node&#39;s network.</p><p>Azure Kubernetes Service offers the Azure CNI, in which pods receive IP addresses from the same Virtual Network (VNet) as the nodes, making them directly routable by the load balancer.</p><p>Another example is EKS with the AWS-CNI: the IP addresses are assigned from the VPC and can be routed by the Application Load Balancer.</p>`,22)),l(c,{data:[{id:"1/2"},{id:"2/2"}],active:0},{title0:a(({value:t,isActive:i})=>e[122]||(e[122]=[n("1/2")])),title1:a(({value:t,isActive:i})=>e[123]||(e[123]=[n("2/2")])),tab0:a(({value:t,isActive:i})=>e[124]||(e[124]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/a6d3093b92c193baf86d2ca9f80c0985.svg",alt:"When you use the AWS-CNI, all pods and nodes share IP addresses from the current VPC.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When you use the AWS-CNI, all pods and nodes share IP addresses from the current VPC.")],-1)])),tab1:a(({value:t,isActive:i})=>e[125]||(e[125]=[s("figure",null,[s("img",{src:"https://learnk8s.io/a/cb1a60252d71ed83bc1b63ee2a5fd8ae.svg",alt:"When you use the Azure CNI, all pods and nodes share IP addresses from the current VNet.",tabindex:"0",loading:"lazy"}),s("figcaption",null,"When you use the Azure CNI, all pods and nodes share IP addresses from the current VNet.")],-1)])),_:1}),e[161]||(e[161]=o(`<p><em>This is only sometimes the case, though.</em></p><p>Taking AKS as an example again, with the default CNI, Kubenet, pods are assigned non-routable IPs from an internal CIDR block, meaning only the nodes are visible to the load balancer.</p><figure><img src="https://learnk8s.io/a/4056f187d30fe03670453ccef293761b.svg" alt="When using kubenet ins AKS, Pod&#39;s IP addresess are using an internal CIDR" tabindex="0" loading="lazy"><figcaption>When using kubenet ins AKS, Pod&#39;s IP addresess are using an internal CIDR</figcaption></figure><p>When you provision a LoadBalancer Service with Kubenet, traffic forwarded by the load balancer reaches the nodes, and external traffic is routed to the pods through Network Address Translation (NAT).</p><p>Let&#39;s amend the previous AKS cluster, upgrade it to Azure CNI and set <code>allocateLoadBalancerNodePorts: false</code>.</p><p>This explicitly instructs the load balancer to bypass NodePorts and route traffic directly to pod IPs.</p><p>First, update your AKS cluster to enable Azure CNI with an overlay network configuration:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">az aks update <span class="token parameter variable">--name</span> <span class="token variable">$CLUSTER_NAME</span> <span class="token punctuation">\\</span></span>
<span class="line">  --resource-group <span class="token variable">$RESOURCE_GROUP</span> <span class="token punctuation">\\</span></span>
<span class="line">  --network-plugin-mode overlay <span class="token punctuation">\\</span></span>
<span class="line">  --pod-cidr <span class="token number">192.168</span>.0.0/16</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Next, apply the following LoadBalancer service configuration that disables NodePort allocation and ensures local traffic routing:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="frontend-service.yaml"><span>frontend-service.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend<span class="token punctuation">-</span>service</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">type</span><span class="token punctuation">:</span> LoadBalancer</span>
<span class="line">  <span class="token key atrule">allocateLoadBalancerNodePorts</span><span class="token punctuation">:</span> <span class="token boolean important">false</span></span>
<span class="line highlighted">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> frontend</span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> frontend</span>
<span class="line">      <span class="token key atrule">protocol</span><span class="token punctuation">:</span> TCP</span>
<span class="line">      <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">      <span class="token key atrule">targetPort</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You can apply the amended file to the cluster with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> frontend-service.yaml</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># service/fronted-service configured</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let&#39;s inspect the IP addresses of your nodes and pods:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get nodes,pods <span class="token parameter variable">-o</span> wide</span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                                     STATUS   VERSION   INTERNAL-IP</span></span>
<span class="line"><span class="token comment"># node/aks-nodepool1-27397936-vmss000000   Ready    v1.29.8   10.224.0.5</span></span>
<span class="line"><span class="token comment"># node/aks-nodepool1-27397936-vmss000001   Ready    v1.29.8   10.224.0.4</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME                                       READY   STATUS    IP              NODE</span></span>
<span class="line"><span class="token comment"># pod/backend-deployment-6b54b594f5-ds297    1/1     Running   192.168.1.125   aks-nodepool1-27397936-vmss000000</span></span>
<span class="line"><span class="token comment"># pod/frontend-deployment-84f666b996-5nlvb   1/1     Running   192.168.0.251   aks-nodepool1-27397936-vmss000001</span></span>
<span class="line"><span class="token comment"># pod/frontend-deployment-84f666b996-ldx22   1/1     Running   192.168.0.171   aks-nodepool1-27397936-vmss000001</span></span>
<span class="line"><span class="token comment"># pod/frontend-deployment-84f666b996-pk7qx   1/1     Running   192.168.1.178   aks-nodepool1-27397936-vmss000000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>If you list the Services, you might notice the missing NodePort:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get <span class="token function">service</span></span>
<span class="line"><span class="token comment"># </span></span>
<span class="line"><span class="token comment"># NAME               TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)</span></span>
<span class="line"><span class="token comment"># backend-service    ClusterIP      10.0.96.133   &lt;none&gt;        3000/TCP</span></span>
<span class="line"><span class="token comment"># frontend-service   LoadBalancer   10.0.190.76   4.255.39.7    80/TCP</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>The node IPs (<code>10.224.x.x</code>) are part of the VNet&#39;s address space, and the pod IPs (<code>192.168.x.x</code>) are allocated from the pod CIDR we&#39;ve configured in the VNet.</strong></p><p>Both nodes and pods are now part of the routable network, which allows the load balancer to communicate directly with the pods without needing NodePorts or NAT.</p><p>Let&#39;s capture the traffic with a privileged container and install <code>tcpdump</code> on the node where the application is running.</p><p>Since the frontend deployment pods run on both nodes, we can choose any pod to capture the traffic.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run <span class="token parameter variable">-it</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--privileged</span> <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--image</span><span class="token operator">=</span>ubuntu <span class="token punctuation">\\</span></span>
<span class="line">  <span class="token parameter variable">--overrides</span><span class="token operator">=</span><span class="token string">&#39;{&quot;spec&quot;: {&quot;hostNetwork&quot;: true, &quot;hostPID&quot;: true}}&#39;</span> <span class="token punctuation">\\</span></span>
<span class="line">  ubuntu -- <span class="token function">bash</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let&#39;s make sure that <code>tcpdump</code> is installed.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">apt</span> update <span class="token operator">&amp;&amp;</span> <span class="token function">apt</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> tcpdump</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Now, capture the traffic to one of the frontend pod IPs, such as <code>192.168.0.171</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">tcpdump <span class="token parameter variable">-i</span> eth0 <span class="token function">host</span> <span class="token number">192.168</span>.0.171</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>On a terminal on your host machine, you may want to generate traffic with:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">curl</span> http://4.255.39.7</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Once you simulate requests, return to the <code>tcpdump</code> session.</p><p>The output may look like this:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="output"><span>output</span></div><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">06:31:41.431009 IP 192.168.0.171.42222 &gt; 192.168.1.125.3000: Flags [.], ack 1, win 502,</span>
<span class="line">06:31:41.431327 IP 192.168.0.171.42222 &gt; 192.168.1.125.3000: Flags [P.], seq 1:102, ack 1,</span>
<span class="line"># truncated output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p><strong>Now, the load balancer connects directly to the pod and routes traffic straight to the pod&#39;s IP.</strong></p><p>The traffic flows straight from the load balancer <code>4.255.39.7</code> to the pod&#39;s IP <code>192.168.0.171</code> on port 80.</p><figure><img src="https://learnk8s.io/a/395c0e1b8c2b1d36aedebeb21ddb9bd3.svg" alt="Routing traffic directly to pods and skipping NodePorts" tabindex="0" loading="lazy"><figcaption>Routing traffic directly to pods and skipping NodePorts</figcaption></figure><p>The frontend pod processes the request and forwards traffic to the backend pod <code>192.168.1.125</code>.</p><p><strong>The request doesn&#39;t touch the node&#39;s IP at all.</strong></p><p>This is possible because the load balancer sends traffic straight to the pod&#39;s IP.</p><p>As a benefit, the source IP is preserved, even without <code>externalTrafficPolicy: Local</code>.</p><p>Even though iptables rules are no longer involved in routing the external traffic, DNS lookups and service-to-service communication within the cluster still depend on CoreDNS, kube-proxy and iptables rules.</p><hr><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><ul><li>Services are logical abstractions. Kube-proxy handles the traffic routing using iptables (or IPVS as an alternative), forwarding traffic to the correct pods by rewriting packet destinations.</li><li>Setting <code>externalTrafficPolicy: Local</code> preserves the original client IP while directing traffic only to nodes that have running pods, eliminating the extra hop.</li><li>Cloud load balancers perform health checks on <code>healthCheckNodePort</code> to verify that nodes are healthy before routing traffic to them, ensuring only nodes with active pods receive traffic.</li><li>Load balancers can be configured to route traffic directly to pods, bypassing NodePorts, which can improve performance and preserve the source IP.</li></ul>`,41)),s("p",null,[e[129]||(e[129]=n("A special thank you goes to ")),s("a",q,[e[126]||(e[126]=n("Michael O'Leary (")),l(p,{icon:"fa-brands fa-linkedin"}),e[127]||(e[127]=s("code",null,"michael-w-oleary",-1)),e[128]||(e[128]=n(")"))]),e[130]||(e[130]=n(", who wrote the initial draft of this article and offered some invaluable feedback."))]),f(" TODO: add ARTICLE CARD "),l(u,h(m({title:"Kubernetes networking: service, kube-proxy, load balancing",desc:"Master Kubernetes networking with Services and load balancing. Learn how traffic flows within clusters and from external sources.",link:"https://chanhi2000.github.io/bookshelf/learnk8s.com/kubernetes-services-and-load-balancing.html",logo:"https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg",background:"rgba(102,152,204,0.2)"})),null,16)])}const V=g(P,[["render",O]]),M=JSON.parse('{"path":"/learnk8s.io/kubernetes-long-lived-connections.html","title":"Kubernetes networking: service, kube-proxy, load balancing","lang":"en-US","frontmatter":{"lang":"en-US","title":"Kubernetes networking: service, kube-proxy, load balancing","description":"Article(s) > Kubernetes networking: service, kube-proxy, load balancing","icon":"iconfont icon-k8s","category":["DevOps","Kubernetes","Article(s)"],"tag":["blog","learnk8s.com","devops","kubernetes","k8s"],"head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Kubernetes networking: service, kube-proxy, load balancing\\",\\"image\\":[\\"https://learnk8s.io/a/a72a5e5ce63b136493614bfc156afdb5.svg\\",\\"https://learnk8s.io/a/449d83d8882e5ded09a64ffa1c49bf88.svg\\",\\"https://learnk8s.io/a/e4f22a3358e5cf13ba9c26bb5ca5c563.svg\\",\\"https://learnk8s.io/a/90c0d1ef18f196de9b7936300b885060.svg\\",\\"https://learnk8s.io/a/7cc292393948f1019ec0cc3e197952e7.svg\\",\\"https://learnk8s.io/a/0c9be1e6d56a3df58dd4d8f12d229e33.svg\\",\\"https://learnk8s.io/a/ae247003c4d93f1266b312e57e89145f.svg\\",\\"https://learnk8s.io/a/3c8b40578b0403dd2092f0d5c1331e2a.svg\\",\\"https://learnk8s.io/a/8c6ab3c2ea0ff2cd43cec519c4235377.svg\\",\\"https://learnk8s.io/a/87d750f27c7ace0cb43ea848d56350d7.svg\\",\\"https://learnk8s.io/a/fe96c9b4869a132f0925e33e7adea509.svg\\",\\"https://learnk8s.io/a/f91bb1224eabbd0d088e703ac6594f29.svg\\",\\"https://learnk8s.io/a/224fd24a0326acd79376409da625a5c5.svg\\",\\"https://learnk8s.io/a/2ce88554447b53c9032c40d3a1624041.svg\\",\\"https://learnk8s.io/a/53443260a7571b226099d352a2eba210.svg\\",\\"https://learnk8s.io/a/e6678fdbb6d550b76a20647acbdc9057.svg\\",\\"https://learnk8s.io/a/b013f255aedbf3fd185b2f84c70ed28a.svg\\",\\"https://learnk8s.io/a/fbb6a8a566625387c201b5d2d4616fac.svg\\",\\"https://learnk8s.io/a/672ab62c81b38aac14c03156104ba120.svg\\",\\"https://learnk8s.io/a/8227d773c076787d1ce4dbc9dddf15b8.svg\\",\\"https://learnk8s.io/a/dba4e17f5635219d3935dedc40cca5b2.svg\\",\\"https://learnk8s.io/a/cbb58cedcff254bc18e574b8abb06727.svg\\",\\"https://learnk8s.io/a/d01bf84aa5efab3dbdda8750cc90609e.svg\\",\\"https://learnk8s.io/a/3969fa46f374a9dd0262b595d706f347.svg\\",\\"https://learnk8s.io/a/e9f3eb841484634102d40ffbc02d72c7.svg\\",\\"https://learnk8s.io/a/1da21ccdaad3cbd0fd81ac99ca230f8e.svg\\",\\"https://learnk8s.io/a/d68ed61b2d2abb2353e2ab6627317f98.svg\\",\\"https://learnk8s.io/a/d09ea0f6efecf91e396f755108fb29a9.svg\\",\\"https://learnk8s.io/a/c97765bc42ce6a32119b8432a68c3a83.svg\\",\\"https://learnk8s.io/a/1c9c1988dfaa8060e1b083e7e729575b.svg\\",\\"https://learnk8s.io/a/4a105e698aa894ec20307ad19ee5e4ec.svg\\",\\"https://learnk8s.io/a/4883ae2bdfc783eb69cae3d10b37c163.svg\\",\\"https://learnk8s.io/a/0ce896813f5427428f79ec25f107c990.svg\\",\\"https://learnk8s.io/a/3caa033d537256d0159e6fa6475951f2.svg\\",\\"https://learnk8s.io/a/e4624dcd5da128da8e1d9b0479c8b73b.svg\\",\\"https://learnk8s.io/a/8ea46f052f0a5bfbae37660e0122f0c7.svg\\",\\"https://learnk8s.io/a/746f1d97820da2ae3166bd9f833e732f.svg\\",\\"https://learnk8s.io/a/c34fd75980cb0a38dd94a302aad8cceb.svg\\",\\"https://learnk8s.io/a/2aadcb00882dab22a8d30e3117868618.svg\\",\\"https://learnk8s.io/a/02d0869171f6756052514e83b250b2cc.svg\\",\\"https://learnk8s.io/a/adee2fea9f2eac5443960964d015173c.svg\\",\\"https://learnk8s.io/a/763e3989f0a2e781c6ab537b3d4e9167.svg\\",\\"https://learnk8s.io/a/131f3e7ff7cc0f48b86db0edccbefd4c.svg\\",\\"https://learnk8s.io/a/6c6e87844ed35382f332ce34a3467948.svg\\",\\"https://learnk8s.io/a/4e112e366eb0f0556af0c431b67fe6bb.svg\\",\\"https://learnk8s.io/a/ac20bc492fb835aa7a0f82445ca45efc.svg\\",\\"https://learnk8s.io/a/7c70133c7cb861952218e1f258dd4587.svg\\",\\"https://learnk8s.io/a/31b5a35d7c62eb6fd1715aeca878ec5e.svg\\",\\"https://learnk8s.io/a/f6ec9453bf3587e715e68b9462805fdc.svg\\",\\"https://learnk8s.io/a/96c6587f57d9c9c590c89c0496e45084.svg\\",\\"https://learnk8s.io/a/a6d3093b92c193baf86d2ca9f80c0985.svg\\",\\"https://learnk8s.io/a/cb1a60252d71ed83bc1b63ee2a5fd8ae.svg\\",\\"https://learnk8s.io/a/4056f187d30fe03670453ccef293761b.svg\\",\\"https://learnk8s.io/a/395c0e1b8c2b1d36aedebeb21ddb9bd3.svg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Gulcan Topcu\\",\\"url\\":\\"https://www.linkedin.com/in/gulcantopcu/\\"}]}"],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-long-lived-connections.html"}],["meta",{"property":"og:site_name","content":"📚Bookshelf"}],["meta",{"property":"og:title","content":"Kubernetes networking: service, kube-proxy, load balancing"}],["meta",{"property":"og:description","content":"Article(s) > Kubernetes networking: service, kube-proxy, load balancing"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://static.learnk8s.io/1fd1550ac4cb6e44a8f57d11d45ad42f.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://static.learnk8s.io/1fd1550ac4cb6e44a8f57d11d45ad42f.png"}],["meta",{"name":"twitter:image:alt","content":"Kubernetes networking: service, kube-proxy, load balancing"}],["meta",{"property":"article:author","content":"Gulcan Topcu"}],["meta",{"property":"article:tag","content":"k8s"}],["meta",{"property":"article:tag","content":"kubernetes"}],["meta",{"property":"article:tag","content":"devops"}],["meta",{"property":"article:tag","content":"learnk8s.com"}],["meta",{"property":"article:tag","content":"blog"}],[{"meta":null},{"property":"og:title","content":"Article(s) > Load balancing and scaling long-lived connections in Kubernetes"},{"property":"og:description","content":"Load balancing and scaling long-lived connections in Kubernetes"},{"property":"og:url","content":"https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-long-lived-connections.html"}]],"prev":"/devops/k8s/articles/README.md","isOriginal":false,"author":[{"name":"Gulcan Topcu","url":"https://www.linkedin.com/in/gulcantopcu/"}],"cover":"https://static.learnk8s.io/1fd1550ac4cb6e44a8f57d11d45ad42f.png"},"git":{},"readingTime":{"minutes":26.83,"words":8049},"filePathRelative":"learnk8s.io/kubernetes-long-lived-connections.md","copyright":{"author":"Gulcan Topcu"}}');export{V as comp,M as data};
