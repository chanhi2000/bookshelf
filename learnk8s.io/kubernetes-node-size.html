<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.24" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.94" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Architecting Kubernetes clusters — choosing a worker node size","image":["https://learnk8s.io/a/c642b260295b87df85d97a6e8c20be48.svg","https://learnk8s.io/a/d627f4247a50662c83a2a40703a8b693.svg","https://learnk8s.io/a/3de0f4647a4b0d71b196d4e394aa5451.svg","https://learnk8s.io/a/e77f8c687be1e38bd35470e177e4290a.svg","https://learnk8s.io/a/c72727b60ee6387d73c40004f9003561.svg","https://learnk8s.io/a/c1be9a8de0824f6e5abcba5371bb9b4e.svg","https://learnk8s.io/a/f97ca286843a3ff5ee0fbc9a55630829.svg","https://learnk8s.io/a/23885fd9cefd09159d3bab9618a55aae.svg","https://learnk8s.io/a/8166b554c895b638a68d2ef76475f943.svg","https://learnk8s.io/a/2e379971f411f7ca2908ebed0798c020.svg","https://learnk8s.io/a/4db4fb1e1af84fcbcec51b23f9b9e77f.svg","https://learnk8s.io/a/4b0ddbe420f754323537c3bde7f938e2.svg","https://learnk8s.io/a/a26ee4d5052d327089d62b6da143f914.svg","https://learnk8s.io/a/6a29222a3225ebb38f4e758210ca85b5.svg","https://learnk8s.io/a/f9fd8d7bfbc21dbf4e2f34d68489d1f7.svg","https://learnk8s.io/a/ab3adead1eea7f1615bff64488318317.svg","https://learnk8s.io/a/4b5b12047d321e34a6c2455677fd99a0.svg","https://learnk8s.io/a/be78be3cd35dc7387e2352c4630c5ae9.svg","https://learnk8s.io/a/b321498cc64bfd4bc424abd7a8a8f461.svg","https://learnk8s.io/a/352ea906f7800c90ed71deea01eac4f1.svg","https://learnk8s.io/a/d4dfe6e68938734993dd6b5bd2498e7b.svg","https://learnk8s.io/a/c77ad0aa9e0afa24e57788948ee1b12c.svg","https://learnk8s.io/a/d3b49aee1e2a56dfe0509c3874c308a8.svg","https://learnk8s.io/a/6b09aa74b75b631b7be55c492d80c084.svg","https://learnk8s.io/a/4f053af905c5ceb1f89c9a70dcabebd9.svg","https://learnk8s.io/a/cb5baa1a4b290bff10e3f693fb5fce97.svg","https://learnk8s.io/a/106f96256da416271d46d757e912c983.svg","https://learnk8s.io/a/cc3274f66d411c87546f1eb88a42511d.svg","https://learnk8s.io/a/01b4a22bf8aa92a7998540f6ac1516e0.svg","https://learnk8s.io/a/ba13194ea5b9ef63597e8fb059355887.svg","https://learnk8s.io/a/eaba36dda660dd289189979c3b6ce3c4.svg","https://learnk8s.io/a/c05c50cd03dba4465b38b95a016b3712.svg","https://learnk8s.io/a/e038d5441bbab5e27bff91fa567517e0.svg","https://learnk8s.io/a/f0bdd2ddeb962d9e3303965eef0006d3.svg","https://learnk8s.io/a/362ab748b014147011e2a723ae8fa71b.svg","https://learnk8s.io/a/7b2d466a8559f46f477801ae2ca257fd.svg","https://learnk8s.io/a/a785b7150b052e5f5f6906c17c7312b8.svg"],"datePublished":"2023-08-15T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"Daniele Polencic","url":"https://linkedin.com/in/danielepolencic"}]}</script><meta property="og:url" content="https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-node-size.html"><meta property="og:site_name" content="📚Bookshelf"><meta property="og:title" content="Architecting Kubernetes clusters — choosing a worker node size"><meta property="og:description" content="Article(s) > Architecting Kubernetes clusters — choosing a worker node size"><meta property="og:type" content="article"><meta property="og:image" content="https://static.learnk8s.io/a102852d1e938e7c95a134501111ed92.png"><meta property="og:locale" content="en-US"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:src" content="https://static.learnk8s.io/a102852d1e938e7c95a134501111ed92.png"><meta name="twitter:image:alt" content="Architecting Kubernetes clusters — choosing a worker node size"><meta property="article:author" content="Daniele Polencic"><meta property="article:tag" content="k8s"><meta property="article:tag" content="kubernetes"><meta property="article:tag" content="devops"><meta property="article:tag" content="learnk8s.io"><meta property="article:tag" content="blog"><meta property="article:published_time" content="2023-08-15T00:00:00.000Z"><link rel="icon" href="/bookshelf/assets/icon/favicon.svg"><title>Architecting Kubernetes clusters — choosing a worker node size | 📚Bookshelf</title><meta name="description" content="Article(s) > Architecting Kubernetes clusters — choosing a worker node size">
    <link rel="preload" href="/bookshelf/assets/style-DmRYlEXZ.css" as="style"><link rel="stylesheet" href="/bookshelf/assets/style-DmRYlEXZ.css">
    
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/bookshelf/" aria-label="Take me home"><img class="vp-nav-logo" src="/bookshelf/assets/icon/favicon.svg" alt><!----><span class="vp-site-name hide-in-pad">📚Bookshelf</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/bookshelf/news.html" aria-label><!--[--><i class="vp-icon fas fa-rss" sizing="height"></i><!--]--><!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label><!--[--><i class="vp-icon fas fa-keyboard" sizing="height"></i><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/hackingwithswift.com/" aria-label="hackingwithswift.com"><!--[--><img class="vp-icon" src="https://hackingwithswift.com/favicon.svg" alt aria-hidden no-view sizing="both"><!--]-->hackingwithswift.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/fcc/" aria-label="freecodecamp.org"><!--[--><img class="vp-icon" src="https://cdn.freecodecamp.org/universal/favicons/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->freecodecamp.org<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/kodeco.com/" aria-label="kodeco.com"><!--[--><img class="vp-icon" src="https://kodeco.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->kodeco.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/blog.kotzilla.io/" aria-label="blog.kotzilla.io"><!--[--><img class="vp-icon" src="https://blog.kotzilla.io/hubfs/favicon.png" alt aria-hidden no-view sizing="both"><!--]-->blog.kotzilla.io<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/kt.academy/" aria-label="kt.academy"><!--[--><img class="vp-icon" src="https://kt.academy/logo.png" alt aria-hidden no-view sizing="both"><!--]-->kt.academy<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/droidcon.com/" aria-label="droidcon.com"><!--[--><img class="vp-icon" src="https://droidcon.com/wp-content/uploads/2021/07/favicon-300x300.png" alt aria-hidden no-view sizing="both"><!--]-->droidcon.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/outcomeschool.com/" aria-label="outcomeschool.com"><!--[--><img class="vp-icon" src="https://outcomeschool.com/static/favicons/apple-touch-icon.png" alt aria-hidden no-view sizing="both"><!--]-->outcomeschool.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/frontendmasters.com/" aria-label="frontendmasters.com"><!--[--><img class="vp-icon" src="https://frontendmasters.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->frontendmasters.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/css-tricks.com/" aria-label="css-tricks.com"><!--[--><img class="vp-icon" src="https://css-tricks.com/favicon.svg" alt aria-hidden no-view sizing="both"><!--]-->css-tricks.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/smashingmagazine.com/" aria-label="smashingmagazine.com"><!--[--><img class="vp-icon" src="https://smashingmagazine.com/images/favicon/favicon.svg" alt aria-hidden no-view sizing="both"><!--]-->smashingmagazine.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/blog.logrocket.com/" aria-label="blog.logrocket.com"><!--[--><img class="vp-icon" src="/bookshelf/assets/image/blog.logrocket.com/favicon.png" alt aria-hidden no-view sizing="both"><!--]-->blog.logrocket.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/realpython.com/" aria-label="realpython.com"><!--[--><img class="vp-icon" src="https://realpython.com/static/favicon.68cbf4197b0c.png" alt aria-hidden no-view sizing="both"><!--]-->realpython.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/digitalocean.com/" aria-label="digitalocean.com"><!--[--><img class="vp-icon" src="https://digitalocean.com/_next/static/media/favicon.594d6067.ico" alt aria-hidden no-view sizing="both"><!--]-->digitalocean.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/antonioleiva.com/" aria-label="antonioleiva.com"><!--[--><img class="vp-icon" src="/bookshelf/assets/image/antonioleiva.com/favicon.png" alt aria-hidden no-view sizing="both"><!--]-->antonioleiva.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/johnnyreilly.com/" aria-label="johnnyreilly.com"><!--[--><img class="vp-icon" src="https://johnnyreilly.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->johnnyreilly.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/code-maze.com/" aria-label="code-maze.com"><!--[--><img class="vp-icon" src="/bookshelf/assets/image/code-maze.com/favicon.png" alt aria-hidden no-view sizing="both"><!--]-->code-maze.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/milanjovanovic.tech/" aria-label="milanjovanovic.tech"><!--[--><img class="vp-icon" src="https://milanjovanovic.tech/profile_favicon.png" alt aria-hidden no-view sizing="both"><!--]-->milanjovanovic.tech<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/shopify.engineering/" aria-label="shopify.engineering"><!--[--><img class="vp-icon" src="https://cdn.shopify.com/static/shopify-favicon.png" alt aria-hidden no-view sizing="both"><!--]-->shopify.engineering<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/devtoolstips.org/" aria-label="devtoolstips.org"><!--[--><img class="vp-icon" src="https://devtoolstips.org/assets/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->devtoolstips.org<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/piccalil.li/" aria-label="piccalil.li"><!--[--><img class="vp-icon" src="https://piccalil.li/favicons/apple-touch-icon.png" alt aria-hidden no-view sizing="both"><!--]-->piccalil.li<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/sitepoint.com/" aria-label="sitepoint.com"><!--[--><img class="vp-icon" src="https://sitepoint.com/favicons/512x512.png" alt aria-hidden no-view sizing="both"><!--]-->sitepoint.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/event-driven.io/" aria-label="event-driven.io"><!--[--><img class="vp-icon" src="/bookshelf/assets/image/event-driven.io/favicon.jfif" alt aria-hidden no-view sizing="both"><!--]-->event-driven.io<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/packagemain.tech/" aria-label="packagemain.tech"><!--[--><img class="vp-icon" src="https://substack-post-media.s3.amazonaws.com/public/images/2ea54e25-eaa6-4630-bfc0-10b8cfdce894/apple-touch-icon-1024x1024.png" alt aria-hidden no-view sizing="both"><!--]-->packagemain.tech<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/gosolve.io/" aria-label="gosolve.io"><!--[--><img class="vp-icon" src="https://gosolve.io/wp-content/uploads/2022/03/cropped-ikona1-192x192.png" alt aria-hidden no-view sizing="both"><!--]-->gosolve.io<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/towardsdatascience.com/" aria-label="towardsdatascience.com"><!--[--><img class="vp-icon" src="https://cdn-images-1.medium.com/v2/resize:fill:128:128/1*VzTUkfeGymHP4Bvav-T-lA.png" alt aria-hidden no-view sizing="both"><!--]-->towardsdatascience.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/douggregor.net/" aria-label="douggregor.net"><!--[--><i class="vp-icon fas fa-globe" sizing="both"></i><!--]-->douggregor.net<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/d2.naver.com/" aria-label="d2.naver.com"><!--[--><img class="vp-icon" src="/bookshelf/assets/image/d2.naver.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->d2.naver.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/tech.kakao.com/" aria-label="tech.kakao.com"><!--[--><img class="vp-icon" src="https://www.kakaocorp.com/page/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->tech.kakao.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/tech.kakaopay.com/" aria-label="tech.kakaopay.com"><!--[--><img class="vp-icon" src="https://tech.kakaopay.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->tech.kakaopay.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/fe-developers.kakaoent.com/" aria-label="fe-developers.kakaoent.com"><!--[--><img class="vp-icon" src="https://fe-developers.kakaoent.com/favicon-32x32.png?v=44803cb16c1e2debd3984cf2e8cb2ded" alt aria-hidden no-view sizing="both"><!--]-->fe-developers.kakaoent.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/yozm.wishket.com/" aria-label="yozm.wishket.com"><!--[--><img class="vp-icon" src="https://yozm.wishket.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->yozm.wishket.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/popit.kr/" aria-label="popit.kr"><!--[--><img class="vp-icon" src="https://popit.kr/wp-content/uploads/2016/08/favicon_32x32.png" alt aria-hidden no-view sizing="both"><!--]-->popit.kr<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/devkuma.com/" aria-label="devkuma.com"><!--[--><img class="vp-icon" src="https://devkuma.com/favicons/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->devkuma.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/blog.gangnamunni.com/" aria-label="blog.gangnamunni.com"><!--[--><img class="vp-icon" src="https://blog.gangnamunni.com/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->blog.gangnamunni.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/codingeverybody.kr/" aria-label="codingeverybody.kr"><!--[--><img class="vp-icon" src="https://codingeverybody.kr/wp-content/uploads/cropped-favicon-origin-192x192.png" alt aria-hidden no-view sizing="both"><!--]-->codingeverybody.kr<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label><!--[--><i class="vp-icon fas fa-network-wired" sizing="height"></i><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/tecmint.com/" aria-label="tecmint.com"><!--[--><img class="vp-icon" src="https://tecmint.com/wp-content/uploads/2020/07/favicon.ico" alt aria-hidden no-view sizing="both"><!--]-->tecmint.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/docker.com/" aria-label="docker.com"><!--[--><img class="vp-icon" src="https://docker.com/app/uploads/2024/02/cropped-docker-logo-favicon-192x192.png" alt aria-hidden no-view sizing="both"><!--]-->docker.com<!----></a></li><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/bookshelf/learnk8s.io/" aria-label="learnk8s.io"><!--[--><img class="vp-icon" src="https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg" alt aria-hidden no-view sizing="both"><!--]-->learnk8s.io<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/bookshelf/itsfoss.com/" aria-label="itsfoss.com"><!--[--><img class="vp-icon" src="https://itsfoss.com/content/images/size/w256h256/2022/12/android-chrome-192x192.png" alt aria-hidden no-view sizing="both"><!--]-->itsfoss.com<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/chanhi2000/articles" target="_blank" rel="noopener noreferrer" aria-label="Github"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-appearance-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-appearance-dropdown"><!----></div></button></div><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><img class="vp-icon" src="https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg" alt aria-hidden no-view sizing="both"><span class="vp-sidebar-title">learnk8s.io</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2025</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2024</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">2023</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/bookshelf/learnk8s.io/kubernetes-node-size.html" aria-label="Architecting Kubernetes clusters — choosing a worker node size"><!--[--><i class="vp-icon iconfont icon-k8s" sizing="both"></i><!--]-->Architecting Kubernetes clusters — choosing a worker node size<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/bookshelf/learnk8s.io/terraform-eks.html" aria-label="Provisioning Kubernetes clusters on AWS with Terraform and EKS"><!--[--><i class="vp-icon fa-brands fa-aws" sizing="both"></i><!--]-->Provisioning Kubernetes clusters on AWS with Terraform and EKS<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/bookshelf/learnk8s.io/terraform-gke.html" aria-label="Provisioning Kubernetes clusters on GCP with Terraform and GKE"><!--[--><i class="vp-icon iconfont icon-gcp" sizing="both"></i><!--]-->Provisioning Kubernetes clusters on GCP with Terraform and GKE<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2022</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2021</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2020</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">2019</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><div class="page-cover"><img src="https://static.learnk8s.io/a102852d1e938e7c95a134501111ed92.png" alt no-view></div><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><i class="vp-icon iconfont icon-k8s" sizing="height"></i>Architecting Kubernetes clusters — choosing a worker node size</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://linkedin.com/in/danielepolencic" target="_blank" rel="noopener noreferrer">Daniele Polencic</a></span><span property="author" content="Daniele Polencic"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">8/15/23</span><meta property="datePublished" content="2023-08-15T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 18 min</span><meta property="timeRequired" content="PT18M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2" role>DevOps</span><span class="page-category-item color7" role>Kubernetes</span><span class="page-category-item color8" role>Article(s)</span><!--]--><meta property="articleSection" content="DevOps,Kubernetes,Article(s)"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color8" role>blog</span><span class="page-tag-item color2" role>learnk8s.io</span><span class="page-tag-item color7" role>devops</span><span class="page-tag-item color2" role>kubernetes</span><span class="page-tag-item color5" role>k8s</span><!--]--><meta property="keywords" content="blog,learnk8s.io,devops,kubernetes,k8s"></span></div><hr></div><!----><div class="" vp-content><!----><div id="markdown-content"><h1 id="frontmatter-title-관련" tabindex="-1"><a class="header-anchor" href="#frontmatter-title-관련"><span>Architecting Kubernetes clusters — choosing a worker node size 관련</span></a></h1><a class="route-link vp-card" href="/bookshelf/devops/k8s/articles/" style="background:rgba(10,10,10,0.2);"><img class="vp-card-logo" src="https://chanhi2000.github.io/images/ico-wind.svg" loading="lazy" no-view><div class="vp-card-content"><div class="vp-card-title">Kubernetes > Article(s)</div><hr><div class="vp-card-desc">Article(s)</div></div></a><nav class="table-of-contents"><ul><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#cluster-capacity" class="router-link-active router-link-exact-active">Cluster capacity</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#reserved-resource-in-kubernetes-worker-nodes" class="router-link-active router-link-exact-active">Reserved resource in Kubernetes worker nodes</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#resource-allocations-and-efficiency-in-worker-nodes" class="router-link-active router-link-exact-active">Resource allocations and efficiency in worker nodes</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#resiliency-and-replication" class="router-link-active router-link-exact-active">Resiliency and replication</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#scaling-increments-and-lead-time" class="router-link-active router-link-exact-active">Scaling increments and lead time</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#pulling-containers-images" class="router-link-active router-link-exact-active">Pulling containers images</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#kubelet-and-scaling-the-kubernetes-api" class="router-link-active router-link-exact-active">Kubelet and scaling the Kubernetes API</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#node-and-cluster-limits" class="router-link-active router-link-exact-active">Node and cluster limits</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#storage" class="router-link-active router-link-exact-active">Storage</a></li><li><a aria-current="page" href="/bookshelf/learnk8s.io/kubernetes-node-size.html#summary-and-conclusions" class="router-link-active router-link-exact-active">Summary and conclusions</a></li></ul></nav><hr><div class="vp-site-info" data-name="Architecting Kubernetes clusters — choosing a worker node size"><a class="vp-site-info-navigator no-external-link-icon" title="Architecting Kubernetes clusters — choosing a worker node size" href="https://learnk8s.io/kubernetes-node-size" target="_blank"></a><div class="vp-site-info-preview" style="background:url(https://static.learnk8s.io/a102852d1e938e7c95a134501111ed92.png) center/cover no-repeat;"></div><div class="vp-site-info-detail"><img class="vp-site-info-logo" src="https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg" alt loading="lazy" no-view><div class="vp-site-info-name">Architecting Kubernetes clusters — choosing a worker node size</div><div class="vp-site-info-desc">What type of worker nodes should I use for my Kubernetes cluster? And how many of them?. This article looks at the pros and cons.</div></div><!----></div><div class="hint-container important"><p class="hint-container-title">TL;DR</p><p>Should you have a Kubernetes cluster with fewer larger nodes or many smaller nodes? This article discusses the pros and cons.</p></div><p><strong>When you create a Kubernetes cluster, one of the first questions you may have is: &quot;What type of worker nodes should I use, and how many of them?&quot;</strong></p><p><em>If you&#39;re building an on-premises cluster, should you order some last-generation power servers or use the dozen or so old machines that are lying around in your data centre?</em></p><p>Or if you&#39;re using a managed Kubernetes service like Google Kubernetes Engine (GKE), should you use eight <code>n1-standard-1</code> or two <code>n1-standard-4</code> instances to achieve your desired computing capacity?</p><hr><h2 id="cluster-capacity" tabindex="-1"><a class="header-anchor" href="#cluster-capacity"><span>Cluster capacity</span></a></h2><p>In general, a Kubernetes cluster can be seen as abstracting a set of individual nodes as a big &quot;super node&quot;.</p><p>This super node&#39;s total compute capacity (CPU and memory) is the sum of all the constituent nodes&#39; capacities.</p><p>There are multiple ways to achieve this.</p><p>For example, imagine you need a cluster with a total capacity of 8 CPU cores and 32 GB of RAM.</p><p>Here are just two of the possible ways to design your cluster:</p><figure><img src="https://learnk8s.io/a/c642b260295b87df85d97a6e8c20be48.svg" alt="Small vs. large nodes in a Kubernetes cluster" tabindex="0" loading="lazy"><figcaption>Small vs. large nodes in a Kubernetes cluster</figcaption></figure><p>Both options result in a cluster with the same capacity — but the left option uses four smaller nodes, whereas the right one uses two larger nodes.</p><p><em>Which is better?</em></p><p>Let&#39;s start by reviewing how resources are allocated in a worker node.</p><hr><h2 id="reserved-resource-in-kubernetes-worker-nodes" tabindex="-1"><a class="header-anchor" href="#reserved-resource-in-kubernetes-worker-nodes"><span>Reserved resource in Kubernetes worker nodes</span></a></h2><p>Each worker node in a Kubernetes cluster is a compute unit that runs the kubelet — the Kubernetes agent.</p><p>The kubelet is a binary that connects to the control plane and keeps the node&#39;s current state in sync with the state of the cluster.</p><p>For example, when the Kubernetes scheduler assigns a pod to a particular node, it doesn&#39;t send a message to the kubelet.</p><p>Instead, <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#scheduling-cycle-binding-cycle" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>it writes a Binding object and stores it in etcd.</a></p><p>The kubelet checks the state of the cluster on a regular schedule, and as soon as it notices a new pod assigned to its node, it proceeds to download the pod specification and create it.</p><p>The kubelet is often deployed as a SystemD service and runs as part of the operating system.</p><p>Kubelet, SystemD, and operating system need resources such as CPU and memory to function correctly.</p><p><strong>Consequently, not all resources from your worker nodes are available for running pods.</strong></p><p>CPU and memory resources are usually reparted as follows:</p><ol><li>Operating system.</li><li>Kubelet.</li><li>Pods.</li><li>Eviction threshold.</li></ol><figure><img src="https://learnk8s.io/a/d627f4247a50662c83a2a40703a8b693.svg" alt="Resource allocations a in a Kubernetes node" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node</figcaption></figure><p>You might wonder what resources are assigned to each of those.</p><p>While those tend to be configurable, most of the time, the CPU is reserved with the following allocations:</p><ul><li>6% of the first core.</li><li>1% of the following core (up to 2 cores).</li><li>0.5% of the following two cores (up to 4).</li><li>0.25% of any cores above four cores.</li></ul><p>For the memory, it could look like this:</p><ul><li>255 MiB of memory for machines with less than 1 GB.</li><li>25% of the first 4GB of memory.</li><li>20% of the following 4GB of memory (up to 8GB).</li><li>10% of the following 8GB of memory (up to 16GB).</li><li>6% of the next 112GB of memory (up to 128GB).</li><li>2% of any memory above 128GB.</li></ul><p>Finally, the eviction threshold is usually 100MB.</p><p><em>What&#39;s the eviction threshold?</em></p><p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>It&#39;s a threshold for memory usage</a> — if the node crosses that threshold, the kubelet starts evicting pods because there isn&#39;t enough memory in the current node.</p><p><em>Let&#39;s have a look at an example.</em></p><p>For an 8GB and 2 vCPU instance, the available resources are reparted as follows:</p><ol><li>70m vCPU and 1.8GB for the kubelet and operating system (those are usually bundled together).</li><li>100MB for the eviction threshold.</li><li>The remaining 6.1GB memory and 1930 millicores can be used by pods.</li></ol><p>Only 75% of the total memory is used to run workloads.</p><figure><img src="https://learnk8s.io/a/3de0f4647a4b0d71b196d4e394aa5451.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 8GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 8GB of memory</figcaption></figure><p><em>But it doesn&#39;t end there.</em></p><p>Your node may need to run pods on every node (e.g. DaemonSets) to function correctly, and those consume memory and CPU too.</p><p>Examples include Kube-proxy, a log agent such as Fluentd or Fluent Bit, NodeLocal DNSCache or a CSI driver.</p><p><strong>This is a fixed cost you must pay regardless of the node size.</strong></p><figure><img src="https://learnk8s.io/a/e77f8c687be1e38bd35470e177e4290a.svg" alt="Resource allocations a in a Kubernetes node with DaemonSets" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with DaemonSets</figcaption></figure><p>With this in mind, let&#39;s examine the pros and cons of the two opposing directions of &quot;few large nodes&quot; and &quot;many small nodes&quot;.</p><div class="hint-container note"><p class="hint-container-title">Note</p><p>Note that &quot;nodes&quot; in this article always refers to worker nodes. The choice of number and size of control plane nodes is an entirely different topic.</p></div><hr><h2 id="resource-allocations-and-efficiency-in-worker-nodes" tabindex="-1"><a class="header-anchor" href="#resource-allocations-and-efficiency-in-worker-nodes"><span>Resource allocations and efficiency in worker nodes</span></a></h2><p>Resources reserved by the kubelet decrease with larger instances.</p><p>Let&#39;s have a look at two extreme scenarios.</p><p>You want to deploy seven replicas for an application with requests of 0.3 vCPU and 2GB of memory.</p><ol><li>In the first scenario, you provision a single worker node to deploy all replicas.</li><li>In the second scenario, you deploy a replica per node.</li></ol><div class="hint-container note"><p class="hint-container-title">Note</p><p>For the sake of simplicity, we will assume that no DaemonSets are running on those nodes.</p></div><p>The total resources needed by seven replicas are 2.1 vCPU and 14GB of memory (i.e. <code>7 x 300m = 2.1 vCPU</code> and <code>7 x 2GB = 14GB</code>).</p><p><em>Can a 4 vCPU and 16GB instance run the workloads?</em></p><p>Let&#39;s do the math for the CPU reserved:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">6% of the first core        = 60m +</span>
<span class="line">1% of the second core       = 10m +</span>
<span class="line">0.5% of the remaining cores = 10m</span>
<span class="line">---------------------------------</span>
<span class="line">total                       = 80m</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The available CPU for running pods is 3.9 vCPU (i.e. <code>4000m - 80m</code>) — more than enough.</p><p>Let&#39;s check the memory reserved for the kubelet:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">25% of the first 4GB of memory = 1GB</span>
<span class="line">20% of the following 4GB of memory  = 0.8GB</span>
<span class="line">10% of the following 8GB of memory  = 0.8GB</span>
<span class="line">--------------------------------------</span>
<span class="line">total                          = 2.8GB</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The total memory available to pods is <code>16GB - (2.8GB + 0.1GB)</code> — where 0.1GB takes into account the 100MB of eviction threshold.</p><p>Finally, pods can consume up to 13.1GB of memory.</p><figure><img src="https://learnk8s.io/a/c72727b60ee6387d73c40004f9003561.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory</figcaption></figure><p><strong>Unfortunately, this is not enough (i.e. 7 replicas require 14GB of memory, but you have only 13.1GB), and you should provision a compute unit with more memory to deploy the workloads.</strong></p><p>If you use a cloud provider, the next available increment for the compute unit is 4 vCPU and 32GB of memory.</p><figure><img src="https://learnk8s.io/a/c1be9a8de0824f6e5abcba5371bb9b4e.svg" alt="A node with 2 vCPU and 16GB of memory is insufficient to run seven replicas" tabindex="0" loading="lazy"><figcaption>A node with 2 vCPU and 16GB of memory is insufficient to run seven replicas</figcaption></figure><p><em>Excellent!</em></p><p>Let&#39;s look at the other scenario where we try to find the smallest instance that could fit a single replica with requests equal to 0.3 vCPU and 2GB of memory.</p><p><strong>Let&#39;s try with an instance type with 1 vCPU and 4GB of memory.</strong></p><p>The total reserved CPU is 6% or 60m, and the available CPU to pods is 940m.</p><p>Since the app only requires 300m of CPU, this is enough.</p><p>The reserved memory for the kubelet is 25% or 1GB plus an additional 0.1GB of eviction threshold.</p><p>The total available memory for pods is 2.9GB; since the app only requires 2GB, this value is sufficient.</p><p><em>Great!</em></p><figure><img src="https://learnk8s.io/a/f97ca286843a3ff5ee0fbc9a55630829.svg" alt="Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory" tabindex="0" loading="lazy"><figcaption>Resource allocations a in a Kubernetes node with 2 vCPU and 16GB of memory</figcaption></figure><p><em>Let&#39;s compare the two setups.</em></p><p>The total resources for the first cluster are just a single node — 4vCPU and 32 GB.</p><p>The second cluster has seven instances with 1 vCPU and 4GB of memory (for a total of 7 vCPU and 28 GB of memory).</p><p>In the first example, 2.9GB of memory and 80m of CPU are reserved for Kubernetes.</p><p>In the second, 7.7GB (1.1GB x 7 instances) of memory and 360m of CPU (60m x 7 instances) are reserved.</p><p><strong>You can already notice how resources are utilized more efficiently when provisioning larger nodes.</strong></p><figure><img src="https://learnk8s.io/a/23885fd9cefd09159d3bab9618a55aae.svg" alt="Comparing resource allocations between a cluster with a single node and another with multiple nodes" tabindex="0" loading="lazy"><figcaption>Comparing resource allocations between a cluster with a single node and another with multiple nodes</figcaption></figure><p><em>But there&#39;s more to it.</em></p><p>The larger instance still has space to run more replicas <em>— but how many?</em></p><ul><li>The reserved memory is 3.66GB (3.56GB kubelet + 0.1GB eviction threshold), and the total available memory to pods is 28.44GB.</li><li>The reserved CPU is still 80m, and pods can use 3920m.</li></ul><p>At this point, you can find the max number of replicas for memory and CPU with the following division:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">Total CPU   3920 /</span>
<span class="line">Pod CPU      300</span>
<span class="line">------------------</span>
<span class="line">Max Pod       13.1</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can repeat the calculation for the memory:</p><div class="language-plaintext line-numbers-mode" data-highlighter="prismjs" data-ext="plaintext"><pre><code class="language-plaintext"><span class="line">Total memory  28.44 /</span>
<span class="line">Pod memory     2</span>
<span class="line">---------------------</span>
<span class="line">Max Pod       14.22</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The above numbers suggest you run out of CPU before memory and can host up to 13 pods in the 4 vCPU and 32GB worker node.</p><figure><img src="https://learnk8s.io/a/8166b554c895b638a68d2ef76475f943.svg" alt="Calculating the pod capacity for a 2 vCPU and 32GB worker node" tabindex="0" loading="lazy"><figcaption>Calculating the pod capacity for a 2 vCPU and 32GB worker node</figcaption></figure><p><em>What about the second scenario?</em></p><p><em>Is there any room to scale?</em></p><p>Not really.</p><p><strong>While the instances still have more CPU, they only have 0.9GB of memory available after you deploy the first pod.</strong></p><figure><img src="https://learnk8s.io/a/2e379971f411f7ca2908ebed0798c020.svg" alt="Calculating the pod capacity for a 1 vCPU and 4GB worker node" tabindex="0" loading="lazy"><figcaption>Calculating the pod capacity for a 1 vCPU and 4GB worker node</figcaption></figure><p>In conclusion, not only does the larger node utilise resources better, but it can also minimise the fragmentation of resources and increase efficiency.</p><p><em>Does this mean that you should always provision larger instances?</em></p><p><em>Let&#39;s look at another extreme scenario: what happens when a node is lost unexpectedly?</em></p><hr><h2 id="resiliency-and-replication" tabindex="-1"><a class="header-anchor" href="#resiliency-and-replication"><span>Resiliency and replication</span></a></h2><p>A small number of nodes may limit your applications&#39; effective degree of replication.</p><p>For example, if you have a high-availability application consisting of 5 replicas but only two nodes, then the effective degree of replication is reduced to 2. This is because the five replicas can be distributed only across two nodes, and if one of them fails, it may take down multiple replicas at once.</p><figure><img src="https://learnk8s.io/a/4db4fb1e1af84fcbcec51b23f9b9e77f.svg" alt="The replication factor for a cluster with two nodes and five replicas is two" tabindex="0" loading="lazy"><figcaption>The replication factor for a cluster with two nodes and five replicas is two</figcaption></figure><p>On the other hand, if you have at least five nodes, each replica can run on a separate node, and a failure of a single node takes down at most one replica.</p><p><strong>Thus, you might require a certain minimum number of nodes in your cluster if you have high-availability requirements.</strong></p><figure><img src="https://learnk8s.io/a/4b0ddbe420f754323537c3bde7f938e2.svg" alt="The replication factor for a cluster with five nodes and five replicas is five" tabindex="0" loading="lazy"><figcaption>The replication factor for a cluster with five nodes and five replicas is five</figcaption></figure><p>You should also take into account the size of the node.</p><p>When a larger node is lost, several replicas are eventually rescheduled to other nodes.</p><p>If the node is smaller and hosts only a few workloads, the scheduler reassigns only a handful of pods.</p><p>While you are unlikely to hit any limits in the scheduler, redeploying many replicas might trigger the Cluster Autoscaler.</p><p>And depending on your setup, this could lead to further slowdowns.</p><p><em>Let&#39;s explore why.</em></p><hr><h2 id="scaling-increments-and-lead-time" tabindex="-1"><a class="header-anchor" href="#scaling-increments-and-lead-time"><span>Scaling increments and lead time</span></a></h2><p>You can scale applications deployed on Kubernetes <a class="route-link" href="/bookshelf/learnk8s.io/kubernetes-autoscaling-strategies.html"><strong>using a combination of a horizontal scaler (i.e. increasing the number of replicas) and cluster autoscaler (i.e. increasing the nodes count)</strong></a>.</p><p><em>Assuming you have a cluster at total capacity, how does the node size impact your autoscaling?</em></p><p>First, you should know that <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#should-i-use-a-cpu-usage-based-node-autoscaler-with-kubernetes" target="_blank" rel="noopener noreferrer">the Cluster Autoscaler doesn&#39;t look at the memory or CPU available (<i class="vp-icon iconfont icon-github" sizing="height"></i><code>kubernetes/autoscaler</code>)</a> when it triggers the autoscaling.</p><p>In other words, a cluster being utilised in total does not trigger the Cluster Autoscaler.</p><p>Instead, the Cluster Autoscaler creates more nodes when a pod is unschedulable due to a lack of resources.</p><p>At that point, the autoscaler calls the cloud provider API to provision more nodes for that cluster.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="v-0" aria-selected="true">1/</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-1" aria-selected="false">2/2</button></div><!--[--><div class="vp-tab active" id="v-0" role="tabpanel" aria-expanded="true"><div class="vp-tab-title">1/</div><!--[--><figure><img src="https://learnk8s.io/a/a26ee4d5052d327089d62b6da143f914.svg" alt="The Cluster Autoscaler provisions new nodes when pods are pending due to lack of resources." tabindex="0" loading="lazy"><figcaption>The Cluster Autoscaler provisions new nodes when pods are pending due to lack of resources.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-1" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">2/2</div><!--[--><figure><img src="https://learnk8s.io/a/6a29222a3225ebb38f4e758210ca85b5.svg" alt="When the node is provisioned, pods can be deployed." tabindex="0" loading="lazy"><figcaption>When the node is provisioned, pods can be deployed.</figcaption></figure><!--]--></div><!--]--></div><p><strong>Unfortunately, provisioning nodes is usually slow.</strong></p><p>It might take several minutes to provision a new virtual machine.</p><p><em>Does the provisioning time change for larger or smaller nodes?</em></p><p>No, it&#39;s usually constant regardless of the instance size.</p><p><strong>Also, the cluster autoscaler isn&#39;t limited to adding a single node at a time; it could add several at once</strong>.</p><p><em>Let&#39;s have a look at an example.</em></p><p>There are two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>An application with 0.3 vCPU and 2GB of memory is deployed in the cluster and scaled to 13 replicas.</p><p>Both setups are running at total capacity — they don&#39;t have any extra space for pods left.</p><figure><img src="https://learnk8s.io/a/f9fd8d7bfbc21dbf4e2f34d68489d1f7.svg" alt="Two clusters with: one pod per node and all pods into a single node" tabindex="0" loading="lazy"><figcaption>Two clusters with: one pod per node and all pods into a single node</figcaption></figure><p><em>What happens when the deployment scales to 15 replicas (i.e. two more)?</em></p><p>In both clusters, the Cluster Autoscaler detects that the extra pods are un-schedulable due to a lack of resources and provisions:</p><ul><li>An extra node of 4 vCPU and 32GB for the first cluster.</li><li>Two 1 vCPU and 4GB for the second cluster.</li></ul><p>Since there isn&#39;t any time difference between provisioning large or small instances, the nodes will be available simultaneously in both scenarios.</p><figure><img src="https://learnk8s.io/a/ab3adead1eea7f1615bff64488318317.svg" alt="Pending pods triggering the autoscaler regardless of their size" tabindex="0" loading="lazy"><figcaption>Pending pods triggering the autoscaler regardless of their size</figcaption></figure><p><em>However, can you spot another difference?</em></p><p>The first cluster has space for 11 more pods since the total capacity is 13. Instead, the second cluster is still maxed out.</p><p>You could argue that smaller increments are more efficient and cheaper because you add only what you need.</p><figure><img src="https://learnk8s.io/a/4b5b12047d321e34a6c2455677fd99a0.svg" alt="Autoscaling increments in large and small nodes" tabindex="0" loading="lazy"><figcaption>Autoscaling increments in large and small nodes</figcaption></figure><p>But let&#39;s observe what happens when you scale the deployment again — this time to 17 replicas (i.e. two more).</p><ul><li>The first cluster creates two extra pods in the existing node.</li><li>The second cluster is running at capacity. The pods are Pending, and the Cluster Autoscaler is triggered. Finally, two more worker nodes are provisioned.</li></ul><figure><img src="https://learnk8s.io/a/be78be3cd35dc7387e2352c4630c5ae9.svg" alt="Trade-offs for autoscaling increments in Kubernetes nodes" tabindex="0" loading="lazy"><figcaption>Trade-offs for autoscaling increments in Kubernetes nodes</figcaption></figure><p><strong>In the first cluster, the scaling is almost instantaneous.</strong></p><p>In the second, you must wait for the nodes to be provisioned before the pods can serve requests.</p><p>In other words, scaling is quicker in the former case and takes more time in the latter.</p><p><strong>In general, since provisioning time is in the range of minutes, you should think carefully about triggering the Cluster Autoscaler sparingly not to incur longer pod lead time.</strong></p><p>In other words, you can have quicker scaling with larger nodes if you are okay with (potentially) having resources not fully utilised.</p><p><em>But it doesn&#39;t end there.</em></p><p>Pulling container images also affects how quickly you can scale your workloads — and that is related to the number of nodes in the cluster.</p><hr><h2 id="pulling-containers-images" tabindex="-1"><a class="header-anchor" href="#pulling-containers-images"><span>Pulling containers images</span></a></h2><p>When a pod is created in Kubernetes, its definition is stored in etcd.</p><p>It&#39;s the kubelet&#39;s job to detect that the pod is assigned to its node and create it.</p><p>The kubelet will:</p><ul><li>Download the definition from the control plane.</li><li><a href="/learnk8s.io/kubernetes-network-packets#how-linux-network-namespaces-work-in-a-pod.md"><strong>Invoke the Container Runtime Interface (CRI) to create the Pod sandbox. The CRI invokes the Container Network Interface (CNI) to attach the Pod to the network</strong></a>.</li><li>Invoke the Container Storage Interface (CSI) to mount any container volume.</li></ul><p>At the end of those steps, the Pod is alive, and the kubelet can move on to checking liveness and readiness probes and update the control plane with the state of the new Pod.</p><figure><img src="https://learnk8s.io/a/b321498cc64bfd4bc424abd7a8a8f461.svg" alt="The Kubelet and the CRI, CSI and CNI interfaces" tabindex="0" loading="lazy"><figcaption>The Kubelet and the CRI, CSI and CNI interfaces</figcaption></figure><p><strong>It&#39;s essential to notice that when the CRI creates the container in the pod, it must first download the container image.</strong></p><p>That&#39;s unless the container image is already cached on the current node.</p><p>Let&#39;s have a look at how this affects scaling with two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>Let&#39;s deploy 13 replicas of an app with 0.3 vCPU and 2GB of memory.</p><p>The app uses a container image <a href="https://hub.docker.com/_/openjdk" target="_blank" rel="noopener noreferrer"><i class="vp-icon fa-brands fa-docker" sizing="height"></i>based on OpenJDK</a> and weighs 1GB (the base image alone is 775MB).</p><p>What happens to the two clusters?</p><ul><li>In the first cluster, the Container Runtime downloads the image once and runs 13 replicas.</li><li>In the second cluster, each Container Runtime downloads and runs the image.</li></ul><p>In the first scenario, only 1GB is downloaded.</p><figure><img src="https://learnk8s.io/a/352ea906f7800c90ed71deea01eac4f1.svg" alt="The container runtime download the container image once and runs 13 replicas" tabindex="0" loading="lazy"><figcaption>The container runtime download the container image once and runs 13 replicas</figcaption></figure><p>However, you download 13GB of container images in the second scenario.</p><p>Since downloading takes time, the second cluster is slower at creating replicas than the first.</p><p>It also uses more bandwidth and makes more requests (i.e. at least one request for each image layer, 13 times), making it more prone to network glitches.</p><figure><img src="https://learnk8s.io/a/d4dfe6e68938734993dd6b5bd2498e7b.svg" alt="Each of the 13 container runtimes download one image" tabindex="0" loading="lazy"><figcaption>Each of the 13 container runtimes download one image</figcaption></figure><p>It&#39;s essential to notice that this issue compounds with the Cluster Autoscaler.</p><p>If you have smaller nodes:</p><ul><li>The Cluster Autoscaler provisions several nodes at once.</li><li>Once ready, each starts to download the container image.</li><li>Finally, the pod is created.</li></ul><p>When you provision larger nodes, the image is likely cached on the node, and the pod can start immediately.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="v-2" aria-selected="true">1/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-3" aria-selected="false">2/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-4" aria-selected="false">3/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-5" aria-selected="false">4/4</button></div><!--[--><div class="vp-tab active" id="v-2" role="tabpanel" aria-expanded="true"><div class="vp-tab-title">1/4</div><!--[--><figure><img src="https://learnk8s.io/a/c77ad0aa9e0afa24e57788948ee1b12c.svg" alt="Imagine having a cluster with 8 nodes, one replica per node." tabindex="0" loading="lazy"><figcaption>Imagine having a cluster with 8 nodes, one replica per node.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-3" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">2/4</div><!--[--><figure><img src="https://learnk8s.io/a/d3b49aee1e2a56dfe0509c3874c308a8.svg" alt="The cluster is full; scaling to 16 replicas triggers the cluster autoscaler." tabindex="0" loading="lazy"><figcaption>The cluster is full; scaling to 16 replicas triggers the cluster autoscaler.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-4" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">3/4</div><!--[--><figure><img src="https://learnk8s.io/a/6b09aa74b75b631b7be55c492d80c084.svg" alt="As soon as the nodes are provisioned, the Container Runtime downloads the container image." tabindex="0" loading="lazy"><figcaption>As soon as the nodes are provisioned, the Container Runtime downloads the container image.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-5" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">4/4</div><!--[--><figure><img src="https://learnk8s.io/a/4f053af905c5ceb1f89c9a70dcabebd9.svg" alt="Finally, the pods are created in the nodes." tabindex="0" loading="lazy"><figcaption>Finally, the pods are created in the nodes.</figcaption></figure><!--]--></div><!--]--></div><p><em>So, should you always provision larger nodes?</em></p><p>Not necessarily.</p><p>You could mitigate nodes downloading the same container image with <a href="https://github.com/rpardini/docker-registry-proxy" target="_blank" rel="noopener noreferrer">a container registry proxy (<i class="vp-icon iconfont icon-github" sizing="height"></i><code>rpardini/docker-registry-proxy</code>)</a>.</p><p>In this case, the image is still downloaded but from a local registry in the current network.</p><p>Or you could warm up the cache for the nodes with tools such as <a href="https://github.com/XenitAB/spegel" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-github" sizing="height"></i><code>XenitAB/spegel</code></a>.</p><p>With Spegel, nodes are peers who can advertise and share container image layers.</p><p>In this other case, container images are downloaded from other worker nodes, and pods can start almost immediately.</p><p>But container bandwidth isn&#39;t the only bandwidth you must keep under control.</p><hr><h2 id="kubelet-and-scaling-the-kubernetes-api" tabindex="-1"><a class="header-anchor" href="#kubelet-and-scaling-the-kubernetes-api"><span>Kubelet and scaling the Kubernetes API</span></a></h2><p>The <code>kubelet</code> is designed to pull information from the control plane.</p><p>So on a regular interval, the <code>kubelet</code> issues a request to the Kubernetes API to check the status of the cluster.</p><p><em>But doesn&#39;t the control plane send instructions to the <code>kubelet</code>?</em></p><p>The pull model is easier to scale because:</p><ul><li>The control plane doesn&#39;t have to push messages to each worker node.</li><li>Nodes can independently query the API server at their own pace.</li><li>The control plane doesn&#39;t have to keep connections with the kubelets open.</li></ul><blockquote><p>Please note that there are notable exceptions. Commands such as <code>kubectl logs</code> and <code>kubectl exec</code> require the control plane to connect to the kubelet (i.e. push model).</p></blockquote><p>But the Kubelet doesn&#39;t just query for info.</p><p>It also reports information back to the master.</p><p>For example, <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/#:~:text=%2D%2Dnode%2Dstatus%2Dupdate%2Dfrequency" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>the <code>kubelet</code> reports the node&#39;s status to the cluster every ten seconds.</a></p><p>Also, the <code>kubelet</code> informs the control plane when a readiness probe fails (and the pod endpoint should be removed from the service).</p><p>And the <code>kubelet</code> keeps the control plane up to date with container metrics.</p><p>In other words, several requests in both directions (i.e. from and to the control plane) are made by the <code>kubelet</code> to the control plane to keep the node functioning correctly.</p><p>In Kubernetes 1.26 and earlier, <a href="https://kubernetes.io/blog/2023/05/15/speed-up-pod-startup/#raised-default-api-query-per-second-limits-for-kubelet" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>the <code>kubelet</code> could issue up to 5 requests per second for this (this has been relaxed with Kubernetes &gt;1.27).</a></p><p><em>So, assuming your kubelet is running at full capacity (i.e. 5rps), what happens when you run several smaller nodes versus a single large node?</em></p><p>Let&#39;s have a look at our two clusters:</p><ol><li>The first has a single node with 4 vCPU and 32GB.</li><li>The second has thirteen nodes with 1 vCPU and 4GB.</li></ol><p>The first generates 5 requests per second.</p><figure><img src="https://learnk8s.io/a/cb5baa1a4b290bff10e3f693fb5fce97.svg" alt="A single kubelet issueing 5 requests per second" tabindex="0" loading="lazy"><figcaption>A single kubelet issueing 5 requests per second</figcaption></figure><p>The second 65 requests per second (i.e. <code>13 x 5</code>).</p><figure><img src="https://learnk8s.io/a/106f96256da416271d46d757e912c983.svg" alt="13 kubelets issueing 5 requests per second each" tabindex="0" loading="lazy"><figcaption>13 kubelets issueing 5 requests per second each</figcaption></figure><p>You should scale your API server to cope with more frequent requests when you run clusters with many smaller nodes.</p><p>And in turn, that usually means running a control plane on a larger instance or running multiple control planes.</p><hr><h2 id="node-and-cluster-limits" tabindex="-1"><a class="header-anchor" href="#node-and-cluster-limits"><span>Node and cluster limits</span></a></h2><p><em>Is there a limit on the number of nodes a Kubernetes cluster can have?</em></p><p><a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>Kubernetes is designed to support up to 5000 nodes.</a></p><p>However, this is not a hard constraint, as the team at Google demonstrated by allowing you to <a href="https://cloud.google.com/blog/products/containers-kubernetes/google-kubernetes-engine-clusters-can-have-up-to-15000-nodes" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-gcp" sizing="height"></i>run GKE clusters with 15,000 nodes.</a></p><p>For most use cases, 5000 nodes is already a large number and might not be a factor that could steer your decision towards larger or smaller nodes.</p><p>Instead, the max number of pods that you can run in a node could drive you to rethink your cluster architecture.</p><p><em>So, how many Pods can you run in a Kubernetes node?</em></p><p>Most cloud providers let you run between 110 and 250 pods per node.</p><p>If you provision a cluster yourself, <a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#:~:text=Default%3A%20%22promiscuous%2Dbridge%22-,maxPods,-int32" target="_blank" rel="noopener noreferrer"><i class="vp-icon iconfont icon-k8s" sizing="height"></i>the default from is 110.</a></p><p>In most cases, this number is not a limitation of the kubelet but the cloud provider&#39;s proneness to the risk of double booking IP addresses.</p><p>To understand what that means, let&#39;s take a step back and look at how the cluster network is constructed.</p><p>In most cases, each worker node is assigned a subnet with 256 addresses (e.g. <code>10.0.1.0/24</code>).</p><figure><img src="https://learnk8s.io/a/cc3274f66d411c87546f1eb88a42511d.svg" alt="Each worker node has a subnet assigned" tabindex="0" loading="lazy"><figcaption>Each worker node has a subnet assigned</figcaption></figure><p>Of those, <a class="route-link" href="/bookshelf/freecodecamp.org/subnet-cheat-sheet-24-subnet-mask-30-26-27-29-and-other-ip-address-cidr-network-references.html"><strong>two are restricted</strong></a> and you can use 254 for running your Pods.</p><p>Consider the scenario where you have 254 pods in the same node.</p><p>You create one more pod but exhausted the available IP addresses, and it stays pending.</p><p>To fix the issue, you decide to decrease the number of replicas to 253. <em>Is the pending pod created in the cluster?</em></p><p>Probably not.</p><p>When you delete the pod, its state changes to &quot;Terminating&quot;.</p><p>The kubelet sends the SIGTERM to the Pod (as well as calling the <code>preStop</code> lifecycle hook, if present) and waits for the containers to shut down gracefully.</p><p>If the containers don&#39;t terminate within 30 seconds, the kubelet sends a SIGKILL signal to the container and forces the process to terminate.</p><p>During this period, the Pod still hasn&#39;t released the IP address, and traffic can still reach it.</p><p>When the pod is finally deleted, the IP address is released.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="v-6" aria-selected="true">1/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-7" aria-selected="false">2/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-8" aria-selected="false">3/4</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-9" aria-selected="false">4/4</button></div><!--[--><div class="vp-tab active" id="v-6" role="tabpanel" aria-expanded="true"><div class="vp-tab-title">1/4</div><!--[--><figure><img src="https://learnk8s.io/a/01b4a22bf8aa92a7998540f6ac1516e0.svg" alt="When a Pod is deleted, the kubelet is notified of the change." tabindex="0" loading="lazy"><figcaption>When a Pod is deleted, the kubelet is notified of the change.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-7" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">2/4</div><!--[--><figure><img src="https://learnk8s.io/a/ba13194ea5b9ef63597e8fb059355887.svg" alt="If the Pod has a preStop hook, it is invoked first. Then, the kubelet sends the SIGTERM signal to the container." tabindex="0" loading="lazy"><figcaption>If the Pod has a preStop hook, it is invoked first. Then, the kubelet sends the SIGTERM signal to the container.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-8" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">3/4</div><!--[--><figure><img src="https://learnk8s.io/a/eaba36dda660dd289189979c3b6ce3c4.svg" alt="By default, the process has 30 seconds to exit, including the preStop hook. If the process isn&#39;t exited by then, the kubelet sends the SIGKILL signal and forces killing the process." tabindex="0" loading="lazy"><figcaption>By default, the process has 30 seconds to exit, including the preStop hook. If the process isn&#39;t exited by then, the kubelet sends the SIGKILL signal and forces killing the process.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-9" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">4/4</div><!--[--><figure><img src="https://learnk8s.io/a/c05c50cd03dba4465b38b95a016b3712.svg" alt="The kubelet notifies the control plane that the Pod was deleted successfully. The IP address is finally released." tabindex="0" loading="lazy"><figcaption>The kubelet notifies the control plane that the Pod was deleted successfully. The IP address is finally released.</figcaption></figure><!--]--></div><!--]--></div><p>At this point, the pending pod can be created, and it is assigned the same IP address as the last.</p><p><em>Is this a good idea?</em></p><p>Well, there isn&#39;t any other IP available — so you don&#39;t have a choice.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="v-10" aria-selected="true">1/3</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-11" aria-selected="false">2/3</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-12" aria-selected="false">3/3</button></div><!--[--><div class="vp-tab active" id="v-10" role="tabpanel" aria-expanded="true"><div class="vp-tab-title">1/3</div><!--[--><figure><img src="https://learnk8s.io/a/e038d5441bbab5e27bff91fa567517e0.svg" alt="Imagine your node is using all available IP addresses." tabindex="0" loading="lazy"><figcaption>Imagine your node is using all available IP addresses.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-11" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">2/3</div><!--[--><figure><img src="https://learnk8s.io/a/f0bdd2ddeb962d9e3303965eef0006d3.svg" alt="When a pod is deleted, the IP address is not released immediately. You have to wait for the graceful shutdown." tabindex="0" loading="lazy"><figcaption>When a pod is deleted, the IP address is not released immediately. You have to wait for the graceful shutdown.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-12" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">3/3</div><!--[--><figure><img src="https://learnk8s.io/a/362ab748b014147011e2a723ae8fa71b.svg" alt="As soon as the pod is deleted, the IP address can be reused." tabindex="0" loading="lazy"><figcaption>As soon as the pod is deleted, the IP address can be reused.</figcaption></figure><!--]--></div><!--]--></div><p><em>What are the consequences?</em></p><p>Remember when we mentioned that the pod should gracefully shut down and handle all pending requests?</p><p>Well, if the pod is terminated abruptly (i.e. no graceful shutdown) and the IP address is immediately assigned to a different pod, all existing apps and kubernetes components might still not be aware of the change.</p><p>As a result, some of the existing traffic could be erroneously sent to the new Pod because it has the same IP address as the old one.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><button type="button" class="vp-tab-nav active" role="tab" aria-controls="v-13" aria-selected="true">1/2</button><button type="button" class="vp-tab-nav" role="tab" aria-controls="v-14" aria-selected="false">2/2</button></div><!--[--><div class="vp-tab active" id="v-13" role="tabpanel" aria-expanded="true"><div class="vp-tab-title">1/2</div><!--[--><figure><img src="https://learnk8s.io/a/7b2d466a8559f46f477801ae2ca257fd.svg" alt="The ingress controller routes traffic to an IP address." tabindex="0" loading="lazy"><figcaption>The ingress controller routes traffic to an IP address.</figcaption></figure><!--]--></div><div class="vp-tab" id="v-14" role="tabpanel" aria-expanded="false"><div class="vp-tab-title">2/2</div><!--[--><figure><img src="https://learnk8s.io/a/a785b7150b052e5f5f6906c17c7312b8.svg" alt="If the IP address is recycled and used by a new Pod without waiting for the graceful shutdown, the ingress controller might still route traffic to that IP address." tabindex="0" loading="lazy"><figcaption>If the IP address is recycled and used by a new Pod without waiting for the graceful shutdown, the ingress controller might still route traffic to that IP address.</figcaption></figure><!--]--></div><!--]--></div><p>To avoid this issue, you can have lesser IP addresses assigned (e.g. 110) and use the remaining ones as buffers.</p><p>That way, you can be reasonably sure that the same IP address isn&#39;t immediately reused.</p><hr><h2 id="storage" tabindex="-1"><a class="header-anchor" href="#storage"><span>Storage</span></a></h2><p>Compute units have restrictions on the number of disks that can be attached.</p><p>For example, a Standard_D2_v5 with 2 vCPU and 8GB of memory can have up to 4 data disks attached on Azure.</p><p>If you wish to deploy a StatefulSet to a worker node that uses the Standard_D2_v5 instance type, you won&#39;t be able to create more than four replicas.</p><p>That&#39;s because each replica in a StatefulSet has a disk attached.</p><p>As soon as you create the fifth, the Pod will stay pending because the Persistent Volume Claim can&#39;t be bound to a Persistent Volume.</p><p><em>And why not?</em></p><p>Because each Persistent Volume is an attached disk, you can have only 4 for that instance.</p><p><em>So, what are your options?</em></p><p>You can provision a larger instance.</p><p>Or you could be reusing the same disk with a different <code>subPath</code> field.</p><p>Let&#39;s have a look at an example.</p><p>The following persistent volume requires a disk with 16GB of space:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="pvc.yaml"><span>pvc.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> PersistentVolumeClaim</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> shared</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> default</span>
<span class="line">  <span class="token key atrule">accessModes</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> ReadWriteOnce</span>
<span class="line">  <span class="token key atrule">resources</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">requests</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">storage</span><span class="token punctuation">:</span> 16Gi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>If you submit this resource to the cluster, you&#39;ll observe that a Persistent Volume is created and bound to it.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pv,pvc</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>There is a one-to-one relationship between Persistent Volume and Persistent Volume Claims, so you won&#39;t be able to have more Persistent Volume Claims to use the same disk.</p><p>If you want to use the claim in your pods, you can do so with:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app1</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>You could have another deployment using the same Persistent Volume Claim:</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>However, with this configuration, both pods will write their data in the same folder.</p><p>You could have them working on subdirectories with <code>subPath</code> to work around the issue.</p><div class="code-block-with-title"><div class="code-block-title-bar" data-title="deployment-1.yaml"><span>deployment-1.yaml</span></div><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> app2</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">          <span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">claimName</span><span class="token punctuation">:</span> shared</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> <span class="token string">&#39;/data&#39;</span></span>
<span class="line">              <span class="token key atrule">name</span><span class="token punctuation">:</span> pv<span class="token punctuation">-</span>storage</span>
<span class="line">              <span class="token key atrule">subPath</span><span class="token punctuation">:</span> app2</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><p>The deployments will write their data on the following paths:</p><ul><li><code>/data/app1</code> for the first deployment and</li><li><code>/data/app2</code> for the second.</li></ul><p>This workaround is not a perfect solution and has a few limitations:</p><ul><li>All deployments have to remember to use the <code>subPath</code>.</li><li>If you need to write to the volume, you should opt for a Read-Write-Many volume that can be accessed from multiple nodes. Those are usually expensive to provision.</li></ul><p>Also, the same workaround won&#39;t work with a StatefulSet since this will create a brand new Persistent Volume Claim (and persistent Volume) for each replica.</p><hr><h2 id="summary-and-conclusions" tabindex="-1"><a class="header-anchor" href="#summary-and-conclusions"><span>Summary and conclusions</span></a></h2><p><em>So, should you use a few large nodes or many small nodes in your cluster?</em></p><p>It depends.</p><p><em>What&#39;s small or large, anyway?</em></p><p>It comes down to the workloads that you deploy in your cluster.</p><p>For example, if your application requires 10 GB of memory, running an instance with 16GB of memory equals to &quot;running a smaller node&quot;.</p><p>The same instance with an app that requires only 64MB of memory could be considered &quot;large&quot; since you can fit several of them.</p><p><em>And what about a mix of workloads with different resource requirements?</em></p><p>In Kubernetes, there is no rule that all your nodes must have the same size.</p><p>Nothing stops you from using a mix of different node sizes in your cluster.</p><p>This might allow you to trade off the pros and cons of both approaches.</p><p>While you might find the answer through trial and error, we&#39;ve also built a tool to help you with the process.</p><p><a href="https://learnk8s.io/kubernetes-instance-calculator/" target="_blank" rel="noopener noreferrer"><i class="vp-icon fas fa-globe" sizing="height"></i>The Kubernetes instance calculator</a> lets you explore the best instance type for a given workload.</p><p>Make sure you give it a try.</p><!-- TODO: add ARTICLE CARD --><a class="vp-card" href="https://chanhi2000.github.io/bookshelf/learnk8s.io/kubernetes-node-size.html" target="_blank" style="background:rgba(102,152,204,0.2);"><img class="vp-card-logo" src="https://static.learnk8s.io/f7e5160d4744cf05c46161170b5c11c9.svg" loading="lazy" no-view><div class="vp-card-content"><div class="vp-card-title">Architecting Kubernetes clusters — choosing a worker node size</div><hr><div class="vp-card-desc">What type of worker nodes should I use for my Kubernetes cluster? And how many of them?. This article looks at the pros and cons.</div></div></a></div><!----><!----><!----></div><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/chanhi2000/articles/edit/main/src/learnk8s.io/kubernetes-node-size.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/bookshelf/devops/k8s/articles/" aria-label="/devops/k8s/articles/"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><!---->/devops/k8s/articles/</div></a><a class="route-link auto-link next" href="/bookshelf/learnk8s.io/terraform-eks.html" aria-label="Provisioning Kubernetes clusters on AWS with Terraform and EKS"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Provisioning Kubernetes clusters on AWS with Terraform and EKS<i class="vp-icon fa-brands fa-aws" sizing="height"></i></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">MIT Licensed | Copyright © 2022-present <a href="https://github.com/chanhi2000">Chan Hee Lee</a></div><div class="vp-copyright">Copyright © 2025 Daniele Polencic </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/bookshelf/assets/app-BVguHYKu.js" defer></script>
  </body>
</html>
